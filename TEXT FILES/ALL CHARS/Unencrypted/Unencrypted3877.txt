programs which will begin to learn a bit more about the structure of the
language, and then, can go to tag more text.

HOCKEY said that the more that is tagged accurately, the more one can
refine the tagging process and thus the bigger body of text one can build
up with linguistic tagging incorporated into it.  Hence, the more tagging
or annotation there is in the text, the more one may begin to learn about
language and the more it will help accomplish more intelligent OCR.  She
recommended the development of software tools that will help one begin to
understand more about a text, which can then be applied to scanning
images of that text in that format and to using more intelligence to help
one interpret or understand the text.

HOCKEY posited the need to think about common methods of text-encoding
for a long time to come, because building these large bodies of text is
extremely expensive and will only be done once.

In the more general discussion on approaches to encoding that followed,
these points were made:

BESSER identified the underlying problem with standards that all have to
struggle with in adopting a standard, namely, the tension between a very
highly defined standard that is very interchangeable but does not work
for everyone because something is lacking, and a standard that is less
defined, more open, more adaptable, but less interchangeable.  Contending
that the way in which people use SGML is not sufficiently defined, BESSER
wondered 1) if people resist the TEI because they think it is too defined
in certain things they do not fit into, and 2) how progress with
interchangeability can be made without frightening people away.

SPERBERG-McQUEEN replied that the published drafts of the TEI had met
with surprisingly little objection on the grounds that they do not allow
one to handle X or Y or Z.  Particular concerns of the affiliated
projects have led, in practice, to discussions of how extensions are to
be made; the primary concern of any project has to be how it can be
represented locally, thus making interchange secondary.  The TEI has
received much criticism based on the notion that everything in it is
required or even recommended, which, as it happens, is a misconception
from the beginning,   because none of it is required and very little is
actually actively recommended for all cases, except that one document
one's source.

SPERBERG-McQUEEN agreed with BESSER about this trade-off:  all the
projects in a set of twenty TEI-conformant projects will not necessarily
tag the material in the same way.  One result of the TEI will be that the
easiest problems will be solved--those dealing with the external form of
the information; but the problem that is hardest in interchange is that
one is not encoding what another wants, and vice versa.  Thus, after
the adoption of a common notation, the differences in the underlying
conceptions of what is interesting about texts become more visible.
The success of a standard like the TEI will lie in the ability of
the recipient of interchanged texts to use some of what it contains
and to add the information that was not encoded that one wants, in a
layered way, so that texts can be gradually enriched and one does not
have to put in everything all at once.  Hence, having a well-behaved
markup scheme is important.

STEVENS followed up on the paradoxical analogy that BESSER alluded to in
the example of the MARC records, namely, the formats that are the same
except that they are different.  STEVENS drew a parallel between
document-type definitions and MARC records for books and serials and maps,
where one has a tagging structure and there is a text-interchange.
STEVENS opined that the producers of the information will set the terms
for the standard (i.e., develop document-type definitions for the users
of their products), creating a situation that will be problematical for
an institution like the Library of Congress, which will have to deal with
the DTDs in the event that a multiplicity of them develops.  Thus,
numerous people are seeking a standard but cannot find the tag set that
will be acceptable to them and their clients.  SPERBERG-McQUEEN agreed
with this view, and said that the situation was in a way worse:  attempting
to unify arbitrary DTDs resembled attempting to unify a MARC record with a
bibliographic record done according to the Prussian instructions.
According to STEVENS, this situation occurred very early in the process.

WATERS recalled from early discussions on Project Open Book the concern
of many people that merely by producing images, POB was not really
enhancing intellectual access to the material.  Nevertheless, not wishing
to overemphasize the opposition between imaging and full text, WATERS
stated that POB views getting the images as a first step toward possibly
converting to full text through character recognition, if the technology
is appropriate.  WATERS also emphasized that encoding is involved even
with a set of images.

SPERBERG-McQUEEN agreed with WATERS that one can create an SGML document
consisting wholly of images.  At first sight, organizing graphic images
with an SGML document may not seem to offer great advantages, but the
advantages of the scheme WATERS described would be precisely that
ability to move into something that is more of a multimedia document:
a combination of transcribed text and page images.  WEIBEL concurred in
this judgment, offering evidence from Project ADAPT, where a page is
divided into text elements and graphic elements, and in fact the text
elements are organized by columns and lines.  These lines may be used as
the basis for distributing documents in a network environment.  As one
develops software intelligent enough to recognize what those elements
are, it makes sense to apply SGML to an image initially, that may, in
fact, ultimately become more and more text, either through OCR or edited
OCR or even just through keying.  For WATERS, the labor of composing the
document and saying this set of documents or this set of images belongs
to this document constitutes a significant investment.

WEIBEL also made the point that the AAP tag sets, while not excessively
prescriptive, offer a common starting point; they do not define the
structure of the documents, though.  They have some recommendations about
DTDs one could use as examples, but they do just suggest tag sets.   For
example, the CORE project attempts to use the AAP markup as much as
possible, but there are clearly areas where structure must be added.
That in no way contradicts the use of AAP tag sets.

SPERBERG-McQUEEN noted that the TEI prepared a long working paper early
on about the AAP tag set and what it lacked that the TEI thought it
needed, and a fairly long critique of the naming conventions, which has
led to a very different style of naming in the TEI.  He stressed the
importance of the opposition between prescriptive markup, the kind that a
publisher or anybody can do when producing documents de novo, and
descriptive markup, in which one has to take what the text carrier
provides.  In these particular tag sets it is easy to overemphasize this
opposition, because the AAP tag set is extremely flexible.  Even if one
just used the DTDs, they allow almost anything to appear almost anywhere.

                                 ******

SESSION VI.  COPYRIGHT ISSUES

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
PETERS * Several cautions concerning copyright in an electronic
environment * Review of copyright law in the United States * The notion
of the public good and the desirability of incentives to promote it *
What copyright protects * Works not protected by copyright * The rights
of copyright holders * Publishers' concerns in today's electronic
environment * Compulsory licenses * The price of copyright in a digital
medium and the need for cooperation * Additional clarifications *  Rough
justice oftentimes the outcome in numerous copyright matters * Copyright
in an electronic society * Copyright law always only sets up the
boundaries; anything can be changed by contract *
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Marybeth PETERS, policy planning adviser to the Register of Copyrights,
Library of Congress,   made several general comments and then opened the
floor to discussion of subjects of interest to the audience.

Having attended several sessions in an effort to gain a sense of what
people did and where copyright would affect their lives, PETERS expressed
the following cautions:

     * If one takes and converts materials and puts them in new forms,
     then, from a copyright point of view, one is creating something and
     will receive some rights.

     * However, if what one is converting already exists, a question
     immediately arises about the status of the materials in question.

     * Putting something in the public domain in the United States offers
     some freedom from anxiety, but distributing it throughout the world
     on a network is another matter, even if one has put it in the public
     domain in the United States.  Re foreign laws, very frequently a
     work can be in the public domain in the United States but protected
     in other countries.  Thus, one must consider all of the places a
     work may reach, lest one unwittingly become liable to being faced
     with a suit for copyright infringement, or at least a letter
     demanding discussion of what one is doing.

PETERS reviewed copyright law in the United States.  The U.S.
Constitution effectively states that Congress has the power to enact
copyright laws for two purposes:  1) to encourage the creation and
dissemination of intellectual works for the good of society as a whole;
and, significantly, 2) to give creators and those who package and
disseminate materials the economic rewards that are due them.

Congress strives to strike a balance, which at times can become an
emotional issue.  The United States has never accepted the notion of the
natural right of an author so much as it has accepted the notion of the
public good and the desirability of incentives to promote it.  This state
of affairs, however, has created strains on the international level and
is the reason for several of the differences in the laws that we have.
Today the United States protects almost every kind of work that can be
called an expression of an author.  The standard for gaining copyright
protection is simply originality.  This is a low standard and means that
a work is not copied from something else, as well as shows a certain

     * The pricing system for the journal resembles that for most medical
     journals:  for 1992, $95 for a year, plus telecommunications charges
     (there are no connect time charges);    for 1993, $110 for the
     entire year for single users, though the journal can be put on a
     local area network (LAN).  However, only one person can access the
     journal at a time.  Site licenses may come in the future.

     * AAAS is working closely with colleagues at OCLC to display
     mathematical equations on screen.

     * Without compromising any steps in the editorial process, the
     technology has reduced the time lag between when a manuscript is
     originally submitted and the time it is accepted; the review process
     does not differ greatly from the standard six-to-eight weeks
     employed by many of the hard-copy journals.  The process still
     depends on people.

     * As far as a preservation copy is concerned, articles will be
     maintained on the computer permanently and subscribers, as part of
     their subscription, will receive a microfiche-quality archival copy
     of everything published during that year; in addition, reprints can
     be purchased in much the same way as in a hard-copy environment.
     Hard copies are prepared but are not the primary medium for the
     dissemination of the information.

     * Because OJCCT is not yet on line, it is difficult to know how many
     people would simply browse through the journal on the screen as
     opposed to downloading the whole thing and printing it out; a mix of
     both types of users likely will result.

                                 ******

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
PERSONIUS * Developments in technology over the past decade * The CLASS
Project * Advantages for technology and for the CLASS Project *
Developing a network application an underlying assumption of the project
* Details of the scanning process * Print-on-demand copies of books *
Future plans include development of a browsing tool *
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Lynne PERSONIUS, assistant director, Cornell Information Technologies for
Scholarly Information Services, Cornell University, first commented on
the tremendous impact that developments in technology over the past ten
years--networking, in particular--have had on the way information is
handled, and how, in her own case, these developments have counterbalanced
Cornell's relative geographical isolation.  Other significant technologies
include scanners, which are much more sophisticated than they were ten years
ago; mass storage and the dramatic savings that result from it in terms of
both space and money relative to twenty or thirty years ago; new and
improved printing technologies, which have greatly affected the distribution
of information; and, of course, digital technologies, whose applicability to
library preservation remains at issue.

Given that context, PERSONIUS described the College Library Access and
Storage System (CLASS) Project, a library preservation project,
primarily, and what has been accomplished.  Directly funded by the
Commission on Preservation and Access and by the Xerox Corporation, which
has provided a significant amount of hardware, the CLASS Project has been
working with a development team at Xerox to develop a software
application tailored to library preservation requirements.  Within
Cornell, participants in the project have been working jointly with both
library and information technologies.  The focus of the project has been
on reformatting and saving books that are in brittle condition.
PERSONIUS showed Workshop participants a brittle book, and described how
such books were the result of developments in papermaking around the
beginning of the Industrial Revolution.  The papermaking process was
changed so that a significant amount of acid was introduced into the
actual paper itself, which deteriorates as it sits on library shelves.

One of the advantages for technology and for the CLASS Project is that
the information in brittle books is mostly out of copyright and thus
offers an opportunity to work with material that requires library
preservation, and to create and work on an infrastructure to save the
material.  Acknowledging the familiarity of those working in preservation
with this information, PERSONIUS noted that several things are being
done:  the primary preservation technology used today is photocopying of
brittle material.  Saving the intellectual content of the material is the
main goal.  With microfilm copy, the intellectual content is preserved on
the assumption that in the future the image can be reformatted in any
other way that then exists.

An underlying assumption of the CLASS Project from the beginning was
that it would develop a network application.  Project staff scan books
at a workstation located in the library, near the brittle material.
An image-server filing system is located at a distance from that
workstation, and a printer is located in another building.  All of the
materials digitized and stored on the image-filing system are cataloged
in the on-line catalogue.  In fact, a record for each of these electronic
books is stored in the RLIN database so that a record exists of what is
in the digital library throughout standard catalogue procedures.  In the
future, researchers working from their own workstations in their offices,
or their networks, will have access--wherever they might be--through a
request server being built into the new digital library.  A second
assumption is that the preferred means of finding the material will be by
looking through a catalogue.  PERSONIUS described the scanning process,
which uses a prototype scanner being developed by Xerox and which scans a
very high resolution image at great speed.  Another significant feature,
because this is a preservation application, is the placing of the pages
that fall apart one for one on the platen.  Ordinarily, a scanner could
be used with some sort of a document feeder, but because of this
application that is not feasible.  Further, because CLASS is a
preservation application, after the paper replacement is made there, a
very careful quality control check is performed.  An original book is
compared to the printed copy and verification is made, before proceeding,
that all of the image, all of the information, has been captured.  Then,
a new library book is produced:  The printed images are rebound by a
commercial binder and a new book is returned to the shelf.
Significantly, the books returned to the library shelves are beautiful
and useful replacements on acid-free paper that should last a long time,
in effect, the equivalent of preservation photocopies.  Thus, the project
has a library of digital books.  In essence, CLASS is scanning and
storing books as 600 dot-per-inch bit-mapped images, compressed using
Group 4 CCITT (i.e., the French acronym for International Consultative
Committee for Telegraph and Telephone) compression.  They are stored as
TIFF files on an optical filing system that is composed of a database
used for searching and locating the books and an optical jukebox that
stores 64 twelve-inch platters.  A very-high-resolution printed copy of
these books at 600 dots per inch is created, using a Xerox DocuTech
printer to make the paper replacements on acid-free paper.

PERSONIUS maintained that the CLASS Project presents an opportunity to
introduce people to books as digital images by using a paper medium.
Books are returned to the shelves while people are also given the ability
to print on demand--to make their own copies of books.  (PERSONIUS
distributed copies of an engineering journal published by engineering
students at Cornell around 1900 as an example of what a print-on-demand
copy of material might be like.  This very cheap copy would be available
to people to use for their own research purposes and would bridge the gap
between an electronic work and the paper that readers like to have.)
PERSONIUS then attempted to illustrate a very early prototype of
networked access to this digital library.  Xerox Corporation has
developed a prototype of a view station that can send images across the
network to be viewed.

The particular library brought down for demonstration contained two
mathematics books.  CLASS is developing and will spend the next year
developing an application that allows people at workstations to browse
the books.  Thus, CLASS is developing a browsing tool, on the assumption
that users do not want to read an entire book from a workstation, but
would prefer to be able to look through and decide if they would like to
have a printed copy of it.

                                 ******

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
DISCUSSION * Re retrieval software * "Digital file copyright" * Scanning
rate during production * Autosegmentation * Criteria employed in
selecting books for scanning * Compression and decompression of images *
OCR not precluded *
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

During the question-and-answer period that followed her presentation,
PERSONIUS made these additional points:

     * Re retrieval software, Cornell is developing a Unix-based server
     as well as clients for the server that support multiple platforms
     (Macintosh, IBM and Sun workstations), in the hope that people from
     any of those platforms will retrieve books; a further operating
     assumption is that standard interfaces will be used as much as
     possible, where standards can be put in place, because CLASS
     considers this retrieval software a library application and would
     like to be able to look at material not only at Cornell but at other
     institutions.

     * The phrase "digital file copyright by Cornell University" was
     added at the advice of Cornell's legal staff with the caveat that it
     probably would not hold up in court.  Cornell does not want people
     to copy its books and sell them but would like to keep them
     available for use in a library environment for library purposes.

     * In production the scanner can scan about 300 pages per hour,
     capturing 600 dots per inch.

     * The Xerox software has filters to scan halftone material and avoid
     the moire patterns that occur when halftone material is scanned.
     Xerox has been working on hardware and software that would enable
     the scanner itself to recognize this situation and deal with it
     appropriately--a kind of autosegmentation that would enable the
     scanner to handle halftone material as well as text on a single page.

     * The books subjected to the elaborate process described above were
     selected because CLASS is a preservation project, with the first 500
     books selected coming from Cornell's mathematics collection, because
     they were still being heavily used and because, although they were
     in need of preservation, the mathematics library and the mathematics
     faculty were uncomfortable having them microfilmed.  (They wanted a
     printed copy.)  Thus, these books became a logical choice for this
     project.  Other books were chosen by the project's selection committees
     for experiments with the technology, as well as to meet a demand or need.

     * Images will be decompressed before they are sent over the line; at
     this time they are compressed and sent to the image filing system
     and then sent to the printer as compressed images; they are returned
     to the workstation as compressed 600-dpi images and the workstation
     decompresses and scales them for display--an inefficient way to
     access the material though it works quite well for printing and
     other purposes.

     * CLASS is also decompressing on Macintosh and IBM, a slow process
     right now.  Eventually, compression and decompression will take
     place on an image conversion server.  Trade-offs will be made, based
     on future performance testing, concerning where the file is
     compressed and what resolution image is sent.

     * OCR has not been precluded; images are being stored that have been
     scanned at a high resolution, which presumably would suit them well
     to an OCR process.  Because the material being scanned is about 100
     years old and was printed with less-than-ideal technologies, very
     early and preliminary tests have not produced good results.  But the
     project is capturing an image that is of sufficient resolution to be
     subjected to OCR in the future.  Moreover, the system architecture
     and the system plan have a logical place to store an OCR image if it
     has been captured.  But that is not being done now.

                                 ******

SESSION III.  DISTRIBUTION, NETWORKS, AND NETWORKING:  OPTIONS FOR
DISSEMINATION

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
ZICH * Issues pertaining to CD-ROMs * Options for publishing in CD-ROM *
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Robert ZICH, special assistant to the associate librarian for special
projects, Library of Congress, and moderator of this session, first noted
the blessed but somewhat awkward circumstance of having four very
distinguished people representing networks and networking or at least
leaning in that direction, while lacking anyone to speak from the
strongest possible background in CD-ROMs.  ZICH expressed the hope that
members of the audience would join the discussion.  He stressed the
subtitle of this particular session, "Options for Dissemination," and,
concerning CD-ROMs, the importance of determining when it would be wise
to consider dissemination in CD-ROM versus networks.  A shopping list of
issues pertaining to CD-ROMs included:  the grounds for selecting
commercial publishers, and in-house publication where possible versus
nonprofit or government publication.  A similar list for networks
included:  determining when one should consider dissemination through a
network, identifying the mechanisms or entities that exist to place items
on networks, identifying the pool of existing networks, determining how a
producer  would choose between networks, and identifying the elements of
a business arrangement in a network.

Options for publishing in CD-ROM:  an outside publisher versus
self-publication.  If an outside publisher is used, it can be nonprofit,
such as the Government Printing Office (GPO) or the National Technical
Information Service (NTIS), in the case of government.  The pros and cons
associated with employing an outside publisher are obvious.  Among the
pros, there is no trouble getting accepted.  One pays the bill and, in
effect, goes one's way.  Among the cons, when one pays an outside
publisher to perform the work, that publisher will perform the work it is
obliged to do, but perhaps without the production expertise and skill in
marketing and dissemination that some would seek.  There is the body of
commercial publishers that do possess that kind of expertise in
distribution and marketing but that obviously are selective.  In
self-publication, one exercises full control, but then one must handle
matters such as distribution and marketing.  Such are some of the options
for publishing in the case of CD-ROM.

In the case of technical and design issues, which are also important,
there are many matters which many at the Workshop already knew a good
deal about:  retrieval system requirements and costs, what to do about
images, the various capabilities and platforms, the trade-offs between
cost and performance, concerns about local-area networkability,
interoperability, etc.

                                 ******

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
LYNCH * Creating networked information is different from using networks
as an access or dissemination vehicle * Networked multimedia on a large
scale does not yet work * Typical CD-ROM publication model a two-edged
sword * Publishing information on a CD-ROM in the present world of
immature standards * Contrast between CD-ROM and network pricing *
Examples demonstrated earlier in the day as a set of insular information
gems * Paramount need to link databases * Layering to become increasingly
necessary * Project NEEDS and the issues of information reuse and active
versus passive use * X-Windows as a way of differentiating between
network access and networked information * Barriers to the distribution
of networked multimedia information * Need for good, real-time delivery
protocols * The question of presentation integrity in client-server
computing in the academic world * Recommendations for producing multimedia
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Clifford LYNCH, director, Library Automation, University of California,
opened his talk with the general observation that networked information
constituted a difficult and elusive topic because it is something just
starting to develop and not yet fully understood.  LYNCH contended that
creating genuinely networked information was different from using
networks as an access or dissemination vehicle and was more sophisticated
and more subtle.  He invited the members of the audience to extrapolate,
from what they heard about the preceding demonstration projects, to what
sort of a world of electronics information--scholarly, archival,
cultural, etc.--they wished to end up with ten or fifteen years from now.
LYNCH suggested that to extrapolate directly from these projects would
produce unpleasant results.

Putting the issue of CD-ROM in perspective before getting into
generalities on networked information, LYNCH observed that those engaged
in multimedia today who wish to ship a product, so to say, probably do
not have much choice except to use CD-ROM:  networked multimedia on a
large scale basically does not yet work because the technology does not
exist.  For example, anybody who has tried moving images around over the
Internet knows that this is an exciting touch-and-go process, a
fascinating and fertile area for experimentation, research, and
