the Smithsonian that mounts images on several networks 


Howard BESSER, School of Library and Information Science, University of
Pittsburgh, spoke primarily about multimedia, focusing on images and the
broad implications of disseminating them on the network.  He argued that
planning the distribution of multimedia documents posed two critical
implementation problems, which he framed in the form of two questions:
1 What platform will one use and what hardware and software will users
have for viewing of the material?  and 2 How can one deliver a
sufficiently robust set of information in an accessible format in a
reasonable amount of time?  Depending on whether network or CDROM is the
medium used, this question raises different issues of storage,
compression, and transmission.

Concerning the design of platforms e.g., sound, gray scale, simple
color, etc. and the various capabilities users may have, BESSER
maintained that a layered approach was the way to deal with users
capabilities.  A result would be that users with less powerful
workstations would simply have less functionality.  He urged members of
the audience to advocate standards and accompanying software that handle
layered functionality across a wide variety of platforms.

BESSER also addressed problems in platform design, namely, deciding how
large a machine to design for situations when the largest number of users
have the lowest level of the machine, and one desires higher
functionality.  BESSER then proceeded to the question of file size and
its implications for networking.  He discussed still images in the main.
For example, a digital color image that fills the screen of a standard
megapel workstation Sun or Next will require one megabyte of storage
for an eightbit image or three megabytes of storage for a true color or
twentyfourbit image.  Lossless compression algorithms that is,
computational procedures in which no data is lost in the process of
compressing and decompressing an imagethe exact bitrepresentation is
maintained might bring storage down to a third of a megabyte per image,
but not much further than that.  The question of size makes it difficult
to fit an appropriately sized set of these images on a single disk or to
transmit them quickly enough on a network.

With these full screen megapel images that constitute a third of a
megabyte, one gets 1,0003,000 fullscreen images on a onegigabyte disk
a standard CDROM represents approximately 60 percent of that.  Storing
images the size of a PC screen just 8 bit color increases storage
capacity to 4,00012,000 images per gigabyte 60 percent of that gives
one the size of a CDROM, which in turn creates a major problem.  One
cannot have fullscreen, fullcolor images with lossless compression one
must compress them or use a lower resolution.  For megabytesize images,
anything slower than a T1 speed is impractical.  For example, on a
fiftysixkilobaud line, it takes three minutes to transfer a
onemegabyte file, if it is not compressed and this speed assumes ideal
circumstances no other user contending for network bandwidth.  Thus,
questions of disk access, remote display, and current telephone
connection speed make transmission of megabytesize images impractical.

BESSER then discussed ways to deal with these large images, for example,
compression and decompression at the users end.  In this connection, the
issues of how much one is willing to lose in the compression process and
what image quality one needs in the first place are unknown.  But what is
known is that compression entails some loss of data.  BESSER urged that
more studies be conducted on image quality in different situations, for
example, what kind of images are needed for what kind of disciplines, and
what kind of image quality is needed for a browsing tool, an intermediate
viewing tool, and archiving.

BESSER remarked two promising trends for compression:  from a technical
perspective, algorithms that use what is called subjective redundancy
employ principles from visual psychophysics to identify and remove
information from the image that the human eye cannot perceive from an
interchange and interoperability perspective, the JPEG i.e., Joint
Photographic Experts Group, an ISO standard compression algorithms also
offer promise.  These issues of compression and decompression, BESSER
argued, resembled those raised earlier concerning the design of different
platforms.  Gauging the capabilities of potential users constitutes a
primary goal.  BESSER advocated layering or separating the images from
the applications that retrieve and display them, to avoid tying them to
particular software.

BESSER detailed several lessons learned from his work at Berkeley with
Imagequery, especially the advantages and disadvantages of using
XWindows.  In the latter category, for example, retrieval is tied
directly to ones data, an intolerable situation in the long run on a
networked system.  Finally, BESSER described a project of Jim Wallace at
the Smithsonian Institution, who is mounting images in a extremely
rudimentary way on the Compuserv and Genie networks and is preparing to
mount them on America On Line.  Although the average user takes over
thirty minutes to download these images assuming a fairly fast modem,
nevertheless, images have been downloaded 25,000 times.

BESSER concluded his talk with several comments on the business
arrangement between the Smithsonian and Compuserv.  He contended that not
enough is known concerning the value of images.

                                 


DISCUSSION  Creating digitized photographic collections nearly
impossible except with large organizations like museums  Need for study
to determine quality of images users will tolerate 


During the brief exchange between LESK and BESSER that followed, several
clarifications emerged.

LESK argued that the photographers were far ahead of BESSER:  It is
almost impossible to create such digitized photographic collections
except with large organizations like museums, because all the
photographic agencies have been going crazy about this and will not sign
licensing agreements on any sort of reasonable terms.  LESK had heard
that National Geographic, for example, had tried to buy the right to use
some image in some kind of educational production for 100 per image, but
the photographers will not touch it.  They want accounting and payment
for each use, which cannot be accomplished within the system.  BESSER
responded that a consortium of photographers, headed by a former National
Geographic photographer, had started assembling its own collection of
electronic reproductions of images, with the money going back to the
cooperative.

LESK contended that BESSER was unnecessarily pessimistic about multimedia
images, because people are accustomed to lowquality images, particularly
from video.  BESSER urged the launching of a study to determine what
users would tolerate, what they would feel comfortable with, and what
absolutely is the highest quality they would ever need.  Conceding that
he had adopted a dire tone in order to arouse people about the issue,
BESSER closed on a sanguine note by saying that he would not be in this
business if he did not think that things could be accomplished.

                                 


LARSEN  Issues of scalability and modularity  Geometric growth of the
Internet and the role played by layering  Basic functions sustaining
this growth  A librarys roles and functions in a network environment 
Effects of implementation of the Z39.50 protocol for information
retrieval on the library system  The tradeoff between volumes of data
and its potential usage  A snapshot of current trends 


Ronald LARSEN, associate director for information technology, University
of Maryland at College Park, first addressed the issues of scalability
and modularity.  He noted the difficulty of anticipating the effects of
ordersofmagnitude growth, reflecting on the twenty years of experience
with the Arpanet and Internet.  Recalling the days demonstrations of
CDROM and optical disk material, he went on to ask if the field has yet
learned how to scale new systems to enable delivery and dissemination
across largescale networks.

LARSEN focused on the geometric growth of the Internet from its inception
circa 1969 to the present, and the adjustments required to respond to
that rapid growth.  To illustrate the issue of scalability, LARSEN
considered computer networks as including three generic components:
computers, network communication nodes, and communication media.  Each
component scales e.g., computers range from PCs to supercomputers
network nodes scale from interface cards in a PC through sophisticated
routers and gateways and communication media range from 2,400baud
dialup facilities through 4.5Mbps backbone links, and eventually to
multigigabitpersecond communication lines, and architecturally, the
components are organized to scale hierarchically from local area networks
to internationalscale networks.  Such growth is made possible by
building layers of communication protocols, as BESSER pointed out.
By layering both physically and logically, a sense of scalability is
maintained from local area networks in offices, across campuses, through
bridges, routers, campus backbones, fiberoptic links, etc., up into
regional networks and ultimately into national and international
networks.

LARSEN then illustrated the geometric growth over a twoyear period
through September 1991of the number of networks that comprise the
Internet.  This growth has been sustained largely by the availability of
three basic functions:  electronic mail, file transfer ftp, and remote
logon telnet.  LARSEN also reviewed the growth in the kind of traffic
that occurs on the network.  Network traffic reflects the joint contributions
of a larger population of users and increasing use per user.  Today one sees
serious applications involving moving images across the networka rarity
ten years ago.  LARSEN recalled and concurred with BESSERs main point
that the interesting problems occur at the application level.

LARSEN then illustrated a model of a librarys roles and functions in a
network environment.  He noted, in particular, the placement of online
catalogues onto the network and patrons obtaining access to the library
increasingly through local networks, campus networks, and the Internet.
LARSEN supported LYNCHs earlier suggestion that we need to address
fundamental questions of networked information in order to build
environments that scale in the information sense as well as in the
physical sense.

LARSEN supported the role of the library system as the access point into
the nations electronic collections.  Implementation of the Z39.50
protocol for information retrieval would make such access practical and
feasible.  For example, this would enable patrons in Maryland to search
California libraries, or other libraries around the world that are
conformant with Z39.50 in a manner that is familiar to University of
Maryland patrons.  This clientserver model also supports moving beyond
secondary content into primary content.  The notion of how one links
from secondary content to primary content, LARSEN said, represents a
fundamental problem that requires rigorous thought.  After noting
numerous network experiments in accessing fulltext materials, including
projects supporting the ordering of materials across the network, LARSEN
revisited the issue of transmitting highdensity, highresolution color
images across the network and the large amounts of bandwidth they
require.  He went on to address the bandwidth and synchronization
problems inherent in sending fullmotion video across the network.

LARSEN illustrated the tradeoff between volumes of data in bytes or
orders of magnitude and the potential usage of that data.  He discussed
transmission rates particularly, the time it takes to move various forms
of information, and what one could do with a network supporting
multigigabitpersecond transmission.  At the moment, the network
environment includes a composite of datatransmission requirements,
volumes and forms, going from steady to bursty highvolume and from
very slow to very fast.  This aggregate must be considered in the design,
construction, and operation of multigigabyte networks.

LARSENs objective is to use the networks and library systems now being
constructed to increase access to resources wherever they exist, and
thus, to evolve toward an online electronic virtual library.

LARSEN concluded by offering a snapshot of current trends:  continuing
geometric growth in network capacity and number of users slower
development of applications and glacial development and adoption of
standards.  The challenge is to design and develop each new application
