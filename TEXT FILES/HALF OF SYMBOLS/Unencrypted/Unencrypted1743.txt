incredibly complex and expensive infrastructure.  The economies of
scale that make multiuser setups cheaper per user served do not operate
in an environment that requires a computer workstation, videodisc player,
and two display devices for each user.

The digital multimedia storage model has required vast amounts of storage
space as much as one gigabyte per thirty still images.  In the past the
cost of such a large amount of storage space made this model a
prohibitive choice as well.  But plunging storage costs are finally
making this second alternative viable.

If storage no longer poses such an impediment, what do we need to
consider in building digitally stored multiuser databases of visual
materials?  This presentation will examine the networking and
telecommunication constraints that must be overcome before such databases
can become commonplace and useful to a large number of people.

The key problem is the vast size of multimedia documents, and how this
affects not only storage but telecommunications transmission time.
Anything slower than T1 speed is impractical for files of 1 megabyte or
larger which is likely to be small for a multimedia document.  For
instance, even on a 56 Kb line it would take three minutes to transfer a
1megabyte file.  And these figures assume ideal circumstances, and do
not take into consideration other users contending for network bandwidth,
disk access time, or the time needed for remote display.  Current common
telephone transmission rates would be completely impractical few users
would be willing to wait the hour necessary to transmit a single image at
2400 baud.

This necessitates compression, which itself raises a number of other
issues.  In order to decrease file sizes significantly, we must employ
lossy compression algorithms.  But how much quality can we afford to
lose?  To date there has been only one significant study done of
imagequality needs for a particular user group, and this study did not
look at loss resulting from compression.  Only after identifying
imagequality needs can we begin to address storage and network bandwidth
needs.

Experience with XWindowsbased applications such as Imagequery, the
University of California at Berkeley image database demonstrates the
utility of a clientserver topology, but also points to the limitation of
current software for a distributed environment.  For example,
applications like Imagequery can incorporate compression, but current X
implementations do not permit decompression at the end users
workstation.  Such decompression at the host computer alleviates storage
capacity problems while doing nothing to address problems of
telecommunications bandwidth.

We need to examine the effects on network throughput of moving
multimedia documents around on a network.  We need to examine various
topologies that will help us avoid bottlenecks around servers and
gateways.  Experience with applications such as these raise still broader
questions. How closely is the multimedia document tied to the software
for viewing it?  Can it be accessed and viewed from other applications?
Experience with the MARC format and more recently with the Z39.50
protocols shows how useful it can be to store documents in a form in
which they can be accessed by a variety of application software.

Finally, from an intellectualaccess standpoint, we need to address the
issue of providing access to these multimedia documents in
interdisciplinary environments.  We need to examine terminology and
indexing strategies that will allow us to provide access to this material
in a crossdisciplinary way.

Ronald LARSEN            Directions in HighPerformance Networking for
                         Libraries

The pace at which computing technology has advanced over the past forty
years shows no sign of abating.  Roughly speaking, each fiveyear period
