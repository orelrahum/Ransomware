the previous speakers.  He introduced the topic of special problems by
noting the advantages of electronic imaging.  For example, it is regenerable
because it is a coded file, and realtime quality control is possible with
electronic capture, whereas in photographic capture it is not.

One of the difficulties discussed in the scanning and storage process was
image quality which, without belaboring the obvious, means different
things for maps, medical Xrays, or broadcast television.  In the case of
documents, THOMA said, image quality boils down to legibility of the
textual parts, and fidelity in the case of gray or color photo printtype
material.  Legibility boils down to scan density, the standard in most
cases being 300 dpi.  Increasing the resolution with scanners that
perform 600 or 1200 dpi, however, comes at a cost.

Better image quality entails at least four different kinds of costs:  1
equipment costs, because the CCD i.e., chargecouple device with
greater number of elements costs more  2 time costs that translate to
the actual capture costs, because manual labor is involved the time is
also dependent on the fact that more data has to be moved around in the
machine in the scanning or network devices that perform the scanning as
well as the storage  3 media costs, because at high resolutions larger
files have to be stored and 4 transmission costs, because there is just
more data to be transmitted.

But while resolution takes care of the issue of legibility in image
quality, other deficiencies have to do with contrast and elements on the
page scanned or the image that needed to be removed or clarified.  Thus,
THOMA proceeded to illustrate various deficiencies, how they are
manifested, and several techniques to overcome them.

Fixed thresholding was the first technique described, suitable for
blackandwhite text, when the contrast does not vary over the page.  One
can have many different threshold levels in scanning devices.  Thus,
THOMA offered an example of extremely poor contrast, which resulted from
the fact that the stock was a heavy red.  This is the sort of image that
when microfilmed fails to provide any legibility whatsoever.  Fixed
thresholding is the way to change the blacktored contrast to the
desired blacktowhite contrast.

Other examples included material that had been browned or yellowed by
age.  This was also a case of contrast deficiency, and correction was
done by fixed thresholding.  A final example boils down to the same
thing, slight variability, but it is not significant.  Fixed thresholding
solves this problem as well.  The microfilm equivalent is certainly legible,
but it comes with dark areas.  Though THOMA did not have a slide of the
microfilm in this case, he did show the reproduced electronic image.

When one has variable contrast over a page or the lighting over the page
area varies, especially in the case where a bound volume has light
shining on it, the image must be processed by a dynamic thresholding
scheme.  One scheme, dynamic averaging, allows the threshold level not to
be fixed but to be recomputed for every pixel from the neighboring
characteristics.  The neighbors of a pixel determine where the threshold
should be set for that pixel.

THOMA showed an example of a page that had been made deficient by a
variety of techniques, including a burn mark, coffee stains, and a yellow
marker.  Application of a fixedthresholding scheme, THOMA argued, might
take care of several deficiencies on the page but not all of them.
Performing the calculation for a dynamic threshold setting, however,
removes most of the deficiencies so that at least the text is legible.

Another problem is representing a gray level with blackandwhite pixels
by a process known as dithering or electronic screening.  But dithering
does not provide good image quality for pure blackandwhite textual
material.  THOMA illustrated this point with examples. Although its
suitability for photoprint is the reason for electronic screening or
dithering, it cannot be used for every compound image.  In the document
that was distributed by CXP, THOMA noticed that the dithered image of the
IEEE test chart evinced some deterioration in the text.  He presented an
extreme example of deterioration in the text in which compounded
documents had to be set right by other techniques.  The technique
illustrated by the present example was an image merge in which the page
is scanned twice and the settings go from fixed threshold to the
dithering matrix the resulting images are merged to give the best
results with each technique.

THOMA illustrated how dithering is also used in nonphotographic or
nonprint materials with an example of a grayish page from a medical text,
which was reproduced to show all of the gray that appeared in the
original.  Dithering provided a reproduction of all the gray in the
original of another example from the same text.

THOMA finally illustrated the problem of bordering, or pageedge,
effects.  Books and bound volumes that are placed on a photocopy machine
or a scanner produce pageedge effects that are undesirable for two
reasons:  1 the aesthetics of the image after all, if the image is to
be preserved, one does not necessarily want to keep all of its
deficiencies 2 compression with the bordering problem THOMA
illustrated, the compression ratio deteriorated tremendously.  One way
to eliminate this more serious problem is to have the operator at the
point of scanning window the part of the image that is desirable and
automatically turn all of the pixels out of that picture to white.

                                 


FLEISCHHAUER  AMs experience with scanning bound materials  Dithering



Carl FLEISCHHAUER, coordinator, American Memory, Library of Congress,
reported AMs experience with scanning bound materials, which he likened
to the problems involved in using photocopying machines.  Very few
devices in the industry offer bookedge scanning, let alone book cradles.
The problem may be unsolvable, FLEISCHHAUER said, because a large enough
market does not exist for a preservationquality scanner.  AM is using a
Kurzweil scanner, which is a bookedge scanner now sold by Xerox.

Devoting the remainder of his brief presentation to dithering,
FLEISCHHAUER related AMs experience with a contractor who was using
unsophisticated equipment and software to reduce moire patterns from
printed halftones.  AM took the same image and used the dithering
algorithm that forms part of the same Kurzweil Xerox scanner it
disguised moire patterns much more effectively.

FLEISCHHAUER also observed that dithering produces a binary file which is
useful for numerous purposes, for example, printing it on a laser printer
without having to "rehalftone" it.  But it tends to defeat efficient
compression, because the very thing that dithers to reduce moire patterns
also tends to work against compression schemes.  AM thought the
difference in image quality was worth it.

                                 


DISCUSSION  Relative use as a criterion for POBs selection of books to
be converted into digital form 


During the discussion period, WATERS noted that one of the criteria for
selecting books among the 10,000 to be converted into digital image form
would be how much relative use they would receivea subject still
requiring evaluation.  The challenge will be to understand whether
coherent bodies of material will increase usage or whether POB should
