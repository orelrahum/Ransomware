transmit them quickly enough on a network.

With these full screen megapel images that constitute a third of a
megabyte, one gets 1,0003,000 fullscreen images on a onegigabyte disk
a standard CDROM represents approximately 60 percent of that.  Storing
images the size of a PC screen just 8 bit color increases storage
capacity to 4,00012,000 images per gigabyte 60 percent of that gives
one the size of a CDROM, which in turn creates a major problem.  One
cannot have fullscreen, fullcolor images with lossless compression one
must compress them or use a lower resolution.  For megabytesize images,
anything slower than a T1 speed is impractical.  For example, on a
fiftysixkilobaud line, it takes three minutes to transfer a
onemegabyte file, if it is not compressed and this speed assumes ideal
circumstances no other user contending for network bandwidth.  Thus,
questions of disk access, remote display, and current telephone
connection speed make transmission of megabytesize images impractical.

BESSER then discussed ways to deal with these large images, for example,
compression and decompression at the users end.  In this connection, the
issues of how much one is willing to lose in the compression process and
what image quality one needs in the first place are unknown.  But what is
known is that compression entails some loss of data.  BESSER urged that
more studies be conducted on image quality in different situations, for
example, what kind of images are needed for what kind of disciplines, and
what kind of image quality is needed for a browsing tool, an intermediate
viewing tool, and archiving.

BESSER remarked two promising trends for compression:  from a technical
perspective, algorithms that use what is called subjective redundancy
employ principles from visual psychophysics to identify and remove
information from the image that the human eye cannot perceive from an
interchange and interoperability perspective, the JPEG i.e., Joint
Photographic Experts Group, an ISO standard compression algorithms also
offer promise.  These issues of compression and decompression, BESSER
argued, resembled those raised earlier concerning the design of different
platforms.  Gauging the capabilities of potential users constitutes a
primary goal.  BESSER advocated layering or separating the images from
the applications that retrieve and display them, to avoid tying them to
particular software.

BESSER detailed several lessons learned from his work at Berkeley with
Imagequery, especially the advantages and disadvantages of using
XWindows.  In the latter category, for example, retrieval is tied
directly to ones data, an intolerable situation in the long run on a
networked system.  Finally, BESSER described a project of Jim Wallace at
the Smithsonian Institution, who is mounting images in a extremely
rudimentary way on the Compuserv and Genie networks and is preparing to
mount them on America On Line.  Although the average user takes over
thirty minutes to download these images assuming a fairly fast modem,
nevertheless, images have been downloaded 25,000 times.

BESSER concluded his talk with several comments on the business
arrangement between the Smithsonian and Compuserv.  He contended that not
enough is known concerning the value of images.

                                 


DISCUSSION  Creating digitized photographic collections nearly
impossible except with large organizations like museums  Need for study
to determine quality of images users will tolerate 


During the brief exchange between LESK and BESSER that followed, several
clarifications emerged.
