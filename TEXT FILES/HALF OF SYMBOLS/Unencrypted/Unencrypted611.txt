larger which is likely to be small for a multimedia document.  For
instance, even on a 56 Kb line it would take three minutes to transfer a
1megabyte file.  And these figures assume ideal circumstances, and do
not take into consideration other users contending for network bandwidth,
disk access time, or the time needed for remote display.  Current common
telephone transmission rates would be completely impractical few users
would be willing to wait the hour necessary to transmit a single image at
2400 baud.

This necessitates compression, which itself raises a number of other
issues.  In order to decrease file sizes significantly, we must employ
lossy compression algorithms.  But how much quality can we afford to
lose?  To date there has been only one significant study done of
imagequality needs for a particular user group, and this study did not
look at loss resulting from compression.  Only after identifying
imagequality needs can we begin to address storage and network bandwidth
needs.

Experience with XWindowsbased applications such as Imagequery, the
University of California at Berkeley image database demonstrates the
utility of a clientserver topology, but also points to the limitation of
current software for a distributed environment.  For example,
applications like Imagequery can incorporate compression, but current X
implementations do not permit decompression at the end users
workstation.  Such decompression at the host computer alleviates storage
capacity problems while doing nothing to address problems of
telecommunications bandwidth.

We need to examine the effects on network throughput of moving
multimedia documents around on a network.  We need to examine various
topologies that will help us avoid bottlenecks around servers and
gateways.  Experience with applications such as these raise still broader
questions. How closely is the multimedia document tied to the software
for viewing it?  Can it be accessed and viewed from other applications?
Experience with the MARC format and more recently with the Z39.50
protocols shows how useful it can be to store documents in a form in
which they can be accessed by a variety of application software.

Finally, from an intellectualaccess standpoint, we need to address the
issue of providing access to these multimedia documents in
interdisciplinary environments.  We need to examine terminology and
indexing strategies that will allow us to provide access to this material
in a crossdisciplinary way.

Ronald LARSEN            Directions in HighPerformance Networking for
                         Libraries

The pace at which computing technology has advanced over the past forty
years shows no sign of abating.  Roughly speaking, each fiveyear period
has yielded an orderofmagnitude improvement in price and performance of
computing equipment.  No fundamental hurdles are likely to prevent this
pace from continuing for at least the next decade.  It is only in the
past five years, though, that computing has become ubiquitous in
libraries, affecting all staff and patrons, directly or indirectly.

During these same five years, communications rates on the Internet, the
principal academic computing network, have grown from 56 kbps to 1.5
Mbps, and the NSFNet backbone is now running 45 Mbps.  Over the next five
years, communication rates on the backbone are expected to exceed 1 Gbps.
Growth in both the population of network users and the volume of network
traffic  has continued to grow geometrically, at rates approaching 15
percent per month.  This flood of capacity and use, likened by some to
"drinking from a firehose,"  creates immense opportunities and challenges
for libraries.  Libraries must anticipate the future implications of this
technology, participate in its development, and deploy it to ensure
access to the worlds information resources.

