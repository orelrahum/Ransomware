very nicely.

SPERBERGMcQUEEN conceded that one other model not illustrated on his two
displays was the model of text as a bitmapped image, an image of a page,
and confessed to having been converted to a limited extent by the
Workshop to the view that electronic images constitute a promising,
probably superior alternative to microfilming.  But he was not convinced
that electronic images represent a serious attempt to represent text in
electronic form.  Many of their problems stem from the fact that they are
not direct attempts to represent the text but attempts to represent the
page, thus making them representations of representations.

In this situation of increasingly complicated textual information and the
need to control that complexity in a useful way which begs the question
of the need for good textual grammars, one has the introduction of SGML.
With SGML, one can develop specific documenttype declarations
for specific text types or, as with the TEI, attempts to generate
general documenttype declarations that can handle all sorts of text.
The TEI is an attempt to develop formats for text representation that
will ensure the kind of reusability and longevity of data discussed earlier.
It offers a way to stay alive in the state of permanent technological
revolution.

It has been a continuing challenge in the TEI to create document grammars
that do some work in controlling the complexity of the textual object but
also allowing one to represent the real text that one will find.
Fundamental to the notion of the TEI is that TEI conformance allows one
the ability to extend or modify the TEI tag set so that it fits the text
that one is attempting to represent.

SPERBERGMcQUEEN next outlined the administrative background of the TEI.
The TEI is an international project to develop and disseminate guidelines
for the encoding and interchange of machinereadable text.  It is
sponsored by the Association for Computers in the Humanities, the
Association for Computational Linguistics, and the Association for
Literary and Linguistic Computing.  Representatives of numerous other
professional societies sit on its advisory board.  The TEI has a number
of affiliated projects that have provided assistance by testing drafts of
the guidelines.

Among the design goals for the TEI tag set, the scheme first of all must
meet the needs of research, because the TEI came out of the research
community, which did not feel adequately served by existing tag sets.
The tag set must be extensive as well as compatible with existing and
emerging standards.  In 1990, version 1.0 of the Guidelines was released
SPERBERGMcQUEEN illustrated their contents.

SPERBERGMcQUEEN noted that one problem besetting electronic text has
been the lack of adequate internal or external documentation for many
existing electronic texts.  The TEI guidelines as currently formulated
contain few fixed requirements, but one of them is this:  There must
always be a document header, an infile SGML tag that provides
1 a bibliographic description of the electronic object one is talking
about that is, who included it, when, what for, and under which title
and 2 the copy text from which it was derived, if any.  If there was
no copy text or if the copy text is unknown, then one states as much.
Version 2.0 of the Guidelines was scheduled to be completed in fall 1992
and a revised third version is to be presented to the TEI advisory board
for its endorsement this coming winter.  The TEI itself exists to provide
a markup language, not a markedup text.

Among the challenges the TEI has attempted to face is the need for a
markup language that will work for existing projects, that is, handle the
level of markup that people are using now to tag only chapter, section,
and paragraph divisions and not much else.  At the same time, such a
language also will be able to scale up gracefully to handle the highly
detailed markup which many people foresee as the future destination of
much electronic text, and which is not the future destination but the
present home of numerous electronic texts in specialized areas.

SPERBERGMcQUEEN dismissed the lowestcommondenominator approach as
unable to support the kind of applications that draw people who have
never been in the public library regularly before, and make them come
back.  He advocated more interesting text and more intelligent text.
Asserting that it is not beyond economic feasibility to have good texts,
SPERBERGMcQUEEN noted that the TEI Guidelines listing 200odd tags
contains tags that one is expected to enter every time the relevant
textual feature occurs.  It contains all the tags that people need now,
and it is not expected that everyone will tag things in the same way.

The question of how people will tag the text is in large part a function
of their reaction to what SPERBERGMcQUEEN termed the issue of
reproducibility.  What one needs to be able to reproduce are the things
one wants to work with.  Perhaps a more useful concept than that of
reproducibility or recoverability is that of processability, that is,
what can one get from an electronic text without reading it again
in the original.  He illustrated this contention with a page from
Jan Comeniuss bilingual Introduction to Latin.

SPERBERGMcQUEEN returned at length to the issue of images as simulacra
for the text, in order to reiterate his belief that in the long run more
than images of pages of particular editions of the text are needed,
because just as secondgeneration photocopies and secondgeneration
microfilm degenerate, so secondgeneration representations tend to
degenerate, and one tends to overstress some relatively trivial aspects
of the text such as its layout on the page, which is not always
significant, despite what the text critics might say, and slight other
pieces of information such as the very important lexical ties between the
English and Latin versions of Comeniuss bilingual text, for example.
Moreover, in many crucial respects it is easy to fool oneself concerning
what a scanned image of the text will accomplish.  For example, in order
to study the transmission of texts, information concerning the text
carrier is necessary, which scanned images simply do not always handle.
Further, even the highquality materials being produced at Cornell use
much of the information that one would need if studying those books as
physical objects.  It is a choice that has been made.  It is an arguably
justifiable choice, but one does not know what color those pen strokes in
the margin are or whether there was a stain on the page, because it has
been filtered out.  One does not know whether there were rips in the page
because they do not show up, and on a couple of the marginal marks one
loses half of the mark because the pen is very light and the scanner
failed to pick it up, and so what is clearly a checkmark in the margin of
the original becomes a little scoop in the margin of the facsimile.
Standard problems for facsimile editions, not new to electronics, but
also true of lightlens photography, and are remarked here because it is
important that we not fool ourselves that even if we produce a very nice
image of this page with good contrast, we are not replacing the
manuscript any more than microfilm has replaced the manuscript.

The TEI comes from the research community, where its first allegiance
lies, but it is not just an academic exercise.  It has relevance far
beyond those who spend all of their time studying text, because ones
model of text determines what ones software can do with a text.  Good
models lead to good software.  Bad models lead to bad software.  That has
economic consequences, and it is these economic consequences that have
led the European Community to help support the TEI, and that will lead,
SPERBERGMcQUEEN hoped, some software vendors to realize that if they
provide software with a better model of the text they can make a killing.

                                 


DISCUSSION  Implications of different DTDs and tag sets  ODA versus SGML 


During the discussion that followed, several additional points were made.
Neither AAP i.e., Association of American Publishers nor CALS i.e.,
Computeraided Acquisition and Logistics Support has a documenttype
definition for ancient Greek drama, although the TEI will be able to
handle that.  Given this state of affairs and assuming that the
technicaljournal producers and the commercial vendors decide to use the
other two types, then an institution like the Library of Congress, which
might receive all of their publications, would have to be able to handle
three different types of document definitions and tag sets and be able to
distinguish among them.

Office Document Architecture ODA has some advantages that flow from its
tight focus on office documents and clear directions for implementation.
Much of the ODA standard is easier to read and clearer at first reading
than the SGML standard, which is extremely general.  What that means is
that if one wants to use graphics in TIFF and ODA, one is stuck, because
ODA defines graphics formats while TIFF does not, whereas SGML says the
world is not waiting for this work group to create another graphics format.
What is needed is an ability to use whatever graphics format one wants.

The TEI provides a socket that allows one to connect the SGML document to
the graphics.  The notation that the graphics are in is clearly a choice
that one needs to make based on her or his environment, and that is one
advantage.  SGML is less megalomaniacal in attempting to define formats
for all kinds of information, though more megalomaniacal in attempting to
cover all sorts of documents.  The other advantage is that the model of
text represented by SGML is simply an order of magnitude richer and more
flexible than the model of text offered by ODA.  Both offer hierarchical
structures, but SGML recognizes that the hierarchical model of the text
that one is looking at may not have been in the minds of the designers,
whereas ODA does not.

ODA is not really aiming for the kind of document that the TEI wants to
encompass.  The TEI can handle the kind of material ODA has, as well as a
significantly broader range of material.  ODA seems to be very much
focused on office documents, which is what it started out being called
office document architecture.

                                 


CALALUCA  Textencoding from a publishers perspective 
Responsibilities of a publisher  Reproduction of Mignes Latin series
whole and complete with SGML tags based on perceived need and expected
use  Particular decisions arising from the general decision to produce
and publish PLD 


The final speaker in this session, Eric CALALUCA, vice president,
ChadwyckHealey, Inc., spoke from the perspective of a publisher re
textencoding, rather than as one qualified to discuss methods of
encoding data, and observed that the presenters sitting in the room,
whether they had chosen to or not, were acting as publishers:  making
choices, gathering data, gathering information, and making assessments.
CALALUCA offered the hardwon conviction that in publishing very large
text files such as PLD, one cannot avoid making personal judgments of
appropriateness and structure.

In CALALUCAs view, encoding decisions stem from prior judgments.  Two
notions have become axioms for him in the consideration of future sources
for electronic publication:  1 electronic text publishing is as personal
as any other kind of publishing, and questions of if and how to encode
the data are simply a consequence of that prior decision  2 all
personal decisions are open to criticism, which is unavoidable.

CALALUCA rehearsed his role as a publisher or, better, as an intermediary
between what is viewed as a sound idea and the people who would make use
of it.  Finding the specialist to advise in this process is the core of
that function.  The publisher must monitor and hug the fine line between
giving users what they want and suggesting what they might need.  One
responsibility of a publisher is to represent the desires of scholars and
research librarians as opposed to bullheadedly forcing them into areas
they would not choose to enter.

CALALUCA likened the questions being raised today about data structure
and standards to the decisions faced by the Abbe Migne himself during
production of the Patrologia series in the midnineteenth century.
ChadwyckHealeys decision to reproduce Mignes Latin series whole and
complete with SGML tags was also based upon a perceived need and an
expected use.  In the same way that Mignes work came to be far more than
a simple handbook for clerics, PLD is already far more than a database
for theologians.  It is a bedrock source for the study of Western
civilization, CALALUCA asserted.

In regard to the decision to produce and publish PLD, the editorial board
offered direct judgments on the question of appropriateness of these
texts for conversion, their encoding and their distribution, and
concluded that the best possible project was one that avoided overt
intrusions or exclusions in so important a resource.  Thus, the general
decision to transmit the original collection as clearly as possible with
the widest possible avenues for use led to other decisions:  1 To encode
the data or not, SGML or not, TEI or not.  Again, the expected user
community asserted the need for normative tagging structures of important
humanities texts, and the TEI seemed the most appropriate structure for
that purpose.  Research librarians, who are trained to view the larger
impact of electronic text sources on 80 or 90 or 100 doctoral
disciplines, loudly approved the decision to include tagging.  They see
what is coming better than the specialist who is completely focused on
one edition of Ambroses De Anima, and they also understand that the
potential uses exceed present expectations.  2 What will be tagged and
what will not.  Once again, the board realized that one must tag the
obvious.  But in no way should one attempt to identify through encoding
schemes every single discrete area of a text that might someday be
searched.  That was another decision.  Searching by a column number, an
author, a word, a volume, permitting combination searches, and tagging
notations seemed logical choices as core elements.  3 How does one make
the data available?  Tieing it to a CDROM edition creates limitations,
but a magnetic tape file that is very large, is accompanied by the
encoding specifications, and that allows one to make local modifications
also allows one to incorporate any changes one may desire within the
bounds of private research, though exporting tag files from a CDROM
could serve just as well.  Since no one on the board could possibly
anticipate each and every way in which a scholar might choose to mine
this data bank, it was decided to satisfy the basics and make some
provisions for what might come.  4 Not to encode the database would rob
it of the interchangeability and portability these important texts should
accommodate.  For CALALUCA, the extensive options presented by fulltext
searching require care in text selection and strongly support encoding of
data to facilitate the widest possible search strategies.  Better
software can always be created, but summoning the resources, the people,
and the energy to reconvert the text is another matter.

PLD is being encoded, captured, and distributed, because to
ChadwyckHealey and the board it offers the widest possible array of
future research applications that can be seen today.  CALALUCA concluded
by urging the encoding of all important text sources in whatever way
seems most appropriate and durable at the time, without blanching at the
thought that ones work may require emendation in the future.  Thus,
ChadwyckHealey produced a very large humanities text database before the
final release of the TEI Guidelines.

                                 


DISCUSSION  Creating texts with markup advocated  Trends in encoding 
The TEI and the issue of interchangeability of standards  A
misconception concerning the TEI  Implications for an institution like
LC in the event that a multiplicity of DTDs develops  Producing images
as a first step towards possible conversion to full text through
character recognition  The AAP tag sets as a common starting point and
the need for caution 


HOCKEY prefaced the discussion that followed with several comments in
