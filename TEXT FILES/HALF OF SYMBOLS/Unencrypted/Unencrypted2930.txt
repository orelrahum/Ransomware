forty to one.  Grayscale compression, which primarily uses JPEG, is much
less economical and can represent a lossy compression i.e., not
lossless, so that as one compresses and decompresses, the illustration
is subtly changed.  While binary files produce a highquality printed
version, it appears 1 that other combinations of spatial resolution with
gray andor color hold great promise as well, and 2 that gray scale can
represent a tremendous advantage for onscreen viewing.  The quality
associated with binary and gray scale also depends on the equipment used.
For instance, binary scanning produces a much better copy on a binary
printer.

Among CXPs findings concerning the production of microfilm from digital
files, KENNEY reported that the digital files for the same Reed lecture
were used to produce sample film using an electron beam recorder.  The
resulting film was faithful to the image capture of the digital files,
and while CXP felt that the text and image pages represented in the Reed
lecture were superior to that of the lightlens film, the resolution
readings for the 600 dpi were not as high as standard microfilming.
KENNEY argued that the standards defined for lightlens technology are
not totally transferable to a digital environment.  Moreover, they are
based on definition of quality for a preservation copy.  Although making
this case will prove to be a long, uphill struggle, CXP plans to continue
to investigate the issue over the course of the next year.

KENNEY concluded this portion of her talk with a discussion of the
advantages of creating film:  it can serve as a primary backup and as a
preservation master to the digital file it could then become the print
or production master and service copies could be paper, film, optical
disks, magnetic media, or onscreen display.

Finally, KENNEY presented details re production:

      Development and testing of a moderatelyhigh resolution production
     scanning workstation represented a third goal of CXP to date, 1,000
     volumes have been scanned, or about 300,000 images.

      The resulting digital files are stored and used to produce
     hardcopy replacements for the originals and additional prints on
     demand although the initial costs are high, scanning technology
     offers an affordable means for reformatting brittle material.

      A technician in production mode can scan 300 pages per hour when
     performing singlesheet scanning, which is a necessity when working
     with truly brittle paper this figure is expected to increase
     significantly with subsequent iterations of the software from Xerox
     a threemonth timeandcost study of scanning found that the average
     300page book would take about an hour and forty minutes to scan
     this figure included the time for setup, which involves keying in
     primary bibliographic data, going into quality control mode to
     define page size, establishing fronttoback registration, and
     scanning sample pages to identify a default range of settings for
     the entire bookfunctions not dissimilar to those performed by
     filmers or those preparing a book for photocopy.

      The final step in the scanning process involved rescans, which
     happily were few and far between, representing well under 1 percent
     of the total pages scanned.

In addition to technician time, CXP costed out equipment, amortized over
four years, the cost of storing and refreshing the digital files every
four years, and the cost of printing and binding, bookcloth binding, a
paper reproduction.  The total amounted to a little under 65 per single
300page volume, with 30 percent overhead includeda figure competitive
with the prices currently charged by photocopy vendors.

Of course, with scanning, in addition to the paper facsimile, one is left
with a digital file from which subsequent copies of the book can be
produced for a fraction of the cost of photocopy, with readers afforded
choices in the form of these copies.

KENNEY concluded that digital technology offers an electronic means for a
library preservation effort to pay for itself.  If a brittlebook program
included the means of disseminating reprints of books that are in demand
by libraries and researchers alike, the initial investment in capture
could be recovered and used to preserve additional but less popular
books.  She disclosed that an economic model for a selfsustaining
program could be developed for CXPs report to the Commission on
Preservation and Access CPA.

KENNEY stressed that the focus of CXP has been on obtaining high quality
in a production environment.  The use of digital technology is viewed as
an affordable alternative to other reformatting options.

                                 


ANDRE  Overview and history of NATDP  Various agricultural CDROM
products created inhouse and by service bureaus  Pilot project on
Internet transmission  Additional products in progress 


Pamela ANDRE, associate director for automation, National Agricultural
Text Digitizing Program NATDP, National Agricultural Library NAL,
presented an overview of NATDP, which has been underway at NAL the last
four years, before Judith ZIDAR discussed the technical details.  ANDRE
defined agricultural information as a broad range of material going from
basic and applied research in the hard sciences to the onepage pamphlets
that are distributed by the cooperative state extension services on such
things as how to grow blueberries.

NATDP began in late 1986 with a meeting of representatives from the
landgrant library community to deal with the issue of electronic
information.  NAL and fortyfive of these libraries banded together to
establish this projectto evaluate the technology for converting what
were then source documents in paper form into electronic form, to provide
access to that digital information, and then to distribute it.
Distributing that material to the communitythe university community as
well as the extension service community, potentially down to the county
levelconstituted the groups chief concern.

Since January 1988 when the microcomputerbased scanning system was
installed at NAL, NATDP has done a variety of things, concerning which
ZIDAR would provide further details.  For example, the first technology
considered in the projects discussion phase was digital videodisc, which
indicates how long ago it was conceived.

Over the four years of this project, four separate CDROM products on
four different agricultural topics were created, two at a
scanningandOCR station installed at NAL, and two by service bureaus.
Thus, NATDP has gained comparative information in terms of those relative
costs.  Each of these products contained the full ASCII text as well as
page images of the material, or between 4,000 and 6,000 pages of material
on these disks.  Topics included aquaculture, food, agriculture and
science i.e., international agriculture and research, acid rain, and
Agent Orange, which was the final product distributed approximately
eighteen months before the Workshop.

The third phase of NATDP focused on delivery mechanisms other than
CDROM.  At the suggestion of Clifford LYNCH, who was a technical
consultant to the project at this point, NATDP became involved with the
Internet and initiated a project with the help of North Carolina State
University, in which fourteen of the landgrant university libraries are
transmitting digital images over the Internet in response to interlibrary
loan requestsa topic for another meeting.  At this point, the pilot
project had been completed for about a year and the final report would be
available shortly after the Workshop.  In the meantime, the projects
success had led to its extension.  ANDRE noted that one of the first
things done under the program title was to select a retrieval package to
use with subsequent products Windows Personal Librarian was the package
of choice after a lengthy evaluation.

Three additional products had been planned and were in progress:

     1 An arrangement with the American Society of Agronomya
     professional society that has published the Agronomy Journal since
     about 1908to scan and create bitmapped images of its journal.
     ASA granted permission first to put and then to distribute this
     material in electronic form, to hold it at NAL, and to use these
     electronic images as a mechanism to deliver documents or print out
     material for patrons, among other uses.  Effectively, NAL has the
     right to use this material in support of its program.
     Significantly, this arrangement offers a potential cooperative
     model for working with other professional societies in agriculture
     to try to do the same thingput the journals of particular interest
     to agriculture research into electronic form.

     2 An extension of the earlier product on aquaculture.

     3 The George Washington Carver Papersa joint project with
     Tuskegee University to scan and convert from microfilm some 3,500
     images of Carvers papers, letters, and drawings.

It was anticipated that all of these products would appear no more than
six months after the Workshop.

                                 


ZIDAR  A separate arena for scanning  Steps in creating a database 
Image capture, with and without performing OCR  Keying in tracking data
 Scanning, with electronic and manual tracking  Adjustments during
scanning process  Scanning resolutions  Compression  Deskewing and
filtering  Image capture from microform:  the papers and letters of
George Washington Carver  Equipment used for a scanning system 


Judith ZIDAR, coordinator, National Agricultural Text Digitizing Program
NATDP, National Agricultural Library NAL, illustrated the technical
details of NATDP, including her primary responsibility, scanning and
creating databases on a topic and putting them on CDROM.

ZIDAR remarked a separate arena from the CDROM projects, although the
processing of the material is nearly identical, in which NATDP is also
scanning material and loading it on a Next microcomputer, which in turn
is linked to NALs integrated library system.  Thus, searches in NALs
bibliographic database will enable people to pull up actual page images
and text for any documents that have been entered.

In accordance with the sessions topic, ZIDAR focused her illustrated
talk on image capture, offering a primer on the three main steps in the
process:  1 assemble the printed publications 2 design the database
database design occurs in the process of preparing the material for
scanning this step entails reviewing and organizing the material,
defining the contentswhat will constitute a record, what kinds of
fields will be captured in terms of author, title, etc. 3 perform a
certain amount of markup on the paper publications.  NAL performs this
task record by record, preparing work sheets or some other sort of
tracking material and designing descriptors and other enhancements to be
added to the data that will not be captured from the printed publication.
Part of this process also involves determining NATDPs file and directory
structure:  NATDP attempts to avoid putting more than approximately 100
images in a directory, because placing more than that on a CDROM would
reduce the access speed.

This upfront process takes approximately two weeks for a
6,0007,000page database.  The next step is to capture the page images.
How long this process takes is determined by the decision whether or not
to perform OCR.  Not performing OCR speeds the process, whereas text
capture requires greater care because of the quality of the image:  it
has to be straighter and allowance must be made for text on a page, not
just for the capture of photographs.

NATDP keys in tracking data, that is, a standard bibliographic record
including the title of the book and the title of the chapter, which will
later either become the access information or will be attached to the
front of a fulltext record so that it is searchable.

Images are scanned from a bound or unbound publication, chiefly from
bound publications in the case of NATDP, however, because often they are
the only copies and the publications are returned to the shelves.  NATDP
usually scans one record at a time, because its database tracking system
tracks the document in that way and does not require further logical
separating of the images.  After performing optical character
recognition, NATDP moves the images off the hard disk and maintains a
volume sheet.  Though the system tracks electronically, all the
processing steps are also tracked manually with a log sheet.

ZIDAR next illustrated the kinds of adjustments that one can make when
scanning from paper and microfilm, for example, redoing images that need
special handling, setting for dithering or gray scale, and adjusting for
brightness or for the whole book at one time.

NATDP is scanning at 300 dots per inch, a standard scanning resolution.
Though adequate for capturing text that is all of a standard size, 300
dpi is unsuitable for any kind of photographic material or for very small
text.  Many scanners allow for different image formats, TIFF, of course,
being a de facto standard.  But if one intends to exchange images with
other people, the ability to scan other image formats, even if they are
less common, becomes highly desirable.

CCITT Group 4 is the standard compression for normal blackandwhite
images, JPEG for gray scale or color.   ZIDAR recommended 1 using the
standard compressions, particularly if one attempts to make material
available and to allow users to download images and reuse them from
CDROMs and 2 maintaining the ability to output an uncompressed image,
because in image exchange uncompressed images are more likely to be able
to cross platforms.

ZIDAR emphasized the importance of deskewing and filtering as
requirements on NATDPs upgraded system.  For instance, scanning bound
books, particularly books published by the federal government whose pages
are skewed, and trying to scan them straight if OCR is to be performed,
is extremely timeconsuming.  The same holds for filtering of
poorquality or older materials.

ZIDAR described image capture from microform, using as an example three
reels from a sixtysevenreel set of the papers and letters of George
Washington Carver that had been produced by Tuskegee University.  These
resulted in approximately 3,500 images, which NATDP had had scanned by
its service contractor, Science Applications International Corporation
SAIC.  NATDP also created bibliographic records for access.  NATDP did
not have such specialized equipment as a microfilm scanner.

Unfortunately, the process of scanning from microfilm was not an
unqualified success, ZIDAR reported:  because microfilm frame sizes vary,
occasionally some frames were missed, which without spending much time
and money could not be recaptured.

OCR could not be performed from the scanned images of the frames.  The
bleeding in the text simply output text, when OCR was run, that could not
even be edited.  NATDP tested for negative versus positive images,
landscape versus portrait orientation, and single versus dualpage
microfilm, none of which seemed to affect the quality of the image but
also on none of them could OCR be performed.

In selecting the microfilm they would use, therefore, NATDP had other
factors in mind.  ZIDAR noted two factors that influenced the quality of
the images:  1 the inherent quality of the original and 2 the amount of
size reduction on the pages.

The Carver papers were selected because they are informative and visually
interesting, treat a single subject, and are valuable in their own right.
The images were scanned and divided into logical records by SAIC, then
delivered, and loaded onto NATDPs system, where bibliographic
information taken directly from the images was added.  Scanning was
completed in summer 1991 and by the end of summer 1992 the disk was
scheduled to be published.

Problems encountered during processing included the following:  Because
the microfilm scanning had to be done in a batch, adjustment for
individual page variations was not possible.  The frame size varied on
account of the nature of the material, and therefore some of the frames
were missed while others were just partial frames.  The only way to go
back and capture this material was to print out the page with the
microfilm reader from the missing frame and then scan it in from the
page, which was extremely timeconsuming.  The quality of the images
scanned from the printout of the microfilm compared unfavorably with that
of the original images captured directly from the microfilm.  The
inability to perform OCR also was a major disappointment.  At the time,
computer output microfilm was unavailable to test.

The equipment used for a scanning system was the last topic addressed by
ZIDAR.  The type of equipment that one would purchase for a scanning
system included:  a microcomputer, at least a 386, but preferably a 486
a large hard disk, 380 megabyte at minimum a multitasking operating
system that allows one to run some things in batch in the background
while scanning or doing text editing, for example, Unix or OS2 and,
theoretically, Windows a highspeed scanner and scanning software that
allows one to make the various adjustments mentioned earlier a
highresolution monitor 150 dpi  OCR software and hardware to perform
text recognition an optical disk subsystem on which to archive all the
images as the processing is done file management and tracking software.

ZIDAR opined that the software one purchases was more important than the
hardware and might also cost more than the hardware, but it was likely to
prove critical to the success or failure of ones system.  In addition to
a standalone scanning workstation for image capture, then, text capture
requires one or two editing stations networked to this scanning station
to perform editing.  Editing the text takes two or three times as long as
capturing the images.

Finally, ZIDAR stressed the importance of buying an open system that allows
for more than one vendor, complies with standards, and can be upgraded.

                                 


WATERS Yale University Librarys master plan to convert microfilm to
digital imagery POB  The place of electronic tools in the library of
the future  The uses of images and an image library  Primary input from
preservation microfilm  Features distinguishing POB from CXP and key
hypotheses guiding POB  Use of vendor selection process to facilitate
organizational work  Criteria for selecting vendor  Finalists and
results of process for Yale  Key factor distinguishing vendors 
Components, design principles, and some estimated costs of POB  Role of
preservation materials in developing imaging market  Factors affecting
quality and cost  Factors affecting the usability of complex documents
in image form 


Donald WATERS, head of the Systems Office, Yale University Library,
reported on the progress of a master plan for a project at Yale to
convert microfilm to digital imagery, Project Open Book POB.  Stating
that POB was in an advanced stage of planning, WATERS detailed, in
particular, the process of selecting a vendor partner and several key
issues under discussion as Yale prepares to move into the project itself.
He commented first on the vision that serves as the context of POB and
then described its purpose and scope.

WATERS sees the library of the future not necessarily as an electronic
library but as a place that generates, preserves, and improves for its
clients ready access to both intellectual and physical recorded
knowledge.  Electronic tools must find a place in the library in the
context of this vision.  Several roles for electronic tools include
serving as:  indirect sources of electronic knowledge or as "finding"
aids the online catalogues, the articlelevel indices, registers for
documents and archives direct sources of recorded knowledge fulltext
images and various kinds of compound sources of recorded knowledge the
socalled compound documents of Hypertext, mixed text and image,
mixedtext image format, and multimedia.

POB is looking particularly at images and an image library, the uses to
which images will be put e.g., storage, printing, browsing, and then use
as input for other processes, OCR as a subsequent process to image
capture, or creating an image library, and also possibly generating
microfilm.

While input will come from a variety of sources, POB is considering
especially input from preservation microfilm.  A possible outcome is that
the film and paper which provide the input for the image library
eventually may go off into remote storage, and that the image library may
be the primary access tool.

The purpose and scope of POB focus on imaging.  Though related to CXP,
POB has two features which distinguish it:  1 scaleconversion of
10,000 volumes into digital image form and 2 sourceconversion from
microfilm.  Given these features, several key working hypotheses guide
POB, including:  1 Since POB is using microfilm, it is not concerned with
the image library as a preservation medium.  2 Digital imagery can improve
access to recorded knowledge through printing and network distribution at
a modest incremental cost of microfilm.  3 Capturing and storing documents
in a digital image form is necessary to further improvements in access.
POB distinguishes between the imaging, digitizing process and OCR,
which at this stage it does not plan to perform.

Currently in its first or organizational phase, POB found that it could
use a vendor selection process to facilitate a good deal of the
organizational work e.g., creating a project team and advisory board,
confirming the validity of the plan, establishing the cost of the project
and a budget, selecting the materials to convert, and then raising the
necessary funds.

POB developed numerous selection criteria, including:  a firm committed
to imagedocument management, the ability to serve as systems integrator
in a largescale project over several years, interest in developing the
requisite software as a standard rather than a custom product, and a
willingness to invest substantial resources in the project itself.

Two vendors, DEC and Xerox, were selected as finalists in October 1991,
and with the support of the Commission on Preservation and Access, each
was commissioned to generate a detailed requirements analysis for the
project and then to submit a formal proposal for the completion of the
project, which included a budget and costs. The terms were that POB would
pay the loser.  The results for Yale of involving a vendor included:
broad involvement of Yale staff across the board at a relatively low
cost, which may have longterm significance in carrying out the project
twentyfive to thirty university people are engaged in POB better
understanding of the factors that affect corporate response to markets
for imaging products a competitive proposal and a more sophisticated
view of the imaging markets.

The most important factor that distinguished the vendors under
consideration was their identification with the customer.  The size and
internal complexity of the company also was an important factor.  POB was
looking at large companies that had substantial resources.  In the end,
the process generated for Yale two competitive proposals, with Xeroxs
the clear winner.  WATERS then described the components of the proposal,
the design principles, and some of the costs estimated for the process.

Components are essentially four:  a conversion subsystem, a
networkaccessible storage subsystem for 10,000 books and POB expects
200 to 600 dpi storage, browsing stations distributed on the campus
network, and network access to the image printers.

Among the design principles, POB wanted conversion at the highest
possible resolution.  Assuming TIFF files, TIFF files with Group 4
compression, TCPIP, and ethernet network on campus, POB wanted a
clientserver approach with image documents distributed to the
workstations and made accessible through native workstation interfaces
such as Windows.  POB also insisted on a phased approach to
implementation:  1 a standalone, singleuser, lowcost entry into the
business with a workstation focused on conversion and allowing POB to
explore user access 2 movement into a highervolume conversion with
networkaccessible storage and multiple access stations and 3 a
highvolume conversion, fullcapacity storage, and multiple browsing
stations distributed throughout the campus.

The costs proposed for startup assumed the existence of the Yale network
and its two DocuTech image printers.  Other startup costs are estimated
at 1 million over the three phases.  At the end of the project, the annual
operating costs estimated primarily for the software and hardware proposed
come to about 60,000, but these exclude costs for labor needed in the
conversion process, network and printer usage, and facilities management.

Finally, the selection process produced for Yale a more sophisticated
view of the imaging markets:  the management of complex documents in
image form is not a preservation problem, not a library problem, but a
general problem in a broad, general industry.  Preservation materials are
useful for developing that market because of the qualities of the
material.  For example, much of it is out of copyright.  The resolution
of key issues such as the quality of scanning and image browsing also
will affect development of that market.

The technology is readily available but changing rapidly.  In this
context of rapid change, several factors affect quality and cost, to
which POB intends to pay particular attention, for example, the various
levels of resolution that can be achieved.  POB believes it can bring
resolution up to 600 dpi, but an interpolation process from 400 to 600 is
more likely.  The variation quality in microfilm will prove to be a
highly important factor.  POB may reexamine the standards used to film in
the first place by looking at this process as a followon to microfilming.

Other important factors include:  the techniques available to the
operator for handling material, the ways of integrating quality control
into the digitizing work flow, and a work flow that includes indexing and
storage.  POBs requirement was to be able to deal with quality control
at the point of scanning.  Thus, thanks to Xerox, POB anticipates having
a mechanism which will allow it not only to scan in batch form, but to
review the material as it goes through the scanner and control quality
from the outset.

The standards for measuring quality and costs depend greatly on the uses
of the material, including subsequent OCR, storage, printing, and
browsing.  But especially at issue for POB is the facility for browsing.
This facility, WATERS said, is perhaps the weakest aspect of imaging
technology and the most in need of development.

A variety of factors affect the usability of complex documents in image
form, among them:  1 the ability of the system to handle the full range
of document types, not just monographs but serials, multipart
monographs, and manuscripts 2 the location of the database of record
for bibliographic information about the image document, which POB wants
to enter once and in the most useful place, the online catalog 3 a
document identifier for referencing the bibliographic information in one
place and the images in another 4 the technique for making the basic
internal structure of the document accessible to the reader and finally,
5 the physical presentation on the CRT of those documents.  POB is ready
to complete this phase now.  One last decision involves deciding which
material to scan.

                                 


DISCUSSION  TIFF files constitute de facto standard  NARAs experience
with image conversion software and text conversion  RFC 1314 
Considerable flux concerning available hardware and software solutions 
NAL throughput rate during scanning  Window management questions 


In the questionandanswer period that followed WATERSs presentation,
the following points emerged:

      ZIDARs statement about using TIFF files as a standard meant de
     facto standard.  This is what most people use and typically exchange
     with other groups, across platforms, or even occasionally across
     display software.

      HOLMES commented on the unsuccessful experience of NARA in
     attempting to run imageconversion software or to exchange between
     applications:  What are supposedly TIFF files go into other software
     that is supposed to be able to accept TIFF but cannot recognize the
     format and cannot deal with it, and thus renders the exchange
     useless.  Re text conversion, he noted the different recognition
     rates obtained by substituting the make and model of scanners in
     NARAs recent test of an "intelligent" characterrecognition product
     for a new company.  In the selection of hardware and software,
     HOLMES argued, software no longer constitutes the overriding factor
     it did until about a year ago rather it is perhaps important to
     look at both now.

      Danny Cohen and Alan Katz of the University of Southern California
     Information Sciences Institute began circulating as an Internet RFC
     RFC 1314 about a month ago a standard for a TIFF interchange
     format for Internet distribution of monochrome bitmapped images,
     which LYNCH said he believed would be used as a de facto standard.

      FLEISCHHAUERs impression from hearing these reports and thinking
     about AMs experience was that there is considerable flux concerning
     available hardware and software solutions.  HOOTON agreed and
     commented at the same time on ZIDARs statement that the equipment
     employed affects the results produced.  One cannot draw a complete
     conclusion by saying it is difficult or impossible to perform OCR
     from scanning microfilm, for example, with that device,  that set of
     parameters, and system requirements, because numerous other people
     are accomplishing just that, using other components, perhaps.
     HOOTON opined that both the hardware and the software were highly
     important.  Most of the problems discussed today have been solved in
     numerous different ways by other people.  Though it is good to be
     cognizant of various experiences, this is not to say that it will
     always be thus.

      At NAL, the throughput rate of the scanning process for paper,
     page by page, performing OCR, ranges from 300 to 600 pages per day
     not performing OCR is considerably faster, although how much faster
     is not known.  This is for scanning from bound books, which is much
     slower.

      WATERS commented on window management questions:  DEC proposed an
     XWindows solution which was problematical for two reasons.  One was
     POBs requirement to be able to manipulate images on the workstation
     and bring them down to the workstation itself and the other was
     network usage.

                                 


THOMA  Illustration of deficiencies in scanning and storage process 
Image quality in this process  Different costs entailed by better image
quality  Techniques for overcoming various deficiencies:  fixed
thresholding, dynamic thresholding, dithering, image merge  Page edge
effects 


George THOMA, chief, Communications Engineering Branch, National Library
of Medicine NLM, illustrated several of the deficiencies discussed by
the previous speakers.  He introduced the topic of special problems by
noting the advantages of electronic imaging.  For example, it is regenerable
because it is a coded file, and realtime quality control is possible with
electronic capture, whereas in photographic capture it is not.

One of the difficulties discussed in the scanning and storage process was
image quality which, without belaboring the obvious, means different
things for maps, medical Xrays, or broadcast television.  In the case of
documents, THOMA said, image quality boils down to legibility of the
textual parts, and fidelity in the case of gray or color photo printtype
material.  Legibility boils down to scan density, the standard in most
cases being 300 dpi.  Increasing the resolution with scanners that
perform 600 or 1200 dpi, however, comes at a cost.

Better image quality entails at least four different kinds of costs:  1
equipment costs, because the CCD i.e., chargecouple device with
greater number of elements costs more  2 time costs that translate to
the actual capture costs, because manual labor is involved the time is
also dependent on the fact that more data has to be moved around in the
machine in the scanning or network devices that perform the scanning as
well as the storage  3 media costs, because at high resolutions larger
files have to be stored and 4 transmission costs, because there is just
more data to be transmitted.

But while resolution takes care of the issue of legibility in image
quality, other deficiencies have to do with contrast and elements on the
page scanned or the image that needed to be removed or clarified.  Thus,
THOMA proceeded to illustrate various deficiencies, how they are
manifested, and several techniques to overcome them.

Fixed thresholding was the first technique described, suitable for
blackandwhite text, when the contrast does not vary over the page.  One
can have many different threshold levels in scanning devices.  Thus,
THOMA offered an example of extremely poor contrast, which resulted from
the fact that the stock was a heavy red.  This is the sort of image that
when microfilmed fails to provide any legibility whatsoever.  Fixed
thresholding is the way to change the blacktored contrast to the
desired blacktowhite contrast.

Other examples included material that had been browned or yellowed by
age.  This was also a case of contrast deficiency, and correction was
done by fixed thresholding.  A final example boils down to the same
thing, slight variability, but it is not significant.  Fixed thresholding
solves this problem as well.  The microfilm equivalent is certainly legible,
but it comes with dark areas.  Though THOMA did not have a slide of the
microfilm in this case, he did show the reproduced electronic image.

When one has variable contrast over a page or the lighting over the page
area varies, especially in the case where a bound volume has light
shining on it, the image must be processed by a dynamic thresholding
scheme.  One scheme, dynamic averaging, allows the threshold level not to
be fixed but to be recomputed for every pixel from the neighboring
characteristics.  The neighbors of a pixel determine where the threshold
should be set for that pixel.

THOMA showed an example of a page that had been made deficient by a
variety of techniques, including a burn mark, coffee stains, and a yellow
marker.  Application of a fixedthresholding scheme, THOMA argued, might
take care of several deficiencies on the page but not all of them.
Performing the calculation for a dynamic threshold setting, however,
removes most of the deficiencies so that at least the text is legible.

Another problem is representing a gray level with blackandwhite pixels
by a process known as dithering or electronic screening.  But dithering
does not provide good image quality for pure blackandwhite textual
material.  THOMA illustrated this point with examples. Although its
suitability for photoprint is the reason for electronic screening or
dithering, it cannot be used for every compound image.  In the document
that was distributed by CXP, THOMA noticed that the dithered image of the
IEEE test chart evinced some deterioration in the text.  He presented an
extreme example of deterioration in the text in which compounded
documents had to be set right by other techniques.  The technique
illustrated by the present example was an image merge in which the page
is scanned twice and the settings go from fixed threshold to the
dithering matrix the resulting images are merged to give the best
results with each technique.

THOMA illustrated how dithering is also used in nonphotographic or
nonprint materials with an example of a grayish page from a medical text,
which was reproduced to show all of the gray that appeared in the
original.  Dithering provided a reproduction of all the gray in the
original of another example from the same text.

THOMA finally illustrated the problem of bordering, or pageedge,
effects.  Books and bound volumes that are placed on a photocopy machine
or a scanner produce pageedge effects that are undesirable for two
reasons:  1 the aesthetics of the image after all, if the image is to
be preserved, one does not necessarily want to keep all of its
deficiencies 2 compression with the bordering problem THOMA
illustrated, the compression ratio deteriorated tremendously.  One way
to eliminate this more serious problem is to have the operator at the
point of scanning window the part of the image that is desirable and
automatically turn all of the pixels out of that picture to white.

                                 


FLEISCHHAUER  AMs experience with scanning bound materials  Dithering



Carl FLEISCHHAUER, coordinator, American Memory, Library of Congress,
reported AMs experience with scanning bound materials, which he likened
to the problems involved in using photocopying machines.  Very few
devices in the industry offer bookedge scanning, let alone book cradles.
The problem may be unsolvable, FLEISCHHAUER said, because a large enough
market does not exist for a preservationquality scanner.  AM is using a
Kurzweil scanner, which is a bookedge scanner now sold by Xerox.

Devoting the remainder of his brief presentation to dithering,
FLEISCHHAUER related AMs experience with a contractor who was using
unsophisticated equipment and software to reduce moire patterns from
printed halftones.  AM took the same image and used the dithering
algorithm that forms part of the same Kurzweil Xerox scanner it
disguised moire patterns much more effectively.

FLEISCHHAUER also observed that dithering produces a binary file which is
useful for numerous purposes, for example, printing it on a laser printer
without having to "rehalftone" it.  But it tends to defeat efficient
compression, because the very thing that dithers to reduce moire patterns
also tends to work against compression schemes.  AM thought the
difference in image quality was worth it.

                                 


DISCUSSION  Relative use as a criterion for POBs selection of books to
be converted into digital form 


During the discussion period, WATERS noted that one of the criteria for
selecting books among the 10,000 to be converted into digital image form
would be how much relative use they would receivea subject still
requiring evaluation.  The challenge will be to understand whether
coherent bodies of material will increase usage or whether POB should
seek material that is being used, scan that, and make it more accessible.
POB might decide to digitize materials that are already heavily used, in
order to make them more accessible and decrease wear on them.  Another
approach would be to provide a large body of intellectually coherent
material that may be used more in digital form than it is currently used
in microfilm.  POB would seek material that was out of copyright.

                                 


BARONAS  Origin and scope of AIIM  Types of documents produced in
AIIMs standards program  Domain of AIIMs standardization work  AIIMs
structure  TC 171 and MS23  Electronic image management standards 
Categories of EIM standardization where AIIM standards are being
developed 


Jean BARONAS, senior manager, Department of Standards and Technology,
Association for Information and Image Management AIIM, described the
notforprofit association and the national and international programs
for standardization in which AIIM is active.

Accredited for twentyfive years as the nations standards development
organization for document image management, AIIM began life in a library
community developing microfilm standards.  Today the association
maintains both its library and businessimage management standardization
activitiesand has moved into electronic imagemanagement
standardization EIM.

BARONAS defined the programs scope.  AIIM deals with:  1 the
terminology of standards and of the technology it uses 2 methods of
measurement for the systems, as well as quality 3 methodologies for
users to evaluate and measure quality 4 the features of apparatus used
to manage and edit images and 5 the procedures used to manage images.

BARONAS noted that three types of documents are produced in the AIIM
standards program:  the first two, accredited by the American National
Standards Institute ANSI, are standards and standard recommended
practices.  Recommended practices differ from standards in that they
contain more tutorial information.  A technical report is not an ANSI
standard.  Because AIIMs policies and procedures for developing
standards are approved by ANSI, its standards are labeled ANSIAIIM,
followed by the number and title of the standard.

BARONAS then illustrated the domain of AIIMs standardization work.  For
example, AIIM is the administrator of the U.S. Technical Advisory Group
TAG to the International Standards Organizations ISO technical
committee, TC l7l Micrographics and Optical Memories for Document and
Image Recording, Storage, and Use.  AIIM officially works through ANSI in
the international standardization process.

BARONAS described AIIMs structure, including its board of directors, its
standards board of twelve individuals active in the imagemanagement
industry, its strategic planning and legal admissibility task forces, and
its National Standards Council, which is comprised of the members of a
number of organizations who vote on every AIIM standard before it is
published.  BARONAS pointed out that AIIMs liaisons deal with numerous
other standards developers, including the optical disk community, office
and publishing systems, imagecodesandcharacter set committees, and the
National Information Standards Organization NISO.

BARONAS illustrated the procedures of TC l7l, which covers all aspects of
image management.  When AIIMs national program has conceptualized a new
project, it is usually submitted to the international level, so that the
member countries of TC l7l can simultaneously work on the development of
the standard or the technical report.  BARONAS also illustrated a classic
microfilm standard, MS23, which deals with numerous imaging concepts that
apply to electronic imaging.  Originally developed in the l970s, revised
in the l980s, and revised again in l991, this standard is scheduled for
another revision.  MS23 is an active standard whereby users may propose
new density ranges and new methods of evaluating film images in the
standards revision.

BARONAS detailed several electronic imagemanagement standards, for
instance, ANSIAIIM MS44, a qualitycontrol guideline for scanning 8.5"
by 11" blackandwhite office documents.  This standard is used with the
IEEE fax imagea continuous tone photographic image with gray scales,
text, and several continuous tone picturesand AIIM test target number
2, a representative document used in office document management.

BARONAS next outlined the four categories of EIM standardization in which
AIIM standards are being developed:  transfer and retrieval, evaluation,
optical disc and document scanning applications, and design and
conversion of documents.  She detailed several of the main projects of
each:  1 in the category of image transfer and retrieval, a bilevel
image transfer format, ANSIAIIM MS53, which is a proposed standard that
describes a file header for image transfer between unlike systems when
the images are compressed using G3 and G4 compression 2 the category of
image evaluation, which includes the AIIMproposed TR26 tutorial on image
resolution this technical report will treat the differences and
similarities between classical or photographic and electronic imaging
3 design and conversion, which includes a proposed technical report
called "Forms Design Optimization for EIM" this report considers how
generalpurpose business forms can be best designed so that scanning is
optimized reprographic characteristics such as type, rules, background,
tint, and color will likewise be treated in the technical report 4
disk and document scanning applications includes a project a on planning
platters and disk management, b on generating an application profile for
EIM when images are stored and distributed on CDROM, and c on
evaluating SCSI2, and how a common command set can be generated for SCSI2
so that document scanners are more easily integrated.  ANSIAIIM MS53
will also apply to compressed images.

                                 


BATTIN  The implications of standards for preservation  A major
obstacle to successful cooperation  A hindrance to access in the digital
environment  Standards a doubleedged sword for those concerned with the
preservation of the human record  Nearterm prognosis for reliable
archival standards  Preservation concerns for electronic media  Need
for reconceptualizing our preservation principles  Standards in the real
world and the politics of reproduction  Need to redefine the concept of
archival and to begin to think in terms of life cycles  Cooperation and
the La Guardia Eight  Concerns generated by discussions on the problems
of preserving text and image  General principles to be adopted in a
world without standards 


Patricia BATTIN, president, the Commission on Preservation and Access
CPA, addressed the implications of standards for preservation.  She
listed several areas where the library profession and the analog world of
the printed book had made enormous contributions over the past hundred
yearsfor example, in bibliographic formats, binding standards, and, most
important, in determining what constitutes longevity or archival quality.

Although standards have lightened the preservation burden through the
development of national and international collaborative programs,
nevertheless, a pervasive mistrust of other peoples standards remains a
major obstacle to successful cooperation, BATTIN said.

The zeal to achieve perfection, regardless of the cost, has hindered
rather than facilitated access in some instances, and in the digital
environment, where no real standards exist, has brought an ironically
just reward.

BATTIN argued that standards are a doubleedged sword for those concerned
with the preservation of the human record, that is, the provision of
access to recorded knowledge in a multitude of media as far into the
future as possible.  Standards are essential to facilitate
interconnectivity and access, but, BATTIN said, as LYNCH pointed out
yesterday, if set too soon they can hinder creativity, expansion of
capability, and the broadening of access.  The characteristics of
standards for digital imagery differ radically from those for analog
imagery.  And the nature of digital technology implies continuing
volatility and change.  To reiterate, precipitous standardsetting can
inhibit creativity, but delayed standardsetting results in chaos.

Since in BATTINS opinion the nearterm prognosis for reliable archival
standards, as defined by librarians in the analog world, is poor, two
alternatives remain:  standing pat with the old technology, or
reconceptualizing.

Preservation concerns for electronic media fall into two general domains.
One is the continuing assurance of access to knowledge originally
generated, stored, disseminated, and used in electronic form.  This
domain contains several subdivisions, including 1 the closed,
proprietary systems discussed the previous day, bundled information such
as electronic journals and government agency records, and electronically
produced or captured raw data and 2 the application of digital
technologies to the reformatting of materials originally published on a
deteriorating analog medium such as acid paper or videotape.

The preservation of electronic media requires a reconceptualizing of our
preservation principles during a volatile, standardless transition which
may last far longer than any of us envision today.  BATTIN urged the
necessity of shifting focus from assessing, measuring, and setting
standards for the permanence of the medium to the concept of managing
continuing access to information stored on a variety of media and
requiring a variety of everchanging hardware and software for accessa
fundamental shift for the library profession.

BATTIN offered a primer on how to move forward with reasonable confidence
in a world without standards.  Her comments fell roughly into two sections:
1 standards in the real world and 2 the politics of reproduction.

In regard to realworld standards, BATTIN argued the need to redefine the
concept of archive and to begin to think in terms of life cycles.  In
the past, the naive assumption that paper would last forever produced a
cavalier attitude toward life cycles.  The transient nature of the
electronic media has compelled people to recognize and accept upfront the
concept of life cycles in place of permanency.

Digital standards have to be developed and set in a cooperative context
to ensure efficient exchange of information.  Moreover, during this
transition period, greater flexibility concerning how concepts such as
backup copies and archival copies in the CXP are defined is necessary,
or the opportunity to move forward will be lost.

In terms of cooperation, particularly in the university setting, BATTIN
also argued the need to avoid going off in a hundred different
directions.  The CPA has catalyzed a small group of universities called
the La Guardia Eightbecause La Guardia Airport is where meetings take
placeHarvard, Yale, Cornell, Princeton, Penn State, Tennessee,
Stanford, and USC, to develop a digital preservation consortium to look
at all these issues and develop de facto standards as we move along,
instead of waiting for something that is officially blessed.  Continuing
to apply analog values and definitions of standards to the digital
environment, BATTIN said, will effectively lead to forfeiture of the
benefits of digital technology to research and scholarship.

Under the second rubric, the politics of reproduction, BATTIN reiterated
an oftmade argument concerning the electronic library, namely, that it
is more difficult to transform than to create, and nowhere is that belief
expressed more dramatically than in the conversion of brittle books to
new media.  Preserving information published in electronic media involves
making sure the information remains accessible and that digital
information is not lost through reproduction.  In the analog world of
photocopies and microfilm, the issue of fidelity to the original becomes
paramount, as do issues of "Whose fidelity?" and "Whose original?"

BATTIN elaborated these arguments with a few examples from a recent study
conducted by the CPA on the problems of preserving text and image.
Discussions with scholars, librarians, and curators in a variety of
disciplines dependent on text and image generated a variety of concerns,
for example:  1 Copy what is, not what the technology is capable of.
This is very important for the history of ideas.  Scholars wish to know
what the author saw and worked from.  And make available at the
workstation the opportunity to erase all the defects and enhance the
presentation.  2 The fidelity of reproductionwhat is good enough, what
can we afford, and the difference it makesissues of subjective versus
objective resolution.  3 The differences between primary and secondary
users.  Restricting the definition of primary user to the one in whose
discipline the material has been published runs one headlong into the
reality that these printed books have had a host of other users from a
host of other disciplines, who not only were looking for very different
things, but who also shared values very different from those of the
primary user.  4 The relationship of the standard of reproduction to new
capabilities of scholarshipthe browsing standard versus an archival
standard.  How good must the archival standard be?  Can a distinction be
drawn between potential users in setting standards for reproduction?
Archival storage, use copies, browsing copiesought an attempt to set
standards even be made?  5 Finally, costs.  How much are we prepared to
pay to capture absolute fidelity?  What are the tradeoffs between vastly
enhanced access, degrees of fidelity, and costs?

These standards, BATTIN concluded, serve to complicate further the
reproduction process, and add to the long list of technical standards
that are necessary to ensure widespread access.  Ways to articulate and
analyze the costs that are attached to the different levels of standards
must be found.

Given the chaos concerning standards, which promises to linger for the
foreseeable future, BATTIN urged adoption of the following general
principles:

      Strive to understand the changing information requirements of
     scholarly disciplines as more and more technology is integrated into
     the process of research and scholarly communication in order to meet
     future scholarly needs, not to build for the past.  Capture
     deteriorating information at the highest affordable resolution, even
     though the dissemination and display technologies will lag.

      Develop cooperative mechanisms to foster agreement on protocols
     for document structure and other interchange mechanisms necessary
     for widespread dissemination and use before official standards are
     set.

      Accept that, in a transition period, de facto standards will have
     to be developed.

      Capture information in a way that keeps all options open and
     provides for total convertibility:  OCR, scanning of microfilm,
     producing microfilm from scanned documents, etc.

      Work closely with the generators of information and the builders
     of networks and databases to ensure that continuing accessibility is
     a primary concern from the beginning.

      Piggyback on standards under development for the broad market, and
     avoid libraryspecific standards work with the vendors, in order to
     take advantage of that which is being standardized for the rest of
     the world.

      Concentrate efforts on managing permanence in the digital world,
     rather than perfecting the longevity of a particular medium.

                                 


DISCUSSION  Additional comments on TIFF 


During the brief discussion period that followed BATTINs presentation,
BARONAS explained that TIFF was not developed in collaboration with or
under the auspices of AIIM.  TIFF is a company product, not a standard,
is owned by two corporations, and is always changing.  BARONAS also
observed that ANSIAIIM MS53, a bilevel image file transfer format that
allows unlike systems to exchange images, is compatible with TIFF as well
as with DECs architecture and IBMs MODCAIOCA.

                                 


HOOTON  Several questions to be considered in discussing text conversion



HOOTON introduced the final topic, text conversion, by noting that it is
becoming an increasingly important part of the imaging business.  Many
people now realize that it enhances their system to be able to have more
and more character data as part of their imaging system.  Re the issue of
OCR versus rekeying, HOOTON posed several questions:  How does one get
text into computerreadable form?  Does one use automated processes?
Does one attempt to eliminate the use of operators where possible?
Standards for accuracy, he said, are extremely important:  it makes a
major difference in cost and time whether one sets as a standard 98.5
percent acceptance or 99.5 percent.  He mentioned outsourcing as a
possibility for converting text.  Finally, what one does with the image
to prepare it for the recognition process is also important, he said,
because such preparation changes how recognition is viewed, as well as
facilitates recognition itself.

                                 


LESK  Roles of participants in CORE  Data flow  The scanning process 
The image interface  Results of experiments involving the use of
electronic resources and traditional paper copies  Testing the issue of
serendipity  Conclusions 


Michael LESK, executive director, Computer Science Research, Bell
Communications Research, Inc. Bellcore, discussed the Chemical Online
Retrieval Experiment CORE, a cooperative project involving Cornell
University, OCLC, Bellcore, and the American Chemical Society ACS.

LESK spoke on 1 how the scanning was performed, including the unusual
feature of page segmentation, and 2 the use made of the text and the
image in experiments.

Working with the chemistry journals because ACS has been saving its
typesetting tapes since the mid1970s and thus has a significant backrun
of the most important chemistry journals in the United States, CORE is
attempting to create an automated chemical library.  Approximately a
quarter of the pages by square inch are made up of images of
quasipictorial material dealing with the graphic components of the
pages is extremely important.  LESK described the roles of participants
in CORE:  1 ACS provides copyright permission, journals on paper,
journals on microfilm, and some of the definitions of the files 2 at
Bellcore, LESK chiefly performs the data preparation, while Dennis Egan
performs experiments on the users of chemical abstracts, and supplies the
indexing and numerous magnetic tapes  3 Cornell provides the site of the
experiment 4 OCLC develops retrieval software and other user interfaces.
Various manufacturers and publishers have furnished other help.

Concerning data flow, Bellcore receives microfilm and paper from ACS the
microfilm is scanned by outside vendors, while the paper is scanned
inhouse on an Improvision scanner, twenty pages per minute at 300 dpi,
which provides sufficient quality for all practical uses.  LESK would
prefer to have more gray level, because one of the ACS journals prints on
some colored pages, which creates a problem.

Bellcore performs all this scanning, creates a pageimage file, and also
selects from the pages the graphics, to mix with the text file which is
discussed later in the Workshop.  The user is always searching the ASCII
file, but she or he may see a display based on the ASCII or a display
based on the images.

LESK illustrated how the program performs page analysis, and the image
interface.  The user types several words, is presented with a list
usually of the titles of articles contained in an issuethat derives
from the ASCII, clicks on an icon and receives an image that mirrors an
ACS page.  LESK also illustrated an alternative interface, based on text
on the ASCII, the socalled SuperBook interface from Bellcore.

LESK next presented the results of an experiment conducted by Dennis Egan
and involving thirtysix students at Cornell, one third of them
undergraduate chemistry majors, one third senior undergraduate chemistry
majors, and one third graduate chemistry students.  A third of them
received the paper journals, the traditional paper copies and chemical
abstracts on paper.  A third received image displays of the pictures of
the pages, and a third received the text display with popup graphics.

The students were given several questions made up by some chemistry
professors.  The questions fell into five classes, ranging from very easy
to very difficult, and included questions designed to simulate browsing
as well as a traditional information retrievaltype task.

LESK furnished the following results.  In the straightforward question
searchthe question being, what is the phosphorus oxygen bond distance
and hydroxy phosphate?the students were told that they could take
fifteen minutes and, then, if they wished, give up.  The students with
paper took more than fifteen minutes on average, and yet most of them
gave up.  The students with either electronic format, text or image,
received good scores in reasonable time, hardly ever had to give up, and
usually found the right answer.

In the browsing study, the students were given a list of eight topics,
told to imagine that an issue of the Journal of the American Chemical
Society had just appeared on their desks, and were also told to flip
through it and to find topics mentioned in the issue.  The average scores
were about the same.  The students were told to answer yes or no about
whether or not particular topics appeared.  The errors, however, were
quite different.  The students with paper rarely said that something
appeared when it had not.  But they often failed to find something
actually mentioned in the issue.  The computer people found numerous
things, but they also frequently said that a topic was mentioned when it
was not.  The reason, of course, was that they were performing word
searches.  They were finding that words were mentioned and they were
concluding that they had accomplished their task.

This question also contained a trick to test the issue of serendipity.
The students were given another list of eight topics and instructed,
without taking a second look at the journal, to recall how many of this
new list of eight topics were in this particular issue.  This was an
attempt to see if they performed better at remembering what they were not
looking for.  They all performed about the same, paper or electronics,
about 62 percent accurate.  In short, LESK said, people were not very
good when it came to serendipity, but they were no worse at it with
computers than they were with paper.

LESK gave a parenthetical illustration of the learning curve of students
who used SuperBook.

The students using the electronic systems started off worse than the ones
using print, but by the third of the three sessions in the series had
caught up to print.  As one might expect, electronics provide a much
better means of finding what one wants to read reading speeds, once the
object of the search has been found, are about the same.

Almost none of the students could perform the hard taskthe analogous
transformation.  It would require the expertise of organic chemists to
complete.  But an interesting result was that the students using the text
search performed terribly, while those using the image system did best.
That the text search system is driven by text offers the explanation.
Everything is focused on the text to see the pictures, one must press
on an icon.  Many students found the right article containing the answer
to the question, but they did not click on the icon to bring up the right
figure and see it.  They did not know that they had found the right place,
and thus got it wrong.

The short answer demonstrated by this experiment was that in the event
one does not know what to read, one needs the electronic systems the
electronic systems hold no advantage at the moment if one knows what to
read, but neither do they impose a penalty.

LESK concluded by commenting that, on one hand, the image system was easy
to use.  On the other hand, the text display system, which represented
twenty manyears of work in programming and polishing, was not winning,
because the text was not being read, just searched.  The much easier
system is highly competitive as well as remarkably effective for the
actual chemists.

                                 


ERWAY  Most challenging aspect of working on AM  Assumptions guiding
AMs approach  Testing different types of service bureaus  AMs
requirement for 99.95 percent accuracy  Requirements for textcoding 
Additional factors influencing AMs approach to coding  Results of AMs
experience with rekeying  Other problems in dealing with service bureaus
 Quality control the most timeconsuming aspect of contracting out
conversion  Longterm outlook uncertain 


To Ricky ERWAY, associate coordinator, American Memory, Library of
Congress, the constant variety of conversion projects taking place
simultaneously represented perhaps the most challenging aspect of working
on AM.  Thus, the challenge was not to find a solution for text
conversion but a tool kit of solutions to apply to LCs varied
collections that need to be converted.  ERWAY limited her remarks to the
process of converting text to machinereadable form, and the variety of
LCs text collections, for example, bound volumes, microfilm, and
handwritten manuscripts.

Two assumptions have guided AMs approach, ERWAY said:  1 A desire not
to perform the conversion inhouse.  Because of the variety of formats and
types of texts, to capitalize the equipment and have the talents and
skills to operate them at LC would be extremely expensive.  Further, the
natural inclination to upgrade to newer and better equipment each year
made it reasonable for AM to focus on what it did best and seek external
conversion services.  Using service bureaus also allowed AM to have
several types of operations take place at the same time.  2 AM was not a
technology project, but an effort to improve access to library
collections.  Hence, whether text was converted using OCR or rekeying
mattered little to AM.  What mattered were cost and accuracy of results.

AM considered different types of service bureaus and selected three to
perform several small tests in order to acquire a sense of the field.
The sample collections with which they worked included handwritten
correspondence, typewritten manuscripts from the 1940s, and
eighteenthcentury printed broadsides on microfilm.  On none of these
samples was OCR performed they were all rekeyed.  AM had several special
requirements for the three service bureaus it had engaged.  For instance,
any errors in the original text were to be retained.  Working from bound
volumes or anything that could not be sheetfed also constituted a factor
eliminating companies that would have performed OCR.

AM requires 99.95 percent accuracy, which, though it sounds high, often
means one or two errors per page.  The initial batch of test samples
contained several handwritten materials for which AM did not require
textcoding.  The results, ERWAY reported, were in all cases fairly
comparable:  for the most part, all three service bureaus achieved 99.95
percent accuracy.  AM was satisfied with the work but surprised at the cost.

As AM began converting whole collections, it retained the requirement for
99.95 percent accuracy and added requirements for textcoding.  AM needed
to begin performing work more than three years ago before LC requirements
for SGML applications had been established.  Since AMs goal was simply
to retain any of the intellectual content represented by the formatting
of the document which would be lost if one performed a straight ASCII
conversion, AM used "SGMLlike" codes.  These codes resembled SGML tags
but were used without the benefit of documenttype definitions.  AM found
that many service bureaus were not yet SGMLproficient.

Additional factors influencing the approach AM took with respect to
coding included:  1 the inability of any known microcomputerbased
userretrieval software to take advantage of SGML coding and 2 the
multiple inconsistencies in format of the older documents, which
confirmed AM in its desire not to attempt to force the different formats
to conform to a single documenttype definition DTD and thus create the
need for a separate DTD for each document.

The five text collections that AM has converted or is in the process of
converting include a collection of eighteenthcentury broadsides, a
collection of pamphlets, two typescript document collections, and a
collection of 150 books.

ERWAY next reviewed the results of AMs experience with rekeying, noting
again that because the bulk of AMs materials are historical, the quality
of the text often does not lend itself to OCR.  While nonEnglish
speakers are less likely to guess or elaborate or correct typos in the
original text, they are also less able to infer what we would they also
are nearly incapable of converting handwritten text.  Another
disadvantage of working with overseas keyers is that they are much less
likely to telephone with questions, especially on the coding, with the
result that they develop their own rules as they encounter new
situations.

Government contracting procedures and time frames posed a major challenge
to performing the conversion.  Many service bureaus are not accustomed to
retaining the image, even if they perform OCR.  Thus, questions of image
format and storage media were somewhat novel to many of them.  ERWAY also
remarked other problems in dealing with service bureaus, for example,
their inability to perform text conversion from the kind of microfilm
that LC uses for preservation purposes.

But quality control, in ERWAYs experience, was the most timeconsuming
aspect of contracting out conversion.  AM has been attempting to perform
a 10percent quality review, looking at either every tenth document or
every tenth page to make certain that the service bureaus are maintaining
99.95 percent accuracy.  But even if they are complying with the
requirement for accuracy, finding errors produces a desire to correct
them and, in turn, to clean up the whole collection, which defeats the
purpose to some extent.  Even a double entry requires a
characterbycharacter comparison to the original to meet the accuracy
requirement.  LC is not accustomed to publish imperfect texts, which
makes attempting to deal with the industry standard an emotionally
fraught issue for AM.  As was mentioned in the previous days discussion,
going from 99.95 to 99.99 percent accuracy usually doubles costs and
means a third keying or another complete runthrough of the text.

Although AM has learned much from its experiences with various collections
and various service bureaus, ERWAY concluded pessimistically that no
breakthrough has been achieved.   Incremental improvements have occurred
in some of the OCR technology, some of the processes, and some of the
standards acceptances, which, though they may lead to somewhat lower costs,
do not offer much encouragement to many people who are anxiously awaiting
the day that the entire contents of LC are available online.

                                 


ZIDAR  Several answers to why one attempts to perform fulltext
conversion  Per page cost of performing OCR  Typical problems
encountered during editing  Editing poor copy OCR vs. rekeying 


Judith ZIDAR, coordinator, National Agricultural Text Digitizing Program
NATDP, National Agricultural Library NAL, offered several answers to
the question of why one attempts to perform fulltext conversion:  1
Text in an image can be read by a human but not by a computer, so of
course it is not searchable and there is not much one can do with it.  2
Some material simply requires wordlevel access.  For instance, the legal
profession insists on fulltext access to its material with taxonomic or
geographic material, which entails numerous names, one virtually requires
wordlevel access.  3 Full text permits rapid browsing and searching,
something that cannot be achieved in an image with todays technology.
4 Text stored as ASCII and delivered in ASCII is standardized and highly
portable.  5 People just want fulltext searching, even those who do not
know how to do it.  NAL, for the most part, is performing OCR at an
actual cost per averagesize page of approximately 7.  NAL scans the
page to create the electronic image and passes it through the OCR device.

ZIDAR next rehearsed several typical problems encountered during editing.
Praising the celerity of her student workers, ZIDAR observed that editing
requires approximately five to ten minutes per page, assuming that there
are no large tables to audit.  Confusion among the three characters I, 1,
and l, constitutes perhaps the most common problem encountered.  Zeroes
and  Os also are  frequently confused.  Double Ms create a particular
problem, even on clean pages.  They are so wide in most fonts that they
touch, and the system simply cannot tell where one letter ends and the
other begins.  Complex page formats occasionally fail to columnate
properly, which entails rescanning as though one were working with a
single column, entering the ASCII, and decolumnating for better
searching.  With proportionally spaced text, OCR can have difficulty
discerning what is a space and what are merely spaces between letters, as
opposed to spaces between words, and therefore will merge text or break
up words where it should not.

ZIDAR said that it can often take longer to edit a poorcopy OCR than to
key it from scratch.  NAL has also experimented with partial editing of
text, whereby project workers go into and clean up the format, removing
stray characters but not running a spellcheck.  NAL corrects typos in
the title and authors names, which provides a foothold for searching and
browsing.  Even extremely poorquality OCR e.g., 60percent accuracy
can still be searched, because numerous words are correct, while the
important words are probably repeated often enough that they are likely
to be found correct somewhere.  Librarians, however, cannot tolerate this
situation, though end users seem more willing to use this text for
searching, provided that NAL indicates that it is unedited.  ZIDAR
concluded that rekeying of text may be the best route to take, in spite
of numerous problems with quality control and cost.

                                 


DISCUSSION  Modifying an image before performing OCR  NALs costs per
page AMs costs per page and experience with Federal Prison Industries 
Elements comprising NATDPs costs per page  OCR and structured markup 
Distinction between the structure of a document and its representation
when put on the screen or printed 


HOOTON prefaced the lengthy discussion that followed with several
comments about modifying an image before one reaches the point of
performing OCR.  For example, in regard to an application containing a
significant amount of redundant data, such as formtype data, numerous
companies today are working on various kinds of form renewal, prior to
going through a recognition process, by using dropout colors.  Thus,
acquiring access to form design or using electronic means are worth
considering.  HOOTON also noted that conversion usually makes or breaks
ones imaging system.  It is extremely important, extremely costly in
terms of either capital investment or service, and determines the quality
of the remainder of ones system, because it determines the character of
the raw material used by the system.

Concerning the four projects undertaken by NAL, two inside and two
performed by outside contractors, ZIDAR revealed that an inhouse service
bureau executed the first at a cost between 8 and 10 per page for
everything, including building of the database.  The project undertaken
by the Consultative Group on International Agricultural Research CGIAR
cost approximately 10 per page for the conversion, plus some expenses
for the software and building of the database.  The Acid Rain Projecta
twodisk set produced by the University of Vermont, consisting of
Canadian publications on acid raincost 6.70 per page for everything,
including keying of the text, which was double keyed, scanning of the
images, and building of the database.  The inhouse project offered
considerable ease of convenience and greater control of the process.  On
the other hand, the service bureaus know their job and perform it
expeditiously, because they have more people.

As a useful comparison, ERWAY revealed AMs costs as follows:  0.75
cents to 0.85 cents per thousand characters, with an average page
containing 2,700 characters.  Requirements for coding and imaging
increase the costs.  Thus, conversion of the text, including the coding,
costs approximately 3 per page.  This figure does not include the
imaging and databasebuilding included in the NAL costs.  AM also
enjoyed a happy experience with Federal Prison Industries, which
precluded the necessity of going through the requestforproposal process
to award a contract, because it is another government agency.  The
prisoners performed AMs rekeying just as well as other service bureaus
and proved handy as well.  AM shipped them the books, which they would
photocopy on a bookedge scanner.  They would perform the markup on
photocopies, return the books as soon as they were done with them,
perform the keying, and return the material to AM on WORM disks.

ZIDAR detailed the elements that constitute the previously noted cost of
approximately 7 per page.  Most significant is the editing, correction
of errors, and spellcheckings, which though they may sound easy to
perform require, in fact, a great deal of time.  Reformatting text also
takes a while, but a significant amount of NALs expenses are for equipment,
which was extremely expensive when purchased because it was one of the few
systems on the market.  The costs of equipment are being amortized over
five years but are still quite high, nearly 2,000 per month.

HOCKEY raised a general question concerning OCR and the amount of editing
required substantial in her experience to generate the kind of
structured markup necessary for manipulating the text on the computer or
loading it into any retrieval system.  She wondered if the speakers could
extend the previous question about the costbenefit of adding or exerting
structured markup.  ERWAY noted that several OCR systems retain italics,
bolding, and other spatial formatting.  While the material may not be in
the format desired, these systems possess the ability to remove the
original materials quickly from the hands of the people performing the
conversion, as well as to retain that information so that users can work
with it.  HOCKEY rejoined that the current thinking on markup is that one
should not say that something is italic or bold so much as why it is that
way.  To be sure, one needs to know that something was italicized, but
how can one get from one to the other?  One can map from the structure to
the typographic representation.

FLEISCHHAUER suggested that, given the 100 million items the Library
holds, it may not be possible for LC to do more than report that a thing
was in italics as opposed to why it was italics, although that may be
desirable in some contexts.  Promising to talk a bit during the afternoon
session about several experiments OCLC performed on automatic recognition
of document elements, and which they hoped to extend, WEIBEL said that in
fact one can recognize the major elements of a document with a fairly
high degree of reliability, at least as good as OCR.  STEVENS drew a
useful distinction between standard, generalized markup i.e., defining
for a documenttype definition the structure of the document, and what
he termed a style sheet, which had to do with italics, bolding, and other
forms of emphasis.  Thus, two different components are at work, one being
the structure of the document itself its logic, and the other being its
representation when it is put on the screen or printed.

                                 

SESSION V.  APPROACHES TO PREPARING ELECTRONIC TEXTS


HOCKEY  Text in ASCII and the representation of electronic text versus
an image  The need to look at ways of using markup to assist retrieval 
The need for an encoding format that will be reusable and multifunctional


Susan HOCKEY, director, Center for Electronic Texts in the Humanities
CETH, Rutgers and Princeton Universities, announced that one talk
WEIBELs was moved into this session from the morning and that David
Packard was unable to attend.  The session would attempt to focus more on
what one can do with a text in ASCII and the representation of electronic
text rather than just an image, what one can do with a computer that
cannot be done with a book or an image.  It would be argued that one can
do much more than just read a text, and from that starting point one can
use markup and methods of preparing the text to take full advantage of
the capability of the computer.  That would lead to a discussion of what
the European Community calls REUSABILITY, what may better be termed
DURABILITY, that is, how to prepare or make a text that will last a long
time and that can be used for as many applications as possible, which
would lead to issues of improving intellectual access.

HOCKEY urged the need to look at ways of using markup to facilitate retrieval,
not just for referencing or to help locate an item that is retrieved, but also to put markup tags in
a text to help retrieve the thing sought either with linguistic tagging or
interpretation.  HOCKEY also argued that little advancement had occurred in
the software tools currently available for retrieving and searching text.
She pressed the desideratum of going beyond Boolean searches and performing
more sophisticated searching, which the insertion of more markup in the text
would facilitate.  Thinking about electronic texts as opposed to images means
considering material that will never appear in print form, or print will not
be its primary form, that is, material which only appears in electronic form.
HOCKEY alluded to the history and the need for markup and tagging and
electronic text, which was developed through the use of computers in the
humanities as MICHELSON had observed, Father Busa had started in 1949
to prepare the firstever text on the computer.

HOCKEY remarked several large projects, particularly in Europe, for the
compilation of dictionaries, language studies, and language analysis, in
which people have built up archives of text and have begun to recognize
the need for an encoding format that will be reusable and multifunctional,
that can be used not just to print the text, which may be assumed to be a
byproduct of what one wants to do, but to structure it inside the computer
so that it can be searched, built into a Hypertext system, etc.

                                 


WEIBEL  OCLCs approach to preparing electronic text:  retroconversion,
keying of texts, more automated ways of developing data  Project ADAPT
and the CORE Project  Intelligent character recognition does not exist 
Advantages of SGML  Data should be free of procedural markup
descriptive markup strongly advocated  OCLCs interface illustrated 
Storage requirements and costs for putting a lot of information on line 


Stuart WEIBEL, senior research scientist, Online Computer Library Center,
Inc. OCLC, described OCLCs approach to preparing electronic text.  He
argued that the electronic world into which we are moving must
accommodate not only the future but the past as well, and to some degree
even the present.  Thus, starting out at one end with retroconversion and
keying of texts, one would like to move toward much more automated ways
of developing data.

For example, Project ADAPT had to do with automatically converting
document images into a structured document database with OCR text as
indexing and also a little bit of automatic formatting and tagging of
that text.  The CORE project hosted by Cornell University, Bellcore,
OCLC, the American Chemical Society, and Chemical Abstracts, constitutes
WEIBELs principal concern at the moment.  This project is an example of
converting text for which one already has a machinereadable version into
a format more suitable for electronic delivery and database searching.
Since Michael LESK had previously described CORE, WEIBEL would say
little concerning it.  Borrowing a chemical phrase, de novo synthesis,
WEIBEL cited the Online Journal of Current Clinical Trials as an example
of de novo electronic publishing, that is, a form in which the primary
form of the information is electronic.

Project ADAPT, then, which OCLC completed a couple of years ago and in
fact is about to resume, is a model in which one takes page images either
in paper or microfilm and converts them automatically to a searchable
electronic database, either online or local.  The operating assumption
is that accepting some blemishes in the data, especially for
retroconversion of materials, will make it possible to accomplish more.
Not enough money is available to support perfect conversion.

WEIBEL related several steps taken to perform image preprocessing
processing on the image before performing optical character
recognition, as well as image postprocessing.  He denied the existence
of intelligent character recognition and asserted that what is wanted is
page recognition, which is a long way off.  OCLC has experimented with
merging of multiple optical character recognition systems that will
reduce errors from an unacceptable rate of 5 characters out of every
l,000 to an unacceptable rate of 2 characters out of every l,000, but it
is not good enough.  It will never be perfect.

Concerning the CORE Project, WEIBEL observed that Bellcore is taking the
topography files, extracting the page images, and converting those
topography files to SGML markup.  LESK hands that data off to OCLC, which
builds that data into a Newton database, the same system that underlies
the online system in virtually all of the reference products at OCLC.
The longterm goal is to make the systems interoperable so that not just
Bellcores system and OCLCs system can access this data, but other
systems can as well, and the key to that is the Z39.50 common command
language and the fulltext extension.  Z39.50 is fine for MARC records,
but is not enough to do it for full text that is, make full texts
interoperable.

WEIBEL next outlined the critical role of SGML for a variety of purposes,
for example, as noted by HOCKEY, in the world of extremely large
databases, using highly structured data to perform field searches.
WEIBEL argued that by building the structure of the data in i.e., the
structure of the data originally on a printed page, it becomes easy to
look at a journal article even if one cannot read the characters and know
where the title or author is, or what the sections of that document would be.
OCLC wants to make that structure explicit in the database, because it will
be important for retrieval purposes.

The second big advantage of SGML is that it gives one the ability to
build structure into the database that can be used for display purposes
without contaminating the data with instructions about how to format
things.  The distinction lies between procedural markup, which tells one
where to put dots on the page, and descriptive markup, which describes
the elements of a document.

WEIBEL believes that there should be no procedural markup in the data at
all, that the data should be completely unsullied by information about
italics or boldness.  That should be left up to the display device,
whether that display device is a page printer or a screen display device.
By keeping ones database free of that kind of contamination, one can
make decisions down the road, for example, reorganize the data in ways
that are not cramped by builtin notions of what should be italic and
what should be bold.  WEIBEL strongly advocated descriptive markup.  As
an example, he illustrated the index structure in the CORE data.  With
subsequent illustrated examples of markup, WEIBEL acknowledged the common
complaint that SGML is hard to read in its native form, although markup
decreases considerably once one gets into the body.  Without the markup,
however, one would not have the structure in the data.  One can pass
markup through a LaTeX processor and convert it relatively easily to a
printed version of the document.

WEIBEL next illustrated an extremely cluttered screen dump of OCLCs
system, in order to show as much as possible the inherent capability on
the screen.  He noted parenthetically that he had become a supporter of
XWindows as a result of the progress of the CORE Project.  WEIBEL also
illustrated the two major parts of the interface:  l a control box that
allows one to generate lists of items, which resembles a small table of
contents based on key words one wishes to search, and 2 a document
viewer, which is a separate process in and of itself.  He demonstrated
how to follow links through the electronic database simply by selecting
the appropriate button and bringing them up.  He also noted problems that
remain to be accommodated in the interface e.g., as pointed out by LESK,
what happens when users do not click on the icon for the figure.

Given the constraints of time, WEIBEL omitted a large number of ancillary
items in order to say a few words concerning storage requirements and
what will be required to put a lot of things on line.  Since it is
extremely expensive to reconvert all of this data, especially if it is
just in paper form and even if it is in electronic form in typesetting
tapes, he advocated building journals electronically from the start.  In
that case, if one only has text graphics and indexing which is all that
one needs with de novo electronic publishing, because there is no need to
go back and look at bitmaps of pages, one can get 10,000 journals of
full text, or almost 6 million pages per year.  These pages can be put in
approximately 135 gigabytes of storage, which is not all that much,
WEIBEL said.  For twenty years, something less than three terabytes would
be required.  WEIBEL calculated the costs of storing this information as
follows:  If a gigabyte costs approximately 1,000, then a terabyte costs
approximately 1 million to buy in terms of hardware.  One also needs a
building to put it in and a staff like OCLC to handle that information.
So, to support a terabyte, multiply by five, which gives 5 million per
year for a supported terabyte of data.

                                 


DISCUSSION  Tapes saved by ACS are the typography files originally
supporting publication of the journal  Cost of building tagged text into
the database 


During the questionandanswer period that followed WEIBELs
presentation, these clarifications emerged.  The tapes saved by the
American Chemical Society are the typography files that originally
supported the publication of the journal.  Although they are not tagged
in SGML, they are tagged in very fine detail.  Every single sentence is
marked, all the registry numbers, all the publications issues, dates, and
volumes.  No cost figures on tagging material on a permegabyte basis
were available.  Because ACSs typesetting system runs from tagged text,
there is no extra cost per article.  It was unknown what it costs ACS to
keyboard the tagged text rather than just keyboard the text in the
cheapest process.  In other words, since one intends to publish things
and will need to build tagged text into a typography system in any case,
if one does that in such a way that it can drive not only typography but
an electronic system which is what ACS intends to domove to SGML
publishing, the marginal cost is zero.  The marginal cost represents the
cost of building tagged text into the database, which is small.

                                 


SPERBERGMcQUEEN  Distinction between texts and computers  Implications
of recognizing that all representation is encoding  Dealing with
complicated representations of text entails the need for a grammar of
documents  Variety of forms of formal grammars  Text as a bitmapped
image does not represent a serious attempt to represent text in
electronic form  SGML, the TEI, documenttype declarations, and the
reusability and longevity of data  TEI conformance explicitly allows
extension or modification of the TEI tag set  Administrative background
of the TEI  Several design goals for the TEI tag set  An absolutely
fixed requirement of the TEI Guidelines  Challenges the TEI has
attempted to face  Good texts not beyond economic feasibility  The
issue of reproducibility or processability  The issue of mages as
simulacra for the text redux  Ones model of text determines what ones
software can do with a text and has economic consequences 


Prior to speaking about SGML and markup, Michael SPERBERGMcQUEEN, editor,
Text Encoding Initiative TEI, University of IllinoisChicago, first drew
a distinction between texts and computers:  Texts are abstract cultural
and linguistic objects while computers are complicated physical devices,
he said.  Abstract objects cannot be placed inside physical devices with
computers one can only represent text and act upon those representations.

The recognition that all representation is encoding, SPERBERGMcQUEEN
argued, leads to the recognition of two things:  1 The topic description
for this session is slightly misleading, because there can be no discussion
of pros and cons of textcoding unless what one means is pros and cons of
working with text with computers.  2 No text can be represented in a
computer without some sort of encoding images are one way of encoding text,
ASCII is another, SGML yet another.  There is no encoding without some
information loss, that is, there is no perfect reproduction of a text that
allows one to do away with the original.  Thus, the question becomes,
What is the most useful representation of text for a serious work?
This depends on what kind of serious work one is talking about.

The projects demonstrated the previous day all involved highly complex
information and fairly complex manipulation of the textual material.
In order to use that complicated information, one has to calculate it
slowly or manually and store the result.  It needs to be stored, therefore,
as part of ones representation of the text.  Thus, one needs to store the
structure in the text.  To deal with complicated representations of text,
one needs somehow to control the complexity of the representation of a text
that means one needs a way of finding out whether a document and an
electronic representation of a document is legal or not and that
means one needs a grammar of documents.

SPERBERGMcQUEEN discussed the variety of forms of formal grammars,
implicit and explicit, as applied to text, and their capabilities.  He
argued that these grammars correspond to different models of text that
different developers have.  For example, one implicit model of the text
is that there is no internal structure, but just one thing after another,
a few characters and then perhaps a starttitle command, and then a few
more characters and an endtitle command.  SPERBERGMcQUEEN also
distinguished several kinds of text that have a sort of hierarchical
structure that is not very well defined, which, typically, corresponds
to grammars that are not very well defined, as well as hierarchies that
are very well defined e.g., the Thesaurus Linguae Graecae and extremely
complicated things such as SGML, which handle strictly hierarchical data
very nicely.

SPERBERGMcQUEEN conceded that one other model not illustrated on his two
displays was the model of text as a bitmapped image, an image of a page,
and confessed to having been converted to a limited extent by the
Workshop to the view that electronic images constitute a promising,
probably superior alternative to microfilming.  But he was not convinced
that electronic images represent a serious attempt to represent text in
electronic form.  Many of their problems stem from the fact that they are
not direct attempts to represent the text but attempts to represent the
page, thus making them representations of representations.

In this situation of increasingly complicated textual information and the
need to control that complexity in a useful way which begs the question
of the need for good textual grammars, one has the introduction of SGML.
With SGML, one can develop specific documenttype declarations
for specific text types or, as with the TEI, attempts to generate
general documenttype declarations that can handle all sorts of text.
The TEI is an attempt to develop formats for text representation that
will ensure the kind of reusability and longevity of data discussed earlier.
It offers a way to stay alive in the state of permanent technological
revolution.

It has been a continuing challenge in the TEI to create document grammars
that do some work in controlling the complexity of the textual object but
also allowing one to represent the real text that one will find.
Fundamental to the notion of the TEI is that TEI conformance allows one
the ability to extend or modify the TEI tag set so that it fits the text
that one is attempting to represent.

SPERBERGMcQUEEN next outlined the administrative background of the TEI.
The TEI is an international project to develop and disseminate guidelines
for the encoding and interchange of machinereadable text.  It is
sponsored by the Association for Computers in the Humanities, the
Association for Computational Linguistics, and the Association for
Literary and Linguistic Computing.  Representatives of numerous other
professional societies sit on its advisory board.  The TEI has a number
of affiliated projects that have provided assistance by testing drafts of
the guidelines.

Among the design goals for the TEI tag set, the scheme first of all must
meet the needs of research, because the TEI came out of the research
community, which did not feel adequately served by existing tag sets.
The tag set must be extensive as well as compatible with existing and
emerging standards.  In 1990, version 1.0 of the Guidelines was released
SPERBERGMcQUEEN illustrated their contents.

SPERBERGMcQUEEN noted that one problem besetting electronic text has
been the lack of adequate internal or external documentation for many
existing electronic texts.  The TEI guidelines as currently formulated
contain few fixed requirements, but one of them is this:  There must
always be a document header, an infile SGML tag that provides
1 a bibliographic description of the electronic object one is talking
about that is, who included it, when, what for, and under which title
and 2 the copy text from which it was derived, if any.  If there was
no copy text or if the copy text is unknown, then one states as much.
Version 2.0 of the Guidelines was scheduled to be completed in fall 1992
and a revised third version is to be presented to the TEI advisory board
for its endorsement this coming winter.  The TEI itself exists to provide
a markup language, not a markedup text.

Among the challenges the TEI has attempted to face is the need for a
markup language that will work for existing projects, that is, handle the
level of markup that people are using now to tag only chapter, section,
and paragraph divisions and not much else.  At the same time, such a
language also will be able to scale up gracefully to handle the highly
detailed markup which many people foresee as the future destination of
much electronic text, and which is not the future destination but the
present home of numerous electronic texts in specialized areas.

SPERBERGMcQUEEN dismissed the lowestcommondenominator approach as
unable to support the kind of applications that draw people who have
never been in the public library regularly before, and make them come
back.  He advocated more interesting text and more intelligent text.
Asserting that it is not beyond economic feasibility to have good texts,
SPERBERGMcQUEEN noted that the TEI Guidelines listing 200odd tags
contains tags that one is expected to enter every time the relevant
textual feature occurs.  It contains all the tags that people need now,
and it is not expected that everyone will tag things in the same way.

The question of how people will tag the text is in large part a function
of their reaction to what SPERBERGMcQUEEN termed the issue of
reproducibility.  What one needs to be able to reproduce are the things
one wants to work with.  Perhaps a more useful concept than that of
reproducibility or recoverability is that of processability, that is,
what can one get from an electronic text without reading it again
in the original.  He illustrated this contention with a page from
Jan Comeniuss bilingual Introduction to Latin.

SPERBERGMcQUEEN returned at length to the issue of images as simulacra
for the text, in order to reiterate his belief that in the long run more
than images of pages of particular editions of the text are needed,
because just as secondgeneration photocopies and secondgeneration
microfilm degenerate, so secondgeneration representations tend to
degenerate, and one tends to overstress some relatively trivial aspects
of the text such as its layout on the page, which is not always
significant, despite what the text critics might say, and slight other
pieces of information such as the very important lexical ties between the
English and Latin versions of Comeniuss bilingual text, for example.
Moreover, in many crucial respects it is easy to fool oneself concerning
what a scanned image of the text will accomplish.  For example, in order
to study the transmission of texts, information concerning the text
carrier is necessary, which scanned images simply do not always handle.
Further, even the highquality materials being produced at Cornell use
much of the information that one would need if studying those books as
physical objects.  It is a choice that has been made.  It is an arguably
justifiable choice, but one does not know what color those pen strokes in
the margin are or whether there was a stain on the page, because it has
been filtered out.  One does not know whether there were rips in the page
because they do not show up, and on a couple of the marginal marks one
loses half of the mark because the pen is very light and the scanner
failed to pick it up, and so what is clearly a checkmark in the margin of
the original becomes a little scoop in the margin of the facsimile.
Standard problems for facsimile editions, not new to electronics, but
also true of lightlens photography, and are remarked here because it is
important that we not fool ourselves that even if we produce a very nice
image of this page with good contrast, we are not replacing the
manuscript any more than microfilm has replaced the manuscript.

The TEI comes from the research community, where its first allegiance
lies, but it is not just an academic exercise.  It has relevance far
beyond those who spend all of their time studying text, because ones
model of text determines what ones software can do with a text.  Good
models lead to good software.  Bad models lead to bad software.  That has
economic consequences, and it is these economic consequences that have
led the European Community to help support the TEI, and that will lead,
SPERBERGMcQUEEN hoped, some software vendors to realize that if they
provide software with a better model of the text they can make a killing.

                                 


DISCUSSION  Implications of different DTDs and tag sets  ODA versus SGML 


During the discussion that followed, several additional points were made.
Neither AAP i.e., Association of American Publishers nor CALS i.e.,
Computeraided Acquisition and Logistics Support has a documenttype
definition for ancient Greek drama, although the TEI will be able to
handle that.  Given this state of affairs and assuming that the
technicaljournal producers and the commercial vendors decide to use the
other two types, then an institution like the Library of Congress, which
might receive all of their publications, would have to be able to handle
three different types of document definitions and tag sets and be able to
distinguish among them.

Office Document Architecture ODA has some advantages that flow from its
tight focus on office documents and clear directions for implementation.
Much of the ODA standard is easier to read and clearer at first reading
than the SGML standard, which is extremely general.  What that means is
that if one wants to use graphics in TIFF and ODA, one is stuck, because
ODA defines graphics formats while TIFF does not, whereas SGML says the
world is not waiting for this work group to create another graphics format.
What is needed is an ability to use whatever graphics format one wants.

The TEI provides a socket that allows one to connect the SGML document to
the graphics.  The notation that the graphics are in is clearly a choice
that one needs to make based on her or his environment, and that is one
advantage.  SGML is less megalomaniacal in attempting to define formats
for all kinds of information, though more megalomaniacal in attempting to
cover all sorts of documents.  The other advantage is that the model of
text represented by SGML is simply an order of magnitude richer and more
flexible than the model of text offered by ODA.  Both offer hierarchical
structures, but SGML recognizes that the hierarchical model of the text
that one is looking at may not have been in the minds of the designers,
whereas ODA does not.

ODA is not really aiming for the kind of document that the TEI wants to
encompass.  The TEI can handle the kind of material ODA has, as well as a
significantly broader range of material.  ODA seems to be very much
focused on office documents, which is what it started out being called
office document architecture.

                                 


CALALUCA  Textencoding from a publishers perspective 
Responsibilities of a publisher  Reproduction of Mignes Latin series
whole and complete with SGML tags based on perceived need and expected
use  Particular decisions arising from the general decision to produce
and publish PLD 


The final speaker in this session, Eric CALALUCA, vice president,
ChadwyckHealey, Inc., spoke from the perspective of a publisher re
textencoding, rather than as one qualified to discuss methods of
encoding data, and observed that the presenters sitting in the room,
whether they had chosen to or not, were acting as publishers:  making
choices, gathering data, gathering information, and making assessments.
CALALUCA offered the hardwon conviction that in publishing very large
text files such as PLD, one cannot avoid making personal judgments of
appropriateness and structure.

In CALALUCAs view, encoding decisions stem from prior judgments.  Two
notions have become axioms for him in the consideration of future sources
for electronic publication:  1 electronic text publishing is as personal
as any other kind of publishing, and questions of if and how to encode
the data are simply a consequence of that prior decision  2 all
personal decisions are open to criticism, which is unavoidable.

CALALUCA rehearsed his role as a publisher or, better, as an intermediary
between what is viewed as a sound idea and the people who would make use
of it.  Finding the specialist to advise in this process is the core of
that function.  The publisher must monitor and hug the fine line between
giving users what they want and suggesting what they might need.  One
responsibility of a publisher is to represent the desires of scholars and
research librarians as opposed to bullheadedly forcing them into areas
they would not choose to enter.

CALALUCA likened the questions being raised today about data structure
and standards to the decisions faced by the Abbe Migne himself during
production of the Patrologia series in the midnineteenth century.
ChadwyckHealeys decision to reproduce Mignes Latin series whole and
complete with SGML tags was also based upon a perceived need and an
expected use.  In the same way that Mignes work came to be far more than
a simple handbook for clerics, PLD is already far more than a database
for theologians.  It is a bedrock source for the study of Western
civilization, CALALUCA asserted.

In regard to the decision to produce and publish PLD, the editorial board
offered direct judgments on the question of appropriateness of these
texts for conversion, their encoding and their distribution, and
concluded that the best possible project was one that avoided overt
intrusions or exclusions in so important a resource.  Thus, the general
decision to transmit the original collection as clearly as possible with
the widest possible avenues for use led to other decisions:  1 To encode
the data or not, SGML or not, TEI or not.  Again, the expected user
community asserted the need for normative tagging structures of important
humanities texts, and the TEI seemed the most appropriate structure for
that purpose.  Research librarians, who are trained to view the larger
impact of electronic text sources on 80 or 90 or 100 doctoral
disciplines, loudly approved the decision to include tagging.  They see
what is coming better than the specialist who is completely focused on
one edition of Ambroses De Anima, and they also understand that the
potential uses exceed present expectations.  2 What will be tagged and
what will not.  Once again, the board realized that one must tag the
obvious.  But in no way should one attempt to identify through encoding
schemes every single discrete area of a text that might someday be
searched.  That was another decision.  Searching by a column number, an
author, a word, a volume, permitting combination searches, and tagging
notations seemed logical choices as core elements.  3 How does one make
the data available?  Tieing it to a CDROM edition creates limitations,
but a magnetic tape file that is very large, is accompanied by the
encoding specifications, and that allows one to make local modifications
also allows one to incorporate any changes one may desire within the
bounds of private research, though exporting tag files from a CDROM
could serve just as well.  Since no one on the board could possibly
anticipate each and every way in which a scholar might choose to mine
this data bank, it was decided to satisfy the basics and make some
provisions for what might come.  4 Not to encode the database would rob
it of the interchangeability and portability these important texts should
accommodate.  For CALALUCA, the extensive options presented by fulltext
searching require care in text selection and strongly support encoding of
data to facilitate the widest possible search strategies.  Better
software can always be created, but summoning the resources, the people,
and the energy to reconvert the text is another matter.

PLD is being encoded, captured, and distributed, because to
ChadwyckHealey and the board it offers the widest possible array of
future research applications that can be seen today.  CALALUCA concluded
by urging the encoding of all important text sources in whatever way
seems most appropriate and durable at the time, without blanching at the
thought that ones work may require emendation in the future.  Thus,
ChadwyckHealey produced a very large humanities text database before the
final release of the TEI Guidelines.

                                 


DISCUSSION  Creating texts with markup advocated  Trends in encoding 
The TEI and the issue of interchangeability of standards  A
misconception concerning the TEI  Implications for an institution like
LC in the event that a multiplicity of DTDs develops  Producing images
as a first step towards possible conversion to full text through
character recognition  The AAP tag sets as a common starting point and
the need for caution 


HOCKEY prefaced the discussion that followed with several comments in
favor of creating texts with markup and on trends in encoding.  In the
future, when many more texts are available for online searching, real
problems in finding what is wanted will develop, if one is faced with
millions of words of data.  It therefore becomes important to consider
putting markup in texts to help searchers home in on the actual things
they wish to retrieve.  Various approaches to refining retrieval methods
toward this end include building on a computer version of a dictionary
and letting the computer look up words in it to obtain more information
about the semantic structure or semantic field of a word, its grammatical
structure, and syntactic structure.

HOCKEY commented on the present keen interest in the encoding world
in creating:  1 machinereadable versions of dictionaries that can be
initially tagged in SGML, which gives a structure to the dictionary entry
these entries can then be converted into a more rigid or otherwise
different database structure inside the computer, which can be treated as
a dynamic tool for searching mechanisms 2 large bodies of text to study
the language.  In order to incorporate more sophisticated mechanisms,
more about how words behave needs to be known, which can be learned in
part from information in dictionaries.  However, the last ten years have
seen much interest in studying the structure of printed dictionaries
converted into computerreadable form.  The information one derives about
many words from those is only partial, one or two definitions of the
common or the usual meaning of a word, and then numerous definitions of
unusual usages.  If the computer is using a dictionary to help retrieve
words in a text, it needs much more information about the common usages,
because those are the ones that occur over and over again.  Hence the
current interest in developing large bodies of text in computerreadable
form in order to study the language.  Several projects are engaged in
compiling, for example, 100 million words. HOCKEY described one with
which she was associated briefly at Oxford University involving
compilation of 100 million words of British English:  about 10 percent of
that will contain detailed linguistic tagging encoded in SGML it will
have word class taggings, with words identified as nouns, verbs,
adjectives, or other parts of speech.  This tagging can then be used by
programs which will begin to learn a bit more about the structure of the
language, and then, can go to tag more text.

HOCKEY said that the more that is tagged accurately, the more one can
refine the tagging process and thus the bigger body of text one can build
up with linguistic tagging incorporated into it.  Hence, the more tagging
or annotation there is in the text, the more one may begin to learn about
language and the more it will help accomplish more intelligent OCR.  She
recommended the development of software tools that will help one begin to
understand more about a text, which can then be applied to scanning
images of that text in that format and to using more intelligence to help
one interpret or understand the text.

HOCKEY posited the need to think about common methods of textencoding
for a long time to come, because building these large bodies of text is
extremely expensive and will only be done once.

In the more general discussion on approaches to encoding that followed,
these points were made:

BESSER identified the underlying problem with standards that all have to
struggle with in adopting a standard, namely, the tension between a very
highly defined standard that is very interchangeable but does not work
for everyone because something is lacking, and a standard that is less
defined, more open, more adaptable, but less interchangeable.  Contending
that the way in which people use SGML is not sufficiently defined, BESSER
wondered 1 if people resist the TEI because they think it is too defined
in certain things they do not fit into, and 2 how progress with
interchangeability can be made without frightening people away.

SPERBERGMcQUEEN replied that the published drafts of the TEI had met
with surprisingly little objection on the grounds that they do not allow
one to handle X or Y or Z.  Particular concerns of the affiliated
projects have led, in practice, to discussions of how extensions are to
be made the primary concern of any project has to be how it can be
represented locally, thus making interchange secondary.  The TEI has
received much criticism based on the notion that everything in it is
required or even recommended, which, as it happens, is a misconception
from the beginning,   because none of it is required and very little is
actually actively recommended for all cases, except that one document
ones source.

SPERBERGMcQUEEN agreed with BESSER about this tradeoff:  all the
projects in a set of twenty TEIconformant projects will not necessarily
tag the material in the same way.  One result of the TEI will be that the
easiest problems will be solvedthose dealing with the external form of
the information but the problem that is hardest in interchange is that
one is not encoding what another wants, and vice versa.  Thus, after
the adoption of a common notation, the differences in the underlying
conceptions of what is interesting about texts become more visible.
The success of a standard like the TEI will lie in the ability of
the recipient of interchanged texts to use some of what it contains
and to add the information that was not encoded that one wants, in a
layered way, so that texts can be gradually enriched and one does not
have to put in everything all at once.  Hence, having a wellbehaved
markup scheme is important.

STEVENS followed up on the paradoxical analogy that BESSER alluded to in
the example of the MARC records, namely, the formats that are the same
except that they are different.  STEVENS drew a parallel between
documenttype definitions and MARC records for books and serials and maps,
where one has a tagging structure and there is a textinterchange.
STEVENS opined that the producers of the information will set the terms
for the standard i.e., develop documenttype definitions for the users
of their products, creating a situation that will be problematical for
an institution like the Library of Congress, which will have to deal with
the DTDs in the event that a multiplicity of them develops.  Thus,
numerous people are seeking a standard but cannot find the tag set that
will be acceptable to them and their clients.  SPERBERGMcQUEEN agreed
with this view, and said that the situation was in a way worse:  attempting
to unify arbitrary DTDs resembled attempting to unify a MARC record with a
bibliographic record done according to the Prussian instructions.
According to STEVENS, this situation occurred very early in the process.

WATERS recalled from early discussions on Project Open Book the concern
of many people that merely by producing images, POB was not really
enhancing intellectual access to the material.  Nevertheless, not wishing
to overemphasize the opposition between imaging and full text, WATERS
stated that POB views getting the images as a first step toward possibly
converting to full text through character recognition, if the technology
is appropriate.  WATERS also emphasized that encoding is involved even
with a set of images.

SPERBERGMcQUEEN agreed with WATERS that one can create an SGML document
consisting wholly of images.  At first sight, organizing graphic images
with an SGML document may not seem to offer great advantages, but the
advantages of the scheme WATERS described would be precisely that
ability to move into something that is more of a multimedia document:
a combination of transcribed text and page images.  WEIBEL concurred in
this judgment, offering evidence from Project ADAPT, where a page is
divided into text elements and graphic elements, and in fact the text
elements are organized by columns and lines.  These lines may be used as
the basis for distributing documents in a network environment.  As one
develops software intelligent enough to recognize what those elements
are, it makes sense to apply SGML to an image initially, that may, in
fact, ultimately become more and more text, either through OCR or edited
OCR or even just through keying.  For WATERS, the labor of composing the
document and saying this set of documents or this set of images belongs
to this document constitutes a significant investment.

WEIBEL also made the point that the AAP tag sets, while not excessively
prescriptive, offer a common starting point they do not define the
structure of the documents, though.  They have some recommendations about
DTDs one could use as examples, but they do just suggest tag sets.   For
example, the CORE project attempts to use the AAP markup as much as
possible, but there are clearly areas where structure must be added.
That in no way contradicts the use of AAP tag sets.

SPERBERGMcQUEEN noted that the TEI prepared a long working paper early
on about the AAP tag set and what it lacked that the TEI thought it
needed, and a fairly long critique of the naming conventions, which has
led to a very different style of naming in the TEI.  He stressed the
importance of the opposition between prescriptive markup, the kind that a
publisher or anybody can do when producing documents de novo, and
descriptive markup, in which one has to take what the text carrier
provides.  In these particular tag sets it is easy to overemphasize this
opposition, because the AAP tag set is extremely flexible.  Even if one
just used the DTDs, they allow almost anything to appear almost anywhere.

                                 

SESSION VI.  COPYRIGHT ISSUES


PETERS  Several cautions concerning copyright in an electronic
environment  Review of copyright law in the United States  The notion
of the public good and the desirability of incentives to promote it 
What copyright protects  Works not protected by copyright  The rights
of copyright holders  Publishers concerns in todays electronic
environment  Compulsory licenses  The price of copyright in a digital
medium and the need for cooperation  Additional clarifications   Rough
justice oftentimes the outcome in numerous copyright matters  Copyright
in an electronic society  Copyright law always only sets up the
boundaries anything can be changed by contract 


Marybeth PETERS, policy planning adviser to the Register of Copyrights,
Library of Congress,   made several general comments and then opened the
floor to discussion of subjects of interest to the audience.

Having attended several sessions in an effort to gain a sense of what
people did and where copyright would affect their lives, PETERS expressed
the following cautions:

      If one takes and converts materials and puts them in new forms,
     then, from a copyright point of view, one is creating something and
     will receive some rights.

      However, if what one is converting already exists, a question
     immediately arises about the status of the materials in question.

      Putting something in the public domain in the United States offers
     some freedom from anxiety, but distributing it throughout the world
     on a network is another matter, even if one has put it in the public
     domain in the United States.  Re foreign laws, very frequently a
     work can be in the public domain in the United States but protected
     in other countries.  Thus, one must consider all of the places a
     work may reach, lest one unwittingly become liable to being faced
     with a suit for copyright infringement, or at least a letter
     demanding discussion of what one is doing.

PETERS reviewed copyright law in the United States.  The U.S.
Constitution effectively states that Congress has the power to enact
copyright laws for two purposes:  1 to encourage the creation and
dissemination of intellectual works for the good of society as a whole
and, significantly, 2 to give creators and those who package and
disseminate materials the economic rewards that are due them.

Congress strives to strike a balance, which at times can become an
emotional issue.  The United States has never accepted the notion of the
natural right of an author so much as it has accepted the notion of the
public good and the desirability of incentives to promote it.  This state
of affairs, however, has created strains on the international level and
is the reason for several of the differences in the laws that we have.
Today the United States protects almost every kind of work that can be
called an expression of an author.  The standard for gaining copyright
protection is simply originality.  This is a low standard and means that
a work is not copied from something else, as well as shows a certain
minimal amount of authorship.  One can also acquire copyright protection
for making a new version of preexisting material, provided it manifests
some spark of creativity.

However, copyright does not protect ideas, methods, systemsonly the way
that one expresses those things.  Nor does copyright protect anything
that is mechanical, anything that does not involve choice, or criteria
concerning whether or not one should do a thing.  For example, the
results of a process called declicking, in which one mechanically removes
impure sounds from old recordings, are not copyrightable.  On the other
hand, the choice to record a song digitally and to increase the sound of
violins or to bring up the tympani constitutes the results of conversion
that are copyrightable.  Moreover, if a work is protected by copyright in
the United States, one generally needs the permission of the copyright
owner to convert it.  Normally, who will own the newthat is, converted
material is a matter of contract.  In the absence of a contract, the
person who creates the new material is the author and owner.  But people
do not generally think about the copyright implications until after the
fact.  PETERS stressed the need when dealing with copyrighted works to
think about copyright in advance.  Ones bargaining power is much greater
up front than it is down the road.

PETERS next discussed works not protected by copyright, for example, any
work done by a federal employee as part of his or her official duties is
in the public domain in the United States.  The issue is not wholly free
of doubt concerning whether or not the work is in the public domain
outside the United States.  Other materials in the public domain include:
any works published more than seventyfive years ago, and any work
published in the United States more than twentyeight years ago, whose
copyright was not renewed.  In talking about the new technology and
putting material in a digital form to send all over the world, PETERS
cautioned, one must keep in mind that while the rights may not be an
issue in the United States, they may be in different parts of the world,
where most countries previously employed a copyright term of the life of
the author plus fifty years.

PETERS next reviewed the economics of copyright holding.  Simply,
economic rights are the rights to control the reproduction of a work in
any form.  They belong to the author, or in the case of a work made for
hire, the employer.  The second right, which is critical to conversion,
is the right to change a work.  The right to make new versions is perhaps
one of the most significant rights of authors, particularly in an
electronic world.  The third right is the right to publish the work and
the right to disseminate it, something that everyone who deals in an
electronic medium needs to know.  The basic rule is if a copy is sold,
all rights of distribution are extinguished with the sale of that copy.
The key is that it must be sold.  A number of companies overcome this
obstacle by leasing or renting their product.  These companies argue that
if the material is rented or leased and not sold, they control the uses
of a work.  The fourth right, and one very important in a digital world,
is a right of public performance, which means the right to show the work
sequentially.  For example, copyright owners control the showing of a
CDROM product in a public place such as a public library.  The reverse
side of public performance is something called the right of public
display.  Moral rights also exist, which at the federal level apply only
to very limited visual works of art, but in theory may apply under
contract and other principles.  Moral rights may include the right of an
author to have his or her name on a work, the right of attribution, and
the right to object to distortion or mutilationthe right of integrity.

The way copyright law is worded gives much latitude to activities such as
preservation to use of material for scholarly and research purposes when
the user does not make multiple copies and to the generation of
facsimile copies of unpublished works by libraries for themselves and
other libraries.  But the law does not allow anyone to become the
distributor of the product for the entire world.  In todays electronic
environment, publishers are extremely concerned that the entire world is
networked and can obtain the information desired from a single copy in a
single library.  Hence, if there is to be only one sale, which publishers
may choose to live with, they will obtain their money in other ways, for
example, from access and use.  Hence, the development of site licenses
and other kinds of agreements to cover what publishers believe they
should be compensated for.  Any solution that the United States takes
today has to consider the international arena.

Noting that the United States is a member of the Berne Convention and
subscribes to its provisions, PETERS described the permissions process.
She also defined compulsory licenses.  A compulsory license, of which the
United States has had a few, builds into the law the right to use a work
subject to certain terms and conditions.  In the international arena,
however, the ability to use compulsory licenses is extremely limited.
Thus, clearinghouses and other collectives comprise one option that has
succeeded in providing for use of a work.  Often overlooked when one
begins to use copyrighted material and put products together is how
expensive the permissions process and managing it is.  According to
PETERS, the price of copyright in a digital medium, whatever solution is
worked out, will include managing and assembling the database.  She
strongly recommended that publishers and librarians or people with
various backgrounds cooperate to work out administratively feasible
systems, in order to produce better results.

In the lengthy questionandanswer period that followed PETERSs
presentation, the following points emerged:

      The Copyright Office maintains that anything mechanical and
     totally exhaustive probably is not protected.  In the event that
     what an individual did in developing potentially copyrightable
     material is not understood, the Copyright Office will ask about the
     creative choices the applicant chose to make or not to make.  As a
     practical matter, if one believes she or he has made enough of those
     choices, that person has a right to assert a copyright and someone
     else must assert that the work is not copyrightable.  The more
     mechanical, the more automatic, a thing is, the less likely it is to
     be copyrightable.

      Nearly all photographs are deemed to be copyrightable, but no one
     worries about them much, because everyone is free to take the same
     image.  Thus, a photographic copyright represents what is called a
     "thin" copyright.  The photograph itself must be duplicated, in
     order for copyright to be violated.

      The Copyright Office takes the position that Xrays are not
     copyrightable because they are mechanical.  It  can be argued
     whether or not image enhancement in scanning can be protected.  One
     must exercise care with material created with public funds and
     generally in the public domain.  An article written by a federal
     employee, if written as part of official duties, is not
     copyrightable.  However, control over a scientific article written
     by a National Institutes of Health grantee i.e., someone who
     receives money from the U.S. government, depends on NIH policy.  If
     the government agency has no policy and that policy can be
     contained in its regulations, the contract, or the grant, the
     author retains copyright.  If a provision of the contract, grant, or
     regulation states that there will be no copyright, then it does not
     exist.  When a work is created, copyright automatically comes into
     existence unless something exists that says it does not.

      An enhanced electronic copy of a print copy of an older reference
     work in the public domain that does not contain copyrightable new
     material is a purely mechanical rendition of the original work, and
     is not copyrightable.

      Usually, when a work enters the public domain, nothing can remove
     it.  For example, Congress recently passed into law the concept of
     automatic renewal, which means that copyright on any work published
     between l964 and l978 does not have to be renewed in order to
     receive a seventyfiveyear term.  But any work not renewed before
     1964 is in the public domain.

      Concerning whether or not the United States keeps track of when
     authors die, nothing was ever done, nor is anything being done at
     the moment by the Copyright Office.

      Software that drives a mechanical process is itself copyrightable.
     If one changes platforms, the software itself has a copyright.  The
     World Intellectual Property Organization will hold a symposium 28
     March through 2 April l993, at Harvard University, on digital
     technology, and will study this entire issue.  If one purchases a
     computer software package, such as MacPaint, and creates something
     new, one receives protection only for that which has been added.

PETERS added that often in copyright matters, rough justice is the
outcome, for example, in collective licensing, ASCAP i.e., American
Society of Composers, Authors, and Publishers, and BMI i.e., Broadcast
Music, Inc., where it may seem that the big guys receive more than their
due.  Of course, people ought not to copy a creative product without
paying for it there should be some compensation.  But the truth of the
world, and it is not a great truth, is that the big guy gets played on
the radio more frequently than the little guy, who has to do much more
until he becomes a big guy.  That is true of every author, every
composer, everyone, and, unfortunately, is part of life.

Copyright always originates with the author, except in cases of works
made for hire.  Most software falls into this category.  When an author
sends his article to a journal, he has not relinquished copyright, though
he retains the right to relinquish it.  The author receives absolutely
everything.  The less prominent the author, the more leverage the
publisher will have in contract negotiations.  In order to transfer the
rights, the author must sign an agreement giving them away.

In an electronic society, it is important to be able to license a writer
and work out deals.  With regard to use of a work, it usually is much
easier when a publisher holds the rights.  In an electronic era, a real
problem arises when one is digitizing and making information available.
PETERS referred again to electronic licensing clearinghouses.  Copyright
ought to remain with the author, but as one moves forward globally in the
electronic arena, a middleman who can handle the various rights becomes
increasingly necessary.

The notion of copyright law is that it resides with the individual, but
in an online environment, where a work can be adapted and tinkered with
by many individuals, there is concern.  If changes are authorized and
there is no agreement to the contrary, the person who changes a work owns
the changes.  To put it another way, the person who acquires permission
to change a work technically will become the author and the owner, unless
some agreement to the contrary has been made.  It is typical for the
original publisher to try to control all of the versions and all of the
uses.  Copyright law always only sets up the boundaries.  Anything can be
changed by contract.

                                 

SESSION VII.  CONCLUSION


GENERAL DISCUSSION  Two questions for discussion  Different emphases in
the Workshop  Bringing the text and image partisans together 
Desiderata in planning the longterm development of something  Questions
surrounding the issue of electronic deposit  Discussion of electronic
deposit as an allusion to the issue of standards  Need for a directory
of preservation projects in digital form and for access to their
digitized files  CETHs catalogue of machinereadable texts in the
humanities  What constitutes a publication in the electronic world? 
Need for LC to deal with the concept of online publishing  LCs Network
Development Office  exploring the limits of MARC as a standard in terms
of handling electronic information  Magnitude of the problem and the
need for distributed responsibility in order to maintain and store
electronic information  Workshop participants to be viewed as a starting
point  Development of a network version of AM urged  A step toward AMs
construction of some sort of apparatus for network access  A delicate
and agonizing policy question for LC  Re the issue of electronic
deposit, LC urged to initiate a catalytic process in terms of distributed
responsibility  Suggestions for cooperative ventures  Commercial
publishers fears  Strategic questions for getting the image and text
people to think through longterm cooperation  Clarification of the
driving force behind both the Perseus and the Cornell Xerox projects 


In his role as moderator of the concluding session, GIFFORD raised two
questions he believed would benefit from discussion:  1 Are there enough
commonalities among those of us that have been here for two days so that
we can see courses of action that should be taken in the future?  And, if
so, what are they and who might take them?  2 Partly derivative from
that, but obviously very dangerous to LC as host, do you see a role for
the Library of Congress in all this?  Of course, the Library of Congress
holds a rather special status in a number of these matters, because it is
not perceived as a player with an economic stake in them, but are there
roles that LC can play that can help advance us toward where we are heading?

Describing himself as an uninformed observer of the technicalities of the
last two days, GIFFORD detected three different emphases in the Workshop:
1 people who are very deeply committed to text 2 people who are almost
passionate about images and 3 a few people who are very committed to
what happens to the networks.  In other words, the new networking
dimension, the accessibility of the processability, the portability of
all this across the networks.  How do we pull those three together?

Adding a question that reflected HOCKEYs comment that this was the
fourth workshop she had attended in the previous thirty days, FLEISCHHAUER
wondered to what extent this meeting had reinvented the wheel, or if it
had contributed anything in the way of bringing together a different group
of people from those who normally appear on the workshop circuit.

HOCKEY confessed to being struck at this meeting and the one the
Electronic Pierce Consortium organized the previous week that this was a
coming together of people working on texts and not images.  Attempting to
bring the two together is something we ought to be thinking about for the
future:  How one can think about working with image material to begin
with, but structuring it and digitizing it in such a way that at a later
stage it can be interpreted into text, and find a common way of building
text and images together so that they can be used jointly in the future,
with the network support to begin there because that is how people will
want to access it.

In planning the longterm development of something, which is what is
being done in electronic text, HOCKEY stressed the importance not only
of discussing the technical aspects of how one does it but particularly
of thinking about what the people who use the stuff will want to do.
But conversely, there are numerous things that people start to do with
electronic text or material that nobody ever thought of in the beginning.

LESK, in response to the question concerning the role of the Library of
Congress, remarked the often suggested desideratum of having electronic
deposit:  Since everything is now computertypeset, an entire decade of
material that was machinereadable exists, but the publishers frequently
did not save it has LC taken any action to have its copyright deposit
operation start collecting these machinereadable versions?  In the
absence of PETERS, GIFFORD replied that the question was being
actively considered but that that was only one dimension of the problem.
Another dimension is the whole question of the integrity of the original
electronic document.  It becomes highly important in science to prove
authorship.  How will that be done?

ERWAY explained that, under the old policy, to make a claim for a
copyright for works that were published in electronic form, including
software, one had to submit a paper copy of the first and last twenty
pages of codesomething that represented the work but did not include
the entire work itself and had little value to anyone.  As a temporary
measure, LC has claimed the right to demand electronic versions of
electronic publications.  This measure entails a proactive role for the
Library to say that it wants a particular electronic version.  Publishers
then have perhaps a year to submit it.  But the real problem for LC is
what to do with all this material in all these different formats.  Will
the Library mount it?  How will it give people access to it?  How does LC
keep track of the appropriate computers, software, and media?  The situation
is so hard to control, ERWAY said, that it makes sense for each publishing
house to maintain its own archive.  But LC cannot enforce that either.

GIFFORD acknowledged LESKs suggestion that establishing a priority
offered the solution, albeit a fairly complicated one.  But who maintains
that register?, he asked.  GRABER noted that LC does attempt to collect a
Macintosh version and the IBMcompatible version of software.  It does
not collect other versions.  But while true for software, BYRUM observed,
this reply does not speak to materials, that is, all the materials that
were published that were on somebodys microcomputer or driver tapes
at a publishing office across the country.  LC does well to acquire
specific machinereadable products selectively that were intended to be
machinereadable.  Materials that were in machinereadable form at one time,
BYRUM said, would be beyond LCs capability at the moment, insofar as
attempting to acquire, organize, and preserve them are concernedand
preservation would be the most important consideration.  In this
connection, GIFFORD reiterated the need to work out some sense of
distributive responsibility for a number of these issues, which
inevitably will require significant cooperation and discussion.
Nobody can do it all.

LESK suggested that some publishers may look with favor on LC beginning
to serve as a depository of tapes in an electronic manuscript standard.
Publishers may view this as a service that they did not have to perform
and they might send in tapes.  However, SPERBERGMcQUEEN countered,
although publishers have had equivalent services available to them for a
long time, the electronic text archive has never turned away or been
flooded with tapes and is forever sending feedback to the depositor.
Some publishers do send in tapes.

ANDRE viewed this discussion as an allusion to the issue of standards.
She recommended that the AAP standard and the TEI, which has already been
somewhat harmonized internationally and which also shares several
compatibilities with the AAP, be harmonized to ensure sufficient
compatibility in the software.  She drew the line at saying LC ought to
be the locus or forum for such harmonization.

Taking the group in a slightly different direction, but one where at
least in the near term LC might play a helpful role, LYNCH remarked the
plans of a number of projects to carry out preservation by creating
digital images that will end up in online or nearline storage at some
institution.   Presumably, LC will link this material somehow to its
online catalog in most cases.  Thus, it is in a digital form.  LYNCH had
the impression that many of these institutions would be willing to make
those files accessible to other people outside the institution, provided
that there is no copyright problem.  This desideratum will require
propagating the knowledge that those digitized files exist, so that they
can end up in other online catalogs.  Although uncertain about the
mechanism for achieving this result, LYNCH said that it warranted
scrutiny because it seemed to be connected to some of the basic issues of
cataloging and distribution of records.  It would be  foolish, given the
amount of work that all of us have to do and our meager resources, to
discover multiple institutions digitizing the same work.  Re microforms,
LYNCH said, we are in pretty good shape.

BATTIN called this a big problem and noted that the Cornell people who
had already departed were working on it.  At issue from the beginning
was to learn how to catalog that information into RLIN and then into
OCLC, so that it would be accessible.  That issue remains to be resolved.
LYNCH rejoined that putting it into OCLC or RLIN was helpful insofar as
somebody who is thinking of performing preservation activity on that work
could learn about it.  It is not necessarily helpful for institutions to
make that available.  BATTIN opined that the idea was that it not only be
for preservation purposes but for the convenience of people looking for
this material.  She endorsed LYNCHs dictum that duplication of this
effort was to be avoided by every means.

HOCKEY informed the Workshop about one major current activity of CETH,
namely a catalogue of machinereadable texts in the humanities.  Held on
RLIN at present, the catalogue has been concentrated on ASCII as opposed
to digitized images of text.  She is exploring ways to improve the
catalogue and make it more widely available, and welcomed suggestions
about these concerns.  CETH owns the records, which are not just
restricted to RLIN, and can distribute them however it wishes.

Taking up LESKs earlier question, BATTIN inquired whether LC, since it
is accepting electronic files and designing a mechanism for dealing with
that rather than putting books on shelves, would become responsible for
the National Copyright Depository of Electronic Materials.  Of course
that could not be accomplished overnight, but it would be something LC
could plan for.  GIFFORD acknowledged that much thought was being devoted
to that set of problems and returned the discussion to the issue raised
by LYNCHwhether or not putting the kind of records that both BATTIN and
HOCKEY have been talking about in RLIN is not a satisfactory solution.
It seemed to him that RLIN answered LYNCHs original point concerning
some kind of directory for these kinds of materials.  In a situation
where somebody is attempting to decide whether or not to scan this or
film that or to learn whether or not someone has already done so, LYNCH
suggested, RLIN is helpful, but it is not helpful in the case of a local,
online catalogue.  Further, one would like to have her or his system be
aware that that exists in digital form, so that one can present it to a
patron, even though one did not digitize it, if it is out of copyright.
The only way to make those linkages would be to perform a tremendous
amount of realtime lookup, which would be awkward at best, or
periodically to yank the whole file from RLIN and match it against ones
own stuff, which is a nuisance.

But where, ERWAY inquired, does one stop including things that are
available with Internet, for instance, in ones local catalogue?
It almost seems that that is LCs means to acquire access to them.
That represents LCs new form of library loan.  Perhaps LCs new online
catalogue is an amalgamation of all these catalogues on line.  LYNCH
conceded that perhaps that was true in the very long term, but was not
applicable to scanning in the short term.  In his view, the totals cited
by Yale, 10,000 books over perhaps a fouryear period, and 1,0001,500
books from Cornell, were not big numbers, while searching all over
creation for relatively rare occurrences will prove to be less efficient.
As GIFFORD wondered if this would not be a separable file on RLIN and
could be requested from them, BATTIN interjected that it was easily
accessible to an institution.  SEVERTSON pointed out that that file, cum
enhancements, was available with reference information on CDROM, which
makes it a little more available.

In HOCKEYs view, the real question facing the Workshop is what to put in
this catalogue, because that raises the question of what constitutes a
publication in the electronic world.  WEIBEL interjected that Eric Joule
in OCLCs Office of Research is also wrestling with this particular
problem, while GIFFORD thought it sounded fairly generic.  HOCKEY
contended that a majority of texts in the humanities are in the hands
of either a small number of large research institutions or individuals
and are not generally available for anyone else to access at all.
She wondered if these texts ought to be catalogued.

After argument proceeded back and forth for several minutes over why
cataloguing might be a necessary service, LEBRON suggested that this
issue involved the responsibility of a publisher.  The fact that someone
has created something electronically and keeps it under his or her
control does not constitute publication.  Publication implies
dissemination.  While it would be important for a scholar to let other
people know that this creation exists, in many respects this is no
different from an unpublished manuscript.  That is what is being accessed
in there, except that now one is not looking at it in the hardcopy but
in the electronic environment.

LEBRON expressed puzzlement at the variety of ways electronic publishing
has been viewed.  Much of what has been discussed throughout these two
days has concerned CDROM publishing, whereas in the online environment
that she confronts, the constraints and challenges are very different.
Sooner or later LC will have to deal with the concept of online
publishing.  Taking up the comment ERWAY made earlier about storing
copies, LEBRON gave her own journal as an example.  How would she deposit
OJCCT for copyright?, she asked, because the journal will exist in the
mainframe at OCLC and people will be able to access it.  Here the
situation is different, ownership versus access, and is something that
arises with publication in the online environment, faster than is
sometimes realized.  Lacking clear answers to all of these questions
herself, LEBRON did not anticipate that LC would be able to take a role
in helping to define some of them for quite a while.

GREENFIELD observed that LCs Network Development Office is attempting,
among other things, to explore the limits of MARC as a standard in terms
of handling electronic information.  GREENFIELD also noted that Rebecca
GUENTHER from that office gave a paper to the American Society for
Information Science ASIS summarizing several of the discussion papers
that were coming out of the Network Development Office.  GREENFIELD said
he understood that that office had a listserver soliciting just the kind
of feedback received today concerning the difficulties of identifying and
cataloguing electronic information.  GREENFIELD hoped that everybody
would be aware of that and somehow contribute to that conversation.

Noting two of LCs roles, first, to act as a repository of record for
material that is copyrighted in this country, and second, to make
materials it holds available in some limited form to a clientele that
goes beyond Congress, BESSER suggested that it was incumbent on LC to
extend those responsibilities to all the things being published in
electronic form.  This would mean eventually accepting electronic
formats.  LC could require that at some point they be in a certain
limited set of formats, and then develop mechanisms for allowing people
to access those in the same way that other things are accessed.  This
does not imply that they are on the network and available to everyone.
LC does that with most of its bibliographic records, BESSER said, which
end up migrating to the utility e.g., OCLC or somewhere else.  But just
as most of LCs books are available in some form through interlibrary
loan or some other mechanism, so in the same way electronic formats ought
to be available to others in some format, though with some copyright
considerations.  BESSER was not suggesting that these mechanisms be
established tomorrow, only that they seemed to fall within LCs purview,
and that there should be longrange plans to establish them.

Acknowledging that those from LC in the room agreed with BESSER
concerning the need to confront difficult questions, GIFFORD underscored
the magnitude of the problem of what to keep and what to select.  GIFFORD
noted that LC currently receives some 31,000 items per day, not counting
electronic materials, and argued for much more distributed responsibility
in order to maintain and store electronic information.

BESSER responded that the assembled group could be viewed as a starting
point, whose initial operating premise could be helping to move in this
direction and defining how LC could do so, for example, in areas of
standardization or distribution of responsibility.

FLEISCHHAUER added that AM was fully engaged, wrestling with some of the
questions that pertain to the conversion of older historical materials,
which would be one thing that the Library of Congress might do.  Several
points mentioned by BESSER and several others on this question have a
much greater impact on those who are concerned with cataloguing and the
networking of bibliographic information, as well as preservation itself.

Speaking directly to AM, which he considered was a largely uncopyrighted
database, LYNCH urged development of a network version of AM, or
consideration of making the data in it available to people interested in
doing network multimedia.  On account of the current great shortage of
digital data that is both appealing and unencumbered by complex rights
problems, this course of action could have a significant effect on making
network multimedia a reality.

In this connection, FLEISCHHAUER reported on a fragmentary prototype in
LCs Office of Information Technology Services that attempts to associate
digital images of photographs with cataloguing information in ways that
work within a local area networka step, so to say, toward AMs
construction of some sort of apparatus for access.  Further, AM has
attempted to use standard data forms in order to help make that
distinction between the access tools and the underlying data, and thus
believes that the database is networkable.

A delicate and agonizing policy question for LC, however, which comes
back to resources and unfortunately has an impact on this, is to find
some appropriate, honorable, and legal costrecovery possibilities.  A
certain skittishness concerning costrecovery has made people unsure
exactly what to do.  AM would be highly receptive to discussing further
LYNCHs offer to test or demonstrate its database in a network
environment, FLEISCHHAUER said.

Returning the discussion to what she viewed as the vital issue of
electronic deposit, BATTIN recommended that LC initiate a catalytic
process in terms of distributed responsibility, that is, bring together
the distributed organizations and set up a study group to look at all
these issues and see where we as a nation should move.  The broader
issues of how we deal with the management of electronic information will
not disappear, but only grow worse.

LESK took up this theme and suggested that LC attempt to persuade one
major library in each state to deal with its state equivalent publisher,
which might produce a cooperative project that would be equitably
distributed around the country, and one in which LC would be dealing with
a minimal number of publishers and minimal copyright problems.

GRABER remarked the recent development in the scientific community of a
willingness to use SGML and either deposit or interchange on a fairly
standardized format.  He wondered if a similar movement was taking place
in the humanities.  Although the National Library of Medicine found only
a few publishers to cooperate in a like venture two or three years ago, a
new effort might generate a much larger number willing to cooperate.

KIMBALL recounted his units MachineReadable Collections Reading Room
troubles with the commercial publishers of electronic media in acquiring
materials for LCs collections, in particular the publishers fear that
they would not be able to cover their costs and would lose control of
their products, that LC would give them away or sell them and make
profits from them.  He doubted that the publishing industry was prepared
to move into this area at the moment, given its resistance to allowing LC
to use its machinereadable materials as the Library would like.

The copyright law now addresses compact disk as a medium, and LC can
request one copy of that, or two copies if it is the only version, and
can request copies of software, but that fails to address magazines or
books or anything like that which is in machinereadable form.

GIFFORD acknowledged the thorny nature of this issue, which he illustrated
with the example of the cumbersome process involved in putting a copy of a
scientific database on a LAN in LCs science reading room.  He also
acknowledged that LC needs help and could enlist the energies and talents
of Workshop participants in thinking through a number of these problems.

GIFFORD returned the discussion to getting the image and text people to
think through together where they want to go in the long term.  MYLONAS
conceded that her experience at the Pierce Symposium the previous week at
Georgetown University and this week at LC had forced her to reevaluate
her perspective on the usefulness of text as images.  MYLONAS framed the
issues in a series of questions:  How do we acquire machinereadable
text?  Do we take pictures of it and perform OCR on it later?  Is it
important to obtain very highquality images and text, etc.?
FLEISCHHAUER agreed with MYLONASs framing of strategic questions, adding
that a large institution such as LC probably has to do all of those
things at different times.  Thus, the trick is to exercise judgment.  The
Workshop had added to his and AMs considerations in making those
judgments.  Concerning future meetings or discussions, MYLONAS suggested
that screening priorities would be helpful.

WEIBEL opined that the diversity reflected in this group was a sign both
of the health and of the immaturity of the field, and more time would
have to pass before we convince one another concerning standards.

An exchange between MYLONAS and BATTIN clarified the point that the
driving force behind both the Perseus and the Cornell Xerox projects was
the preservation of knowledge for the future, not simply for particular
research use.  In the case of Perseus, MYLONAS said, the assumption was
that the texts would not be entered again into electronically readable
form.  SPERBERGMcQUEEN added that a scanned image would not serve as an
archival copy for purposes of preservation in the case of, say, the Bill
of Rights, in the sense that the scanned images are effectively the
archival copies for the Cornell mathematics books.


                                 


                          Appendix I:  PROGRAM



                                WORKSHOP
                                   ON
                               ELECTRONIC
                                  TEXTS



                             910 June 1992

                           Library of Congress
                            Washington, D.C.



    Supported by a Grant from the David and Lucile Packard Foundation


Tuesday, 9 June 1992

NATIONAL DEMONSTRATION LAB, ATRIUM, LIBRARY MADISON

8:30 AM   Coffee and Danish, registration

9:00 AM   Welcome

          Prosser Gifford, Director for Scholarly Programs, and Carl
             Fleischhauer, Coordinator, American Memory, Library of
             Congress

9:l5 AM   Session I.  Content in a New Form:  Who Will Use It and What
          Will They Do?

          Broad description of the range of electronic information.
          Characterization of who uses it and how it is or may be used.
          In addition to a look at scholarly uses, this session will
          include a presentation on use by students K12 and college
          and the general public.

          Moderator:  James Daly
          Avra Michelson, Archival Research and Evaluation Staff,
             National Archives and Records Administration Overview
          Susan H. Veccia, Team Leader, American Memory, User Evaluation,
             and
          Joanne Freeman, Associate Coordinator, American Memory, Library
             of Congress Beyond the scholar

10:30
11:00 AM  Break

11:00 AM  Session II.  Show and Tell.

          Each presentation to consist of a fifteenminute
          statementshow group discussion will follow lunch.

          Moderator:  Jacqueline Hess, Director, National Demonstration
             Lab

            1.  A classics project, stressing texts and text retrieval
                more than multimedia:  Perseus Project, Harvard
                University
                Elli Mylonas, Managing Editor

            2.  Other humanities projects employing the emerging norms of
                the Text Encoding Initiative TEI:  ChadwyckHealeys
                The English Poetry Full Text Database andor Patrologia
                Latina Database
                Eric M. Calaluca, Vice President, ChadwyckHealey, Inc.

            3.  American Memory
                Carl Fleischhauer, Coordinator, and
                Ricky Erway, Associate Coordinator, Library of Congress

            4.  Founding Fathers example from Packard Humanities
                Institute:  The Papers of George Washington, University
                of Virginia
                Dorothy Twohig, Managing Editor, andor
                David Woodley Packard

            5.  An electronic medical journal offering graphics and
                fulltext searchability:  The Online Journal of Current
                Clinical Trials, American Association for the Advancement
                of Science
                Maria L. Lebron, Managing Editor

            6.  A project that offers facsimile images of pages but omits
                searchable text:  Cornell math books
                Lynne K. Personius, Assistant Director, Cornell
                   Information Technologies for Scholarly Information
                   Sources, Cornell University

12:30 PM  Lunch  Dining Room A, Library Madison 620.  Exhibits
          available.

1:30 PM   Session II.  Show and Tell Contd..

3:00
3:30 PM   Break

3:30
5:30 PM   Session III.  Distribution, Networks, and Networking:  Options
          for Dissemination.

          Published disks:  University presses and publicsector
             publishers, privatesector publishers
          Computer networks

          Moderator:  Robert G. Zich, Special Assistant to the Associate
             Librarian for Special Projects, Library of Congress
          Clifford A. Lynch, Director, Library Automation, University of
             California
          Howard Besser, School of Library and Information Science,
             University of Pittsburgh
          Ronald L. Larsen, Associate Director of Libraries for
             Information Technology, University of Maryland at College
             Park
          Edwin B. Brownrigg, Executive Director, Memex Research
             Institute

6:30 PM   Reception  Montpelier Room, Library Madison 619.

                                 

Wednesday, 10 June 1992

DINING ROOM A, LIBRARY MADISON 620

8:30 AM   Coffee and Danish

9:00 AM   Session IV.  Image Capture, Text Capture, Overview of Text and
          Image Storage Formats.

          Moderator:  William L. Hooton, Vice President of Operations,
             INET

          A Principal Methods for Image Capture of Text:
             Direct scanning
             Use of microform

          Anne R. Kenney, Assistant Director, Department of Preservation
             and Conservation, Cornell University
          Pamela Q.J. Andre, Associate Director, Automation, and
          Judith A. Zidar, Coordinator, National Agricultural Text
             Digitizing Program NATDP, National Agricultural Library
             NAL
          Donald J. Waters, Head, Systems Office, Yale University Library

          B Special Problems:
             Bound volumes
             Conservation
             Reproducing printed halftones

          Carl Fleischhauer, Coordinator, American Memory, Library of
             Congress
          George Thoma, Chief, Communications Engineering Branch,
             National Library of Medicine NLM

10:30
11:00 AM  Break

11:00 AM  Session IV.  Image Capture, Text Capture, Overview of Text and
          Image Storage Formats Contd..

          C Image Standards and Implications for Preservation

          Jean Baronas, Senior Manager, Department of Standards and
             Technology, Association for Information and Image Management
             AIIM
          Patricia Battin, President, The Commission on Preservation and
             Access CPA

          D Text Conversion:
             OCR vs. rekeying
             Standards of accuracy and use of imperfect texts
             Service bureaus

          Stuart Weibel, Senior Research Specialist, Online Computer
             Library Center, Inc. OCLC
          Michael Lesk, Executive Director, Computer Science Research,
             Bellcore
          Ricky Erway, Associate Coordinator, American Memory, Library of
             Congress
          Pamela Q.J. Andre, Associate Director, Automation, and
          Judith A. Zidar, Coordinator, National Agricultural Text
             Digitizing Program NATDP, National Agricultural Library
             NAL

12:30
1:30 PM   Lunch

1:30 PM   Session V.  Approaches to Preparing Electronic Texts.

          Discussion of approaches to structuring text for the computer
          pros and cons of text coding, description of methods in
          practice, and comparison of textcoding methods.

          Moderator:  Susan Hockey, Director, Center for Electronic Texts
             in the Humanities CETH, Rutgers and Princeton Universities
          David Woodley Packard
          C.M. SperbergMcQueen, Editor, Text Encoding Initiative TEI,
             University of IllinoisChicago
          Eric M. Calaluca, Vice President, ChadwyckHealey, Inc.

3:30
4:00 PM   Break

4:00 PM   Session VI.  Copyright Issues.

          Marybeth Peters, Policy Planning Adviser to the Register of
             Copyrights, Library of Congress

5:00 PM   Session VII. Conclusion.

          General discussion.
          What topics were omitted or given short shrift that anyone
             would like to talk about now?
          Is there a "group" here?  What should the group do next, if
             anything?  What should the Library of Congress do next, if
             anything?
          Moderator:  Prosser Gifford, Director for Scholarly Programs,
             Library of Congress

6:00 PM   Adjourn


                                 


                         Appendix II:  ABSTRACTS


SESSION I

Avra MICHELSON           Forecasting the Use of Electronic Texts by
                         Social Sciences and Humanities Scholars

This presentation explores the ways in which electronic texts are likely
to be used by the nonscientific scholarly community.  Many of the
remarks are drawn from a report the speaker coauthored with Jeff
Rothenberg, a computer scientist at The RAND Corporation.

The speaker assesses 1 current scholarly use of information technology
and 2 the key trends in information technology most relevant to the
research process, in order to predict how social sciences and humanities
scholars are apt to use electronic texts.  In introducing the topic,
current use of electronic texts is explored broadly within the context of
scholarly communication.  From the perspective of scholarly
communication, the work of humanities and social sciences scholars
involves five processes:  1 identification of sources, 2 communication
with colleagues, 3 interpretation and analysis of data, 4 dissemination
of research findings, and 5 curriculum development and instruction.  The
extent to which computation currently permeates aspects of scholarly
communication represents a viable indicator of the prospects for
electronic texts.

The discussion of current practice is balanced by an analysis of key
trends in the scholarly use of information technology.  These include the
trends toward enduser computing and connectivity, which provide a
framework for forecasting the use of electronic texts through this
millennium.  The presentation concludes with a summary of the ways in
which the nonscientific scholarly community can be expected to use
electronic texts, and the implications of that use for information
providers.

Susan VECCIA and Joanne FREEMAN    Electronic Archives for the Public:
                                   Use of American Memory in Public and
                                   School Libraries

This joint discussion focuses on nonscholarly applications of electronic
library materials, specifically addressing use of the Library of Congress
American Memory AM program in a small number of public and school
libraries throughout the United States.  AM consists of selected Library
of Congress primary archival materials, stored on optical media
CDROMvideodisc, and presented with little or no editing.  Many
collections are accompanied by electronic introductions and users guides
offering background information and historical context.  Collections
represent a variety of formats including photographs, graphic arts,
motion pictures, recorded sound, music, broadsides and manuscripts,
books, and pamphlets.

In 1991, the Library of Congress began a nationwide evaluation of AM in
different types of institutions.  Test sites include public libraries,
elementary and secondary school libraries, college and university
libraries, state libraries, and special libraries.  Susan VECCIA and
Joanne FREEMAN will discuss their observations on the use of AM by the
nonscholarly community, using evidence gleaned from this ongoing
evaluation effort.

VECCIA will comment on the overall goals of the evaluation project, and
the types of public and school libraries included in this study.  Her
comments on nonscholarly use of AM will focus on the public library as a
cultural and community institution, often bridging the gap between formal
and informal education.  FREEMAN will discuss the use of AM in school
libraries.  Use by students and teachers has revealed some broad
questions about the use of electronic resources, as well as definite
benefits gained by the "nonscholar."  Topics will include the problem of
grasping content and context in an electronic environment, the stumbling
blocks created by "new" technologies, and the unique skills and interests
awakened through use of electronic resources.

SESSION II

Elli MYLONAS             The Perseus Project:  Interactive Sources and
                         Studies in Classical Greece

The Perseus Project 5 has just released Perseus 1.0, the first publicly
available version of its hypertextual database of multimedia materials on
classical Greece.  Perseus is designed to be used by a wide audience,
comprised of readers at the student and scholar levels.  As such, it must
be able to locate information using different strategies, and it must
contain enough detail to serve the different needs of its users.  In
addition, it must be delivered so that it is affordable to its target
audience.  These problems and the solutions we chose are described in
Mylonas, "An Interface to Classical Greek Civilization," JASIS 43:2,
March 1992.

In order to achieve its objective, the project staff decided to make a
conscious separation between selecting and converting textual, database,
and image data on the one hand, and putting it into a delivery system on
the other.  That way, it is possible to create the electronic data
without thinking about the restrictions of the delivery system.  We have
made a great effort to choose systemindependent formats for our data,
and to put as much thought and work as possible into structuring it so
that the translation from paper to electronic form will enhance the value
of the data. A discussion of these solutions as of two years ago is in
Elli Mylonas, Gregory Crane, Kenneth Morrell, and D. Neel Smith, "The
Perseus Project:  Data in the Electronic Age," in Accessing Antiquity:
The Computerization of Classical Databases, J. Solomon and T. Worthen
eds.,  University of Arizona Press, in press.

Much of the work on Perseus is focused on collecting and converting the
data on which the project is based.  At the same time, it is necessary to
provide means of access to the information, in order to make it usable,
and them to investigate how it is used.  As we learn more about what
students and scholars from different backgrounds do with Perseus, we can
adjust our data collection, and also modify the system to accommodate
them.  In creating a delivery system for general use, we have tried to
avoid favoring any one type of use by allowing multiple forms of access
to and navigation through the system.

The way text is handled exemplifies some of these principles.  All text
in Perseus is tagged using SGML, following the guidelines of the Text
Encoding Initiative TEI.  This markup is used to index the text, and
process it so that it can be imported into HyperCard.  No SGML markup
remains in the text that reaches the user, because currently it would be
too expensive to create a system that acts on SGML in real time.
However, the regularity provided by SGML is essential for verifying the
content of the texts, and greatly speeds all the processing performed on
them.  The fact that the texts exist in SGML ensures that they will be
relatively easy to port to different hardware and software, and so will
outlast the current delivery platform.  Finally, the SGML markup
incorporates existing canonical reference systems chapter, verse, line,
etc. indexing and navigation are based on these features.  This ensures
that the same canonical reference will always resolve to the same point
within a text, and that all versions of our texts, regardless of delivery
platform even paper printouts will function the same way.

In order to provide tools for users, the text is processed by a
morphological analyzer, and the results are stored in a database.
Together with the index, the GreekEnglish Lexicon, and the index of all
the English words in the definitions of the lexicon, the morphological
analyses comprise a set of linguistic tools that allow users of all
levels to work with the textual information, and to accomplish different
tasks.  For example, students who read no Greek may explore a concept as
it appears in Greek texts by using the EnglishGreek index, and then
looking up works in the texts and translations, or scholars may do
detailed morphological studies of word use by using the morphological
analyses of the texts.  Because these tools were not designed for any one
use, the same tools and the same data can be used by both students and
scholars.

NOTES:
     5  Perseus is based at Harvard University, with collaborators at
     several other universities.  The project has been funded primarily
     by the AnnenbergCPB Project, as well as by Harvard University,
     Apple Computer, and others.  It is published by Yale University
     Press.  Perseus runs on Macintosh computers, under the HyperCard
     program.

Eric CALALUCA

ChadwyckHealey embarked last year on two distinct yet related fulltext
humanities database projects.

The English Poetry FullText Database and the Patrologia Latina Database
represent new approaches to linguistic research resources.  The size and
complexity of the projects present problems for electronic publishers,
but surmountable ones if they remain abreast of the latest possibilities
in data capture and retrieval software techniques.

The issues which required address prior to the commencement of the
projects were legion:

     1.   Editorial selection or exclusion of materials in each
          database

     2.   Deciding whether or not to incorporate a normative encoding
          structure into the databases?
               A.  If one is selected, should it be SGML?
               B.  If SGML, then the TEI?

     3.   Deliver as CDROM, magnetic tape, or both?

     4.   Can one produce retrieval software advanced enough for the
          postdoctoral linguist, yet accessible enough for unattended
          general use?  Should one try?

     5.   Re fair and liberal networking policies, what are the risks to
          an electronic publisher?

     6.   How does the emergence of national and international education
          networks affect the use and viability of research projects
          requiring high investment?  Do the new European Community
          directives concerning database protection necessitate two
          distinct publishing projects, one for North America and one for
          overseas?

From new notions of "scholarly fair use" to the future of optical media,
virtually every issue related to electronic publishing was aired.  The
result is two projects which have been constructed to provide the quality
research resources with the fewest encumbrances to use by teachers and
private scholars.

Dorothy TWOHIG

In spring 1988 the editors of the papers of George Washington, John
Adams, Thomas Jefferson, James Madison, and Benjamin Franklin were
approached by classics scholar David Packard on behalf of the Packard
Humanities Foundation with a proposal to produce a CDROM edition of the
complete papers of each of the Founding Fathers.  This electronic edition
will supplement the published volumes, making the documents widely
available to students and researchers at reasonable cost.  We estimate
that our CDROM edition of Washingtons Papers will be substantially
completed within the next two years and ready for publication.  Within
the next ten years or so, similar CDROM editions of the Franklin, Adams,
Jefferson, and Madison papers also will be available.  At the Library of
Congresss session on technology, I would like to discuss not only the
experience of the Washington Papers in producing the CDROM edition, but
the impact technology has had on these major editorial projects.
Already, we are editing our volumes with an eye to the material that will
be readily available in the CDROM edition.  The completed electronic
edition will provide immense possibilities for the searching of documents
for information in a way never possible before.  The kind of technical
innovations that are currently available and on the drawing board will
soon revolutionize historical research and the production of historical
documents.  Unfortunately, much of this new technology is not being used
in the planning stages of historical projects, simply because many
historians are aware only in the vaguest way of its existence.  At least
two major new historical editing projects are considering microfilm
editions, simply because they are not aware of the possibilities of
electronic alternatives and the advantages of the new technology in terms
of flexibility and research potential compared to microfilm.  In fact,
too many of us in history and literature are still at the stage of
struggling with our PCs.  There are many historical editorial projects in
progress presently, and an equal number of literary projects.  While the
two fields have somewhat different approaches to textual editing, there
are ways in which electronic technology can be of service to both.

Since few of the editors involved in the Founding Fathers CDROM editions
are technical experts in any sense, I hope to point out in my discussion
of our experience how many of these electronic innovations can be used
successfully by scholars who are novices in the world of new technology.
One of the major concerns of the sponsors of the multitude of new
scholarly editions is the limited audience reached by the published
volumes.  Most of these editions are being published in small quantities
and the publishers price for them puts them out of the reach not only of
individual scholars but of most public libraries and all but the largest
educational institutions.  However, little attention is being given to
ways in which technology can bypass conventional publication to make
historical and literary documents more widely available.

What attracted us most to the CDROM edition of The Papers of George
Washington was the fact that David Packards aim was to make a complete
edition of all of the 135,000 documents we have collected available in an
inexpensive format that would be placed in public libraries, small
colleges, and even high schools.  This would provide an audience far
beyond our present 1,000copy, 45 published edition.  Since the CDROM
edition will carry none of the explanatory annotation that appears in the
published volumes, we also feel that the use of the CDROM will lead many
researchers to seek out the published volumes.

In addition to ignorance of new technical advances, I have found that too
many editorsand historians and literary scholarsare resistant and
even hostile to suggestions that electronic technology may enhance their
work.  I intend to discuss some of the arguments traditionalists are
advancing to resist technology, ranging from distrust of the speed with
which it changes we are already wondering what is out there that is
better than CDROM to suspicion of the technical language used to
describe electronic developments.

Maria LEBRON

The Online Journal of Current Clinical Trials, a joint venture of the
American Association for the Advancement of Science AAAS and the Online
Computer Library Center, Inc. OCLC, is the first peerreviewed journal
to provide full text, tabular material, and line illustrations on line.
This presentation will discuss the genesis and startup period of the
journal.  Topics of discussion will include historical overview,
daytoday management of the editorial peer review, and manuscript
tagging and publication.  A demonstration of the journal and its features
will accompany the presentation.

Lynne PERSONIUS

Cornell University Library, Cornell Information Technologies, and Xerox
Corporation, with the support of the Commission on Preservation and
Access, and Sun Microsystems, Inc., have been collaborating in a project
to test a prototype system for recording brittle books as digital images
and producing, on demand, highquality archival paper replacements.  The
project goes beyond that, however, to investigate some of the issues
surrounding scanning, storing, retrieving, and providing access to
digital images in a network environment.

The Joint Study in Digital Preservation began in January 1990.  Xerox
provided the College Library Access and Storage System CLASS software,
a prototype 600dotsperinch dpi scanner, and the hardware necessary
to support network printing on the DocuTech printer housed in Cornells
Computing and Communications Center CCC.

The Cornell staff using the hardware and software became an integral part
of the development and testing process for enhancements to the CLASS
software system.  The collaborative nature of this relationship is
resulting in a system that is specifically tailored to the preservation
application.

A digital library of 1,000 volumes or approximately 300,000 images has
been created and is stored on an optical jukebox that resides in CCC.
The library includes a collection of select mathematics monographs that
provides mathematics faculty with an opportunity to use the electronic
library.  The remaining volumes were chosen for the library to test the
various capabilities of the scanning system.

One project objective is to provide users of the Cornell library and the
library staff with the ability to request facsimiles of digitized images
or to retrieve the actual electronic image for browsing.  A prototype
viewing workstation has been created by Xerox, with input into the design
by a committee of Cornell librarians and computer professionals.  This
will allow us to experiment with patron access to the images that make up
the digital library.  The viewing station provides search, retrieval, and
ultimately printing functions with enhancements to facilitate
navigation through multiple documents.

Cornell currently is working to extend access to the digital library to
readers using workstations from their offices.  This year is devoted to
the development of a network resident image conversion and delivery
server, and client software that will support readers who use Apple
Macintosh computers, IBM windows platforms, and Sun workstations.
Equipment for this development was provided by Sun Microsystems with
support from the Commission on Preservation and Access.

During the showandtell session of the Workshop on Electronic Texts, a
prototype view station will be demonstrated.  In addition, a display of
original library books that have been digitized will be available for
review with associated printed copies for comparison.  The fifteenminute
overview of the project will include a slide presentation that
constitutes a "tour" of the preservation digitizing process.

The final networkconnected version of the viewing station will provide
library users with another mechanism for accessing the digital library,
and will also provide the capability of viewing images directly.  This
will not require special software, although a powerful computer with good
graphics will be needed.

The Joint Study in Digital Preservation has generated a great deal of
interest in the library community.  Unfortunately, or perhaps
fortunately, this project serves to raise a vast number of other issues
surrounding the use of digital technology for the preservation and use of
deteriorating library materials, which subsequent projects will need to
examine.  Much work remains.

SESSION III

Howard BESSER                      Networking Multimedia Databases

What do we have to consider in building and distributing databases of
visual materials in a multiuser environment?  This presentation examines
a variety of concerns that need to be addressed before a multimedia
database can be set up in a networked environment.

In the past it has not been feasible to implement databases of visual
materials in shareduser environments because of technological barriers.
Each of the two basic models for multiuser multimedia databases has
posed its own problem.  The analog multimedia storage model represented
by Project Athenas parallel analog and digital networks has required an
incredibly complex and expensive infrastructure.  The economies of
scale that make multiuser setups cheaper per user served do not operate
in an environment that requires a computer workstation, videodisc player,
and two display devices for each user.

The digital multimedia storage model has required vast amounts of storage
space as much as one gigabyte per thirty still images.  In the past the
cost of such a large amount of storage space made this model a
prohibitive choice as well.  But plunging storage costs are finally
making this second alternative viable.

If storage no longer poses such an impediment, what do we need to
consider in building digitally stored multiuser databases of visual
materials?  This presentation will examine the networking and
telecommunication constraints that must be overcome before such databases
can become commonplace and useful to a large number of people.

The key problem is the vast size of multimedia documents, and how this
affects not only storage but telecommunications transmission time.
Anything slower than T1 speed is impractical for files of 1 megabyte or
larger which is likely to be small for a multimedia document.  For
instance, even on a 56 Kb line it would take three minutes to transfer a
1megabyte file.  And these figures assume ideal circumstances, and do
not take into consideration other users contending for network bandwidth,
disk access time, or the time needed for remote display.  Current common
telephone transmission rates would be completely impractical few users
would be willing to wait the hour necessary to transmit a single image at
2400 baud.

This necessitates compression, which itself raises a number of other
issues.  In order to decrease file sizes significantly, we must employ
lossy compression algorithms.  But how much quality can we afford to
lose?  To date there has been only one significant study done of
imagequality needs for a particular user group, and this study did not
look at loss resulting from compression.  Only after identifying
imagequality needs can we begin to address storage and network bandwidth
needs.

Experience with XWindowsbased applications such as Imagequery, the
University of California at Berkeley image database demonstrates the
utility of a clientserver topology, but also points to the limitation of
current software for a distributed environment.  For example,
applications like Imagequery can incorporate compression, but current X
implementations do not permit decompression at the end users
workstation.  Such decompression at the host computer alleviates storage
capacity problems while doing nothing to address problems of
telecommunications bandwidth.

We need to examine the effects on network throughput of moving
multimedia documents around on a network.  We need to examine various
topologies that will help us avoid bottlenecks around servers and
gateways.  Experience with applications such as these raise still broader
questions. How closely is the multimedia document tied to the software
for viewing it?  Can it be accessed and viewed from other applications?
Experience with the MARC format and more recently with the Z39.50
protocols shows how useful it can be to store documents in a form in
which they can be accessed by a variety of application software.

Finally, from an intellectualaccess standpoint, we need to address the
issue of providing access to these multimedia documents in
interdisciplinary environments.  We need to examine terminology and
indexing strategies that will allow us to provide access to this material
in a crossdisciplinary way.

Ronald LARSEN            Directions in HighPerformance Networking for
                         Libraries

The pace at which computing technology has advanced over the past forty
years shows no sign of abating.  Roughly speaking, each fiveyear period
has yielded an orderofmagnitude improvement in price and performance of
computing equipment.  No fundamental hurdles are likely to prevent this
pace from continuing for at least the next decade.  It is only in the
past five years, though, that computing has become ubiquitous in
libraries, affecting all staff and patrons, directly or indirectly.

During these same five years, communications rates on the Internet, the
principal academic computing network, have grown from 56 kbps to 1.5
Mbps, and the NSFNet backbone is now running 45 Mbps.  Over the next five
years, communication rates on the backbone are expected to exceed 1 Gbps.
Growth in both the population of network users and the volume of network
traffic  has continued to grow geometrically, at rates approaching 15
percent per month.  This flood of capacity and use, likened by some to
"drinking from a firehose,"  creates immense opportunities and challenges
for libraries.  Libraries must anticipate the future implications of this
technology, participate in its development, and deploy it to ensure
access to the worlds information resources.

The infrastructure for the information age is being put in place.
Libraries face strategic decisions about their role in the development,
deployment, and use of this infrastructure.  The emerging infrastructure
is much more than computers and communication lines.  It is more than the
ability to compute at a remote site, send electronic mail to a peer
across the country, or move a file from one library to another.  The next
five years will witness substantial development of the information
infrastructure of the network.

In order to provide appropriate leadership, library professionals must
have a fundamental understanding of and appreciation for computer
networking, from local area networks to the National Research and
Education Network NREN.  This presentation addresses these
fundamentals, and how they relate to libraries today and in the near
future.

Edwin BROWNRIGG               Electronic Library Visions and Realities

The electronic library has been a vision desired by manyand rejected by
somesince Vannevar Bush coined the term memex to describe an automated,
intelligent, personal information system.  Variations on this vision have
included Ted Nelsons Xanadau, Alan Kays Dynabook, and Lancasters
"paperless library," with the most recent incarnation being the
"Knowledge Navigator" described by John Scully of Apple.  But the reality
of library service has been less visionary and the leap to the electronic
library has eluded universities, publishers, and information technology
files.

The Memex Research Institute MemRI, an independent, nonprofit research
and development organization, has created an Electronic Library Program
of shared research and development in order to make the collective vision
more concrete.  The program is working toward the creation of large,
indexed publicly available electronic image collections of published
documents in academic, special, and public libraries.  This strategic
plan is the result of the first stage of the program, which has been an
investigation of the information technologies available to support such
an effort, the economic parameters of electronic service compared to
traditional library operations, and the business and political factors
affecting the shift from print distribution to electronic networked
access.

The strategic plan envisions a combination of publicly searchable access
databases, image and text document collections stored on network "file
servers," local and remote network access, and an intellectual property
managementcontrol system.  This combination of technology and
information content is defined in this plan as an Elibrary or Elibrary
collection.  Some participating sponsors are already developing projects
based on MemRIs recommended directions.

The Elibrary strategy projected in this plan is a visionary one that can
enable major changes and improvements in academic, public, and special
library service.  This vision is, though, one that can be realized with
todays technology.  At the same time, it will challenge the political
and social structure within which libraries operate:  in academic
libraries, the traditional emphasis on local collections, extending to
accreditation issues in public libraries, the potential of electronic
branch and central libraries fully available to the public and for
special libraries, new opportunities for shared collections and networks.

The environment in which this strategic plan has been developed is, at
the moment, dominated by a sense of library limits.  The continued
expansion and rapid growth of local academic library collections is now
clearly at an end.  Corporate libraries, and even law libraries, are
faced with operating within a difficult economic climate, as well as with
very active competition from commercial information sources.  For
example, public libraries may be seen as a desirable but not critical
municipal service in a time when the budgets of safety and health
agencies are being cut back.

Further, libraries in general have a very high labortocost ratio in
their budgets, and labor costs are still increasing, notwithstanding
automation investments.  It is difficult for libraries to obtain capital,
startup, or seed funding for innovative activities, and those
technologyintensive initiatives that offer the potential of decreased
labor costs can provoke the opposition of library staff.

However, libraries have achieved some considerable successes in the past
two decades by improving both their service and their credibility within
their organizationsand these positive changes have been accomplished
mostly with judicious use of information technologies.  The advances in
computing and information technology have been wellchronicled:  the
continuing precipitous drop in computing costs, the growth of the
Internet and private networks, and the explosive increase in publicly
available information databases.

For example, OCLC has become one of the largest computer network
organizations in the world by creating a cooperative cataloging network
of more than 6,000 libraries worldwide.  Online public access catalogs
now serve millions of users on more than 50,000 dedicated terminals in
the United States alone.  The University of California MELVYL online
catalog system has now expanded into an index database reference service
and supports more than six million searches a year.  And, libraries have
become the largest group of customers of CDROM publishing technology
more than 30,000 optical media publications such as those offered by
InfoTrac and Silver Platter are subscribed to by U.S. libraries.

This march of technology continues and in the next decade will result in
further innovations that are extremely difficult to predict.  What is
clear is that libraries can now go beyond automation of their order files
and catalogs to automation of their collections themselvesand it is
possible to circumvent the fiscal limitations that appear to obtain
today.

This Electronic Library Strategic Plan recommends a paradigm shift in
library service, and demonstrates the steps necessary to provide improved
library services with limited capacities and operating investments.

SESSION IVA

Anne KENNEY

The CornellXerox Joint Study in Digital Preservation resulted in the
recording of 1,000 brittle books as 600dpi digital images and the
production, on demand, of highquality and archivally sound paper
replacements.  The project, which was supported by the Commission on
Preservation and Access, also investigated some of the issues surrounding
scanning, storing, retrieving, and providing access to digital images in
a network environment.

Anne Kenney will focus on some of the issues surrounding direct scanning
as identified in the Cornell Xerox Project.  Among those to be discussed
are:  image versus text capture indexing and access imagecapture
capabilities a comparison to photocopy and microfilm production and
cost analysis storage formats, protocols, and standards and the use of
this scanning technology for preservation purposes.

The 600dpi digital images produced in the Cornell Xerox Project proved
highly acceptable for creating paper replacements of deteriorating
originals.  The 1,000 scanned volumes provided an array of imagecapture
challenges that are common to nineteenthcentury printing techniques and
embrittled material, and that defy the use of textconversion processes.
These challenges include diminished contrast between text and background,
fragile and deteriorated pages, uneven printing, elaborate type faces,
faint and bold text adjacency, handwritten text and annotations, nonRoman
languages, and a proliferation of illustrated material embedded in text.
The latter category included highfrequency and lowfrequency halftones,
continuous tone photographs, intricate mathematical drawings, maps,
etchings, reversepolarity drawings, and engravings.

The Xerox prototype scanning system provided a number of important
features for capturing this diverse material.  Technicians used multiple
threshold settings, filters, line art and halftone definitions,
autosegmentation, windowing, and softwareediting programs to optimize
image capture.  At the same time, this project focused on production.
The goal was to make scanning as affordable and acceptable as
photocopying and microfilming for preservation reformatting.  A
timeandcost study conducted during the last three months of this
project confirmed the economic viability of digital scanning, and these
findings will be discussed here.

From the outset, the Cornell Xerox Project was predicated on the use of
nonproprietary standards and the use of common protocols when standards
did not exist.  Digital files were created as TIFF images which were
compressed prior to storage using Group 4 CCITT compression.  The Xerox
software is MS DOS based and utilizes offthe shelf programs such as
Microsoft Windows and Wang Image Wizard.  The digital library is designed
to be hardwareindependent and to provide interchangeability with other
institutions through network connections.  Access to the digital files
themselves is twotiered:  Bibliographic records for the computer files
are created in RLIN and Cornells local system and access into the actual
digital images comprising a book is provided through a document control
structure and a networked image fileserver, both of which will be
described.

The presentation will conclude with a discussion of some of the issues
surrounding the use of this technology as a preservation tool storage,
refreshing, backup.

Pamela ANDRE and Judith ZIDAR

The National Agricultural Library NAL has had extensive experience with
raster scanning of printed materials.  Since 1987, the Library has
participated in the National Agricultural Text Digitizing Project NATDP
a cooperative effort between NAL and fortyfive land grant university
libraries.  An overview of the project will be presented, giving its
history and NALs strategy for the future.

An indepth discussion of NATDP will follow, including a description of
the scanning process, from the gathering of the printed materials to the
archiving of the electronic pages.  The type of equipment required for a
standalone scanning workstation and the importance of file management
software will be discussed.  Issues concerning the images themselves will
be addressed briefly, such as image format black and white versus color
gray scale versus dithering and resolution.

Also described will be a study currently in progress by NAL to evaluate
the usefulness of converting microfilm to electronic images in order to
improve access.  With the cooperation of Tuskegee University, NAL has
selected three reels of microfilm from a collection of sixtyseven reels
containing the papers, letters, and drawings of George Washington Carver.
The three reels were converted into 3,500 electronic images using a
specialized microfilm scanner.  The selection, filming, and indexing of
this material will be discussed.

Donald WATERS

Project Open Book, the Yale University Librarys effort to convert 10,
000 books from microfilm to digital imagery, is currently in an advanced
state of planning and organization.  The Yale Library has selected a
major vendor to serve as a partner in the project and as systems
integrator.  In its proposal, the successful vendor helped isolate areas
of risk and uncertainty as well as key issues to be addressed during the
life of the project.  The Yale Library is now poised to decide what
material it will convert to digital image form and to seek funding,
initially for the first phase and then for the entire project.

The proposal that Yale accepted for the implementation of Project Open
Book will provide at the end of three phases a conversion subsystem,
browsing stations distributed on the campus network within the Yale
Library, a subsystem for storing 10,000 books at 200 and 600 dots per
inch, and network access to the image printers.  Pricing for the system
implementation assumes the existence of Yales campus ethernet network
and its highspeed image printers, and includes other requisite hardware
and software, as well as system integration services.  Proposed operating
costs include hardware and software maintenance, but do not include
estimates for the facilities management of the storage devices and image
servers.

Yale selected its vendor partner in a formal process, partly funded by
the Commission for Preservation and Access.  Following a request for
proposal, the Yale Library selected two vendors as finalists to work with
Yale staff to generate a detailed analysis of requirements for Project
Open Book.  Each vendor used the results of the requirements analysis to
generate and submit a formal proposal for the entire project.  This
competitive process not only enabled the Yale Library to select its
primary vendor partner but also revealed much about the state of the
imaging industry, about the varying, corporate commitments to the markets
for imaging technology, and about the varying organizational dynamics
through which major companies are responding to and seeking to develop
these markets.

Project Open Book is focused specifically on the conversion of images
from microfilm to digital form.  The technology for scanning microfilm is
readily available but is changing rapidly.  In its project requirements,
the Yale Library emphasized features of the technology that affect the
technical quality of digital image production and the costs of creating
and storing the image library:  What levels of digital resolution can be
achieved by scanning microfilm?  How does variation in the quality of
microfilm, particularly in film produced to preservation standards,
affect the quality of the digital images?  What technologies can an
operator effectively and economically apply when scanning film to
separate twoup images and to control for and correct image
imperfections?  How can quality control best be integrated into
digitizing work flow that includes document indexing and storage?

The actual and expected uses of digital imagesstorage, browsing,
printing, and OCRhelp determine the standards for measuring their
quality.  Browsing is especially important, but the facilities available
for readers to browse image documents is perhaps the weakest aspect of
imaging technology and most in need of development.  As it defined its
requirements, the Yale Library concentrated on some fundamental aspects
of usability for image documents:  Does the system have sufficient
flexibility to handle the full range of document types, including
monographs, multipart and multivolume sets, and serials, as well as
manuscript collections?  What conventions are necessary to identify a
document uniquely for storage and retrieval?  Where is the database of
record for storing bibliographic information about the image document?
How are basic internal structures of documents, such as pagination, made
accessible to the reader?  How are the image documents physically
presented on the screen to the reader?

The Yale Library designed Project Open Book on the assumption that
microfilm is more than adequate as a medium for preserving the content of
deteriorated library materials.  As planning in the project has advanced,
it is increasingly clear that the challenge of digital image technology
and the key to the success of efforts like Project Open Book is to
provide a means of both preserving and improving access to those
deteriorated materials.

SESSION IVB

George THOMA

In the use of electronic imaging for document preservation, there are
several issues to consider, such as:  ensuring adequate image quality,
maintaining substantial conversion rates throughput, providing unique
identification for automated access and retrieval, and accommodating
bound volumes and fragile material.

To maintain high image quality, image processing functions are required
to correct the deficiencies in the scanned image.  Some commercially
available systems include these functions, while some do not.  The
scanned raw image must be processed to correct contrast deficiencies
both poor overall contrast resulting from light print andor dark
background, and variable contrast resulting from stains and
bleedthrough.  Furthermore, the scan density must be adequate to allow
legibility of print and sufficient fidelity in the pseudohalftoned gray
material.  Borders or pageedge effects must be removed for both
compactibility and aesthetics.  Page skew must be corrected for aesthetic
reasons and to enable accurate character recognition if desired.
Compound images consisting of both twotoned text and grayscale
illustrations must be processed appropriately to retain the quality of
each.

SESSION IVC

Jean BARONAS

Standards publications being developed by scientists, engineers, and
business managers in Association for Information and Image Management
AIIM standards committees can be applied to electronic image management
EIM processes including:  document image transfer, retrieval and
evaluation optical disk and document scanning and document design and
conversion.  When combined with EIM system planning and operations,
standards can assist in generating image databases that are
interchangeable among a variety of systems.  The applications of
different approaches for imagetagging, indexing, compression, and
transfer often cause uncertainty concerning EIM system compatibility,
calibration, performance, and upward compatibility, until standard
implementation parameters are established.  The AIIM standards that are
being developed for these applications can be used to decrease the
uncertainty, successfully integrate imaging processes, and promote "open
systems."  AIIM is an accredited American National Standards Institute
ANSI standards developer with more than twenty committees comprised of
300 volunteers representing users, vendors, and manufacturers.  The
standards publications that are developed in these committees have
national acceptance and provide the basis for international harmonization
in the development of new International Organization for Standardization
ISO standards.

This presentation describes the development of AIIMs EIM standards and a
new effort at AIIM, a database on standards projects in a wide framework
of imaging industries including capture, recording, processing,
duplication, distribution, display, evaluation, and preservation.  The
AIIM Imagery Database will cover imaging standards being developed by
many organizations in many different countries.  It will contain
standards publications dates, origins, related national and
international projects, status, key words, and abstracts.  The ANSI Image
Technology Standards Board requested that such a database be established,
as did the ISOInternational Electrotechnical Commission Joint Task Force
on Imagery.  AIIM will take on the leadership role for the database and
coordinate its development with several standards developers.

Patricia BATTIN

     Characteristics of standards for digital imagery:

           Nature of digital technology implies continuing volatility.

           Precipitous standardsetting not possible and probably not
          desirable.

           Standards are a complex issue involving the medium, the
          hardware, the software, and the technical capacity for
          reproductive fidelity and clarity.

           The prognosis for reliable archival standards as defined by
          librarians in the foreseeable future is poor.

     Significant potential and attractiveness of digital technology as a
     preservation medium and access mechanism.

     Productive use of digital imagery for preservation requires a
     reconceptualizing of preservation principles in a volatile,
     standardless world.

     Concept of managing continuing access in the digital environment
     rather than focusing on the permanence of the medium and longterm
     archival standards developed for the analog world.

     Transition period:  How long and what to do?

            Redefine "archival."

            Remove the burden of "archival copy" from paper artifacts.

            Use digital technology for storage, develop management
          strategies for refreshing medium, hardware and software.

            Create acidfree paper copies for transition period backup
          until we develop reliable procedures for ensuring continuing
          access to digital files.

SESSION IVD

Stuart WEIBEL            The Role of SGML Markup in the CORE Project 6

The emergence of highspeed telecommunications networks as a basic
feature of the scholarly workplace is driving the demand for electronic
document delivery.  Three distinct categories of electronic
publishingrepublishing are necessary to support access demands in this
emerging environment:

     1.  Conversion of paper or microfilm archives to electronic format
     2.  Conversion of electronic files to formats tailored to
          electronic retrieval and display
     3.  Primary electronic publishing materials for which the
          electronic version is the primary format

OCLC has experimental or product development activities in each of these
areas.  Among the challenges that lie ahead is the integration of these
three types of information stores in coherent distributed systems.

The CORE Chemistry Online Retrieval Experiment Project is a model for
the conversion of large text and graphics collections for which
electronic typesetting files are available category 2.  The American
Chemical Society has made available computer typography files dating from
1980 for its twenty journals.  This collection of some 250 journalyears
is being converted to an electronic format that will be accessible
through several enduser applications.

The use of Standard Generalized Markup Language SGML offers the means
to capture the structural richness of the original articles in a way that
will support a variety of retrieval, navigation, and display options
necessary to navigate effectively in very large text databases.

An SGML document consists of text that is marked up with descriptive tags
that specify the function of a given element within the document.  As a
formal language construct, an SGML document can be parsed against a
documenttype definition DTD that unambiguously defines what elements
are allowed and where in the document they can or must occur.  This
formalized map of article structure allows the user interface design to
be uncoupled from the underlying database system, an important step
toward interoperability.  Demonstration of this separability is a part of
the CORE project, wherein user interface designs born of very different
philosophies will access the same database.

NOTES:
     6  The CORE project is a collaboration among Cornell Universitys
     Mann Library, Bell Communications Research Bellcore, the American
     Chemical Society ACS, the Chemical Abstracts Service CAS, and
     OCLC.

Michael LESK                  The CORE Electronic Chemistry Library

A major online file of chemical journal literature complete with
graphics is being developed to test the usability of fully electronic
access to documents, as a joint project of Cornell University, the
American Chemical Society, the Chemical Abstracts Service, OCLC, and
Bellcore with additional support from Sun Microsystems, SpringerVerlag,
DigitaI Equipment Corporation, Sony Corporation of America, and Apple
Computers.  Our file contains the American Chemical Societys online
journals, supplemented with the graphics from the paper publication.  The
indexing of the articles from Chemical Abstracts Documents is available
in both image and text format, and several different interfaces can be
used.  Our goals are 1 to assess the effectiveness and acceptability of
electronic access to primary journals as compared with paper, and 2 to
identify the most desirable functions of the user interface to an
electronic system of journals, including in particular a comparison of
pageimage display with ASCII display interfaces.  Early experiments with
chemistry students on a variety of tasks suggest that searching tasks are
completed much faster with any electronic system than with paper, but
that for reading all versions of the articles are roughly equivalent.

Pamela ANDRE and Judith ZIDAR

Text conversion is far more expensive and timeconsuming than image
capture alone.  NALs experience with optical character recognition OCR
will be related and compared with the experience of having text rekeyed.
What factors affect OCR accuracy?  How accurate does full text have to be
in order to be useful?  How do different users react to imperfect text?
These are questions that will be explored.  For many, a service bureau
may be a better solution than performing the work inhouse this will also
be discussed.

SESSION VI

Marybeth PETERS

Copyright law protects creative works.  Protection granted by the law to
authors and disseminators of works includes the right to do or authorize
the following:  reproduce the work, prepare derivative works, distribute
the work to the public, and publicly perform or display the work.  In
addition, copyright owners of sound recordings and computer programs have
the right to control rental of their works.  These rights are not
unlimited there are a number of exceptions and limitations.

An electronic environment places strains on the copyright system.
Copyright owners want to control uses of their work and be paid for any
use the public wants quick and easy access at little or no cost.  The
marketplace is working in this area.  Contracts, guidelines on electronic
use, and collective licensing are in use and being refined.

Issues concerning the ability to change works without detection are more
difficult to deal with.  Questions concerning the integrity of the work
and the status of the changed version under the copyright law are to be
addressed.  These are public policy issues which require informed
dialogue.


                                 


                Appendix III:  DIRECTORY OF PARTICIPANTS


PRESENTERS:

     Pamela Q.J. Andre
     Associate Director, Automation
     National Agricultural Library
     10301 Baltimore Boulevard
     Beltsville, MD 207052351
     Phone:  301 5046813
     Fax:  301 5047473
     Email:  INTERNET:  PANDREASRR.ARSUSDA.GOV

     Jean Baronas, Senior Manager
     Department of Standards and Technology
     Association for Information and Image Management AIIM
     1100 Wayne Avenue, Suite 1100
     Silver Spring, MD 20910
     Phone:  301 5878202
     Fax:  301 5872711

     Patricia Battin, President
     The Commission on Preservation and Access
     1400 16th Street, N.W.
     Suite 740
     Washington, DC 200362217
     Phone:  202 9393400
     Fax:  202 9393407
     Email:  CPAGWUVM.BITNET

     Howard Besser
     Centre Canadien dArchitecture
     Canadian Center for Architecture
     1920, rue Baile
     Montreal, Quebec H3H 2S6
     CANADA
     Phone:  514 9397001
     Fax:  514 9397020
     Email:  howardlis.pitt.edu

     Edwin B. Brownrigg, Executive Director
     Memex Research Institute
     422 Bonita Avenue
     Roseville, CA 95678
     Phone:  916 7842298
     Fax:  916 7867559
     Email:  BITNET:  MEMEXCALSTATE.2

     Eric M. Calaluca, Vice President
     ChadwyckHealey, Inc.
     1101 King Street
     Alexandria, VA 223l4
     Phone:  800 75205l5
     Fax:  703 6837589

     James Daly
     4015 Deepwood Road
     Baltimore, MD 212181404
     Phone:  410 2350763

     Ricky Erway, Associate Coordinator
     American Memory
     Library of Congress
     Phone:  202 7076233
     Fax:  202 7073764

     Carl Fleischhauer, Coordinator
     American Memory
     Library of Congress
     Phone:  202 7076233
     Fax:  202 7073764

     Joanne Freeman
     2000 Jefferson Park Avenue, No. 7
     Charlottesville, VA  22903

     Prosser Gifford
     Director for Scholarly Programs
     Library of Congress
     Phone:  202 7071517
     Fax:  202 7079898
     Email:  pgifseq1.loc.gov

     Jacqueline Hess, Director
     National Demonstration Laboratory
       for Interactive Information Technologies
     Library of Congress
     Phone:  202 7074157
     Fax:  202 7072829

     Susan Hockey, Director
     Center for Electronic Texts in the Humanities CETH
     Alexander Library
     Rutgers University
     169 College Avenue
     New Brunswick, NJ 08903
     Phone:  908 9321384
     Fax:  908 9321386
     Email:  hockeyzodiac.rutgers.edu

     William L. Hooton, Vice President
     Business  Technical Development
       Imaging  Information Systems Group
     INET
     6430 Rockledge Drive, Suite 400
     Bethesda, MD 208l7
     Phone:  301 5646750
     Fax:  513 5646867

     Anne R. Kenney, Associate Director
     Department of Preservation and Conservation
     701 Olin Library
     Cornell University
     Ithaca, NY 14853
     Phone:  607 2556875
     Fax:  607 2559346
     Email:  LYDYCORNELLA.BITNET

     Ronald L. Larsen
     Associate Director for Information Technology
     University of Maryland at College Park
     Room B0224, McKeldin Library
     College Park, MD 207427011
     Phone:  301 4059194
     Fax:  301 3149865
     Email:  rlarsenlibr.umd.edu

     Maria L. Lebron, Managing Editor
     The Online Journal of Current Clinical Trials
     l333 H Street, N.W.
     Washington, DC 20005
     Phone:  202 3266735
     Fax:  202 8422868
     Email:  PUBSAAASGWUVM.BITNET

     Michael Lesk, Executive Director
     Computer Science Research
     Bell Communications Research, Inc.
     Rm 2A385
     445 South Street
     Morristown, NJ 07960l9l0
     Phone:  201 8294070
     Fax:  201 8295981
     Email:  leskbellcore.com Internet or bellcore!lesk uucp

     Clifford A. Lynch
     Director, Library Automation
     University of California,
        Office of the President
     300 Lakeside Drive, 8th Floor
     Oakland, CA 946123350
     Phone:  510 9870522
     Fax:  510 8393573
     Email:  caluruccmvsa

     Avra Michelson
     National Archives and Records Administration
     NSZ Rm. 14N
     7th  Pennsylvania, N.W.
     Washington, D.C. 20408
     Phone:  202 5015544
     Fax:  202 5015533
     Email:  tmicu.nih.gov

     Elli Mylonas, Managing Editor
     Perseus Project
     Department of the Classics
     Harvard University
     319 Boylston Hall
     Cambridge, MA 02138
     Phone:  617 4959025, 617 4950456 direct
     Fax:  617 4968886
     Email:  ElliIKAROS.Harvard.EDU or elliwjh12.harvard.edu

     David Woodley Packard
     Packard Humanities Institute
     300 Second Street, Suite 201
     Los Altos, CA 94002
     Phone:  415 9480150 PHI
     Fax:  415 9485793

     Lynne K. Personius, Assistant Director
     Cornell Information Technologies for
      Scholarly Information Sources
     502 Olin Library
     Cornell University
     Ithaca, NY 14853
     Phone:  607 2553393
     Fax:  607 2559346
     Email:  JRNCORNELLC.BITNET

     Marybeth Peters
     Policy Planning Adviser to the
       Register of Copyrights
     Library of Congress
     Office LM 403
     Phone:  202 7078350
     Fax:  202 7078366

     C. Michael SperbergMcQueen
     Editor, Text Encoding Initiative
     Computer Center MC 135
     University of Illinois at Chicago
     Box 6998
     Chicago, IL 60680
     Phone:  312 4130317
     Fax:  312 9966834
     Email:  u35395uicvm..cc.uic.edu or u35395uicvm.bitnet

     George R. Thoma, Chief
     Communications Engineering Branch
     National Library of Medicine
     8600 Rockville Pike
     Bethesda, MD 20894
     Phone:  301 4964496
     Fax:  301 4020341
     Email:  thomalhc.nlm.nih.gov

     Dorothy Twohig, Editor
     The Papers of George Washington
     504 Alderman Library
     University of Virginia
     Charlottesville, VA 229032498
     Phone:  804 9240523
     Fax:  804 9244337

     Susan H. Veccia, Team leader
     American Memory, User Evaluation
     Library of Congress
     American Memory Evaluation Project
     Phone:  202 7079104
     Fax:  202 7073764
     Email:  svecseq1.loc.gov

     Donald J. Waters, Head
     Systems Office
     Yale University Library
     New Haven, CT 06520
     Phone:  203 4324889
     Fax:  203 4327231
     Email:  DWATERSYALEVM.BITNET or DWATERSYALEVM.YCC.YALE.EDU

     Stuart Weibel, Senior Research Scientist
     OCLC
     6565 Frantz Road
     Dublin, OH 43017
     Phone:  614 764608l
     Fax:  614 7642344
     Email:  INTERNET:  Stursch.oclc.org

     Robert G. Zich
     Special Assistant to the Associate Librarian
       for Special Projects
     Library of Congress
     Phone:  202 7076233
     Fax:  202 7073764
     Email:  rzicseq1.loc.gov

     Judith A. Zidar, Coordinator
     National Agricultural Text Digitizing Program
     Information Systems Division
     National Agricultural Library
     10301 Baltimore Boulevard
     Beltsville, MD 207052351
     Phone:  301 5046813 or 5045853
     Fax:  301 5047473
     Email:  INTERNET:  JZIDARASRR.ARSUSDA.GOV


OBSERVERS:

     Helen Aguera, Program Officer
     Division of Research
     Room 318
     National Endowment for the Humanities
     1100 Pennsylvania Avenue, N.W.
     Washington, D.C. 20506
     Phone:  202 7860358
     Fax:  202 7860243

     M. Ellyn Blanton, Deputy Director
     National Demonstration Laboratory
       for Interactive Information Technologies
     Library of Congress
     Phone:  202 7074157
     Fax:  202 7072829

     Charles M. Dollar
     National Archives and Records Administration
     NSZ Rm. 14N
     7th  Pennsylvania, N.W.
     Washington, DC 20408
     Phone:  202 5015532
     Fax:  202 5015512

     Jeffrey Field, Deputy to the Director
     Division of Preservation and Access
     Room 802
     National Endowment for the Humanities
     1100 Pennsylvania Avenue, N.W.
     Washington, DC 20506
     Phone:  202 7860570
     Fax:  202 7860243

     Lorrin Garson
     American Chemical Society
     Research and Development Department
     1155 16th Street, N.W.
     Washington, D.C. 20036
     Phone:  202 8724541
     Fax:  Email:  INTERNET:  LRG96ACS.ORG

     William M. Holmes, Jr.
     National Archives and Records Administration
     NSZ Rm. 14N
     7th  Pennsylvania, N.W.
     Washington, DC 20408
     Phone:  202 5015540
     Fax:  202 5015512
     Email:  WHOLMESAMERICAN.EDU

     Sperling Martin
     Information Resource Management
     20030 Doolittle Street
     Gaithersburg, MD 20879
     Phone:  301 9241803

     Michael Neuman, Director
     The Center for Text and Technology
     Academic Computing Center
     238 Reiss Science Building
     Georgetown University
     Washington, DC 20057
     Phone:  202 6876096
     Fax:  202 6876003
     Email:  neumanguvax.bitnet, neumanguvax.georgetown.edu

     Barbara Paulson, Program Officer
     Division of Preservation and Access
     Room 802
     National Endowment for the Humanities
     1100 Pennsylvania Avenue, N.W.
     Washington, DC 20506
     Phone:  202 7860577
     Fax:  202 7860243

     Allen H. Renear
     Senior Academic Planning Analyst
     Brown University Computing and Information Services
     115 Waterman Street
     Campus Box 1885
     Providence, R.I. 02912
     Phone:  401 8637312
     Fax:  401 8637329
     Email:  BITNET:  AllenBROWNVM or
     INTERNET:  Allenbrownvm.brown.edu

     Susan M. Severtson, President
     ChadwyckHealey, Inc.
     1101 King Street
     Alexandria, VA 223l4
     Phone:  800 75205l5
     Fax:  703 6837589

     Frank Withrow
     U.S. Department of Education
     555 New Jersey Avenue, N.W.
     Washington, DC 202085644
     Phone:  202 2192200
     Fax:  202 2192106


LC STAFF

     Linda L. Arret
     MachineReadable Collections Reading Room LJ 132
     202 7071490

     John D. Byrum, Jr.
     Descriptive Cataloging Division LM 540
     202 7075194

     Mary Jane Cavallo
     Science and Technology Division LA 5210
     202 7071219

     Susan Thea David
     Congressional Research Service LM 226
     202 7077169

     Robert Dierker
     Senior Adviser for Multimedia Activities LM 608
     202 7076151

     William W. Ellis
     Associate Librarian for Science and Technology LM 611
     202 7076928

     Ronald Gephart
     Manuscript Division LM 102
     202 7075097

     James Graber
     Information Technology Services LM G51
     202 7079628

     Rich Greenfield
     American Memory LM 603
     202 7076233

     Rebecca Guenther
     Network Development LM 639
     202 7075092

     Kenneth E. Harris
     Preservation LM G21
     202 7075213

     Staley Hitchcock
     Manuscript Division LM 102
     202 7075383

     Bohdan Kantor
     Office of Special Projects LM 612
     202 7070180

     John W. Kimball, Jr
     MachineReadable Collections Reading Room LJ 132
     202 7076560

     Basil Manns
     Information Technology Services LM G51
     202 7078345

     Sally Hart McCallum
     Network Development LM 639
     202 7076237

     Dana J. Pratt
     Publishing Office LM 602
     202 7076027

     Jane Riefenhauser
     American Memory LM 603
     202 7076233

     William Z. Schenck
     Collections Development LM 650
     202 7077706

     Chandru J. Shahani
     Preservation Research and Testing Office RT LM G38
     202 7075607

     William J. Sittig
     Collections Development LM 650
     202 7077050

     Paul Smith
     Manuscript Division LM 102
     202 7075097

     James L. Stevens
     Information Technology Services LM G51
     202 7079688

     Karen Stuart
     Manuscript Division LM 130
     202 7075389

     Tamara Swora
     Preservation Microfilming Office LM G05
     202 7076293

     Sarah Thomas
     Collections Cataloging LM 642
     202 7075333


                                   END
      

Note:  This file has been edited for use on computer networks.  This
editing required the removal of diacritics, underlining, and fonts such
as italics and bold.

kde 1192

A few of the italics when used for emphasis were replaced by CAPS mh

End of The Project Gutenberg Etext of LOC WORKSHOP ON ELECTRONIC ETEXTS


This is the February 1992 Project Gutenberg release of:

Paradise Lost by John Milton

The oldest etext known to Project Gutenberg ca. 19641965
If you know of any older ones, please let us know.


Introduction  one page

This etext was originally created in 19641965 according to Dr.
Joseph Raben of Queens College, NY, to whom it is attributed by
Project Gutenberg.  We had heard of this etext for years but it
was not until 1991 that we actually managed to track it down to
a specific location, and then it took months to convince people
to let us have a copy, then more months for them actually to do
the copying and get it to us.  Then another month to convert to
something we could massage with our favorite 486 in DOS.  After
that is was only a matter of days to get it into this shape you
will see below.  The original was, of course, in CAPS only, and
so were all the other etexts of the 60s and early 70s.  Dont
let anyone fool you into thinking any etext with both upper and
lower case is an original all those original Project Gutenberg
etexts were also in upper case and were translated or rewritten
many times to get them into their current condition.  They have
been worked on by many people throughout the world.

In the course of our searches for Professor Raben and his etext
we were never able to determine where copies were or which of a
variety of editions he may have used as a source.  We did get a
little information here and there, but even after we received a
copy of the etext we were unwilling to release it without first
determining that it was in fact Public Domain and finding Raben
to verify this and get his permission.  Interested enough, in a
totally unrelated action to our searches for him, the professor
subscribed to the Project Gutenberg listserver and we happened,
by accident, to notice his name. We dont really look at every
subscription request as the computers usually handle them. The
etext was then properly identified, copyright analyzed, and the
current edition prepared.

To give you an estimation of the difference in the original and
what we have today:  the original was probably entered on cards
commonly known at the time as "IBM cards" Do Not Fold, Spindle
or Mutilate and probably took in excess of 100,000 of them.  A
single card could hold 80 characters hence 80 characters is an
accepted standard for so many computer margins, and the entire
original edition we received in all caps was over 800,000 chars
in length, including line enumeration, symbols for caps and the
punctuation marks, etc., since they were not available keyboard
characters at the time probably the keyboards operated at baud
rates of around 113, meaning the typists had to type slowly for
the keyboard to keep up.

This is the second version of Paradise Lost released by Project
Gutenberg.  The first was released as our October, 1991 etext.





Paradise Lost




Book I


Of Mans first disobedience, and the fruit
Of that forbidden tree whose mortal taste
Brought death into the World, and all our woe,
With loss of Eden, till one greater Man
Restore us, and regain the blissful seat,
Sing, Heavenly Muse, that, on the secret top
Of Oreb, or of Sinai, didst inspire
That shepherd who first taught the chosen seed
In the beginning how the heavens and earth
Rose out of Chaos: or, if Sion hill
Delight thee more, and Siloas brook that flowed
Fast by the oracle of God, I thence
Invoke thy aid to my adventurous song,
That with no middle flight intends to soar
Above th Aonian mount, while it pursues
Things unattempted yet in prose or rhyme.
And chiefly thou, O Spirit, that dost prefer
Before all temples th upright heart and pure,
Instruct me, for thou knowst thou from the first
Wast present, and, with mighty wings outspread,
Dovelike satst brooding on the vast Abyss,
And madst it pregnant: what in me is dark
Illumine, what is low raise and support
That, to the height of this great argument,
I may assert Eternal Providence,
And justify the ways of God to men.
  Say firstfor Heaven hides nothing from thy view,
Nor the deep tract of Hellsay first what cause
Moved our grand parents, in that happy state,
Favoured of Heaven so highly, to fall off
From their Creator, and transgress his will
For one restraint, lords of the World besides.
Who first seduced them to that foul revolt?
  Th infernal Serpent he it was whose guile,
Stirred up with envy and revenge, deceived
The mother of mankind, what time his pride
Had cast him out from Heaven, with all his host
Of rebel Angels, by whose aid, aspiring
To set himself in glory above his peers,
He trusted to have equalled the Most High,
If he opposed, and with ambitious aim
Against the throne and monarchy of God,
Raised impious war in Heaven and battle proud,
With vain attempt. Him the Almighty Power
Hurled headlong flaming from th ethereal sky,
With hideous ruin and combustion, down
To bottomless perdition, there to dwell
In adamantine chains and penal fire,
Who durst defy th Omnipotent to arms.
  Nine times the space that measures day and night
To mortal men, he, with his horrid crew,
Lay vanquished, rolling in the fiery gulf,
Confounded, though immortal. But his doom
Reserved him to more wrath for now the thought
Both of lost happiness and lasting pain
Torments him: round he throws his baleful eyes,
That witnessed huge affliction and dismay,
Mixed with obdurate pride and steadfast hate.
At once, as far as Angels ken, he views
The dismal situation waste and wild.
A dungeon horrible, on all sides round,
As one great furnace flamed yet from those flames
No light but rather darkness visible
Served only to discover sights of woe,
Regions of sorrow, doleful shades, where peace
And rest can never dwell, hope never comes
That comes to all, but torture without end
Still urges, and a fiery deluge, fed
With everburning sulphur unconsumed.
Such place Eternal Justice has prepared
For those rebellious here their prison ordained
In utter darkness, and their portion set,
As far removed from God and light of Heaven
As from the centre thrice to th utmost pole.
Oh how unlike the place from whence they fell!
There the companions of his fall, oerwhelmed
With floods and whirlwinds of tempestuous fire,
He soon discerns and, weltering by his side,
One next himself in power, and next in crime,
Long after known in Palestine, and named
Beelzebub. To whom th ArchEnemy,
And thence in Heaven called Satan, with bold words
Breaking the horrid silence, thus began:
  "If thou beest hebut O how fallen! how changed
From him who, in the happy realms of light
Clothed with transcendent brightness, didst outshine
Myriads, though bright!if he whom mutual league,
United thoughts and counsels, equal hope
And hazard in the glorious enterprise
Joined with me once, now misery hath joined
In equal ruin into what pit thou seest
From what height fallen: so much the stronger proved
He with his thunder and till then who knew
The force of those dire arms? Yet not for those,
Nor what the potent Victor in his rage
Can else inflict, do I repent, or change,
Though changed in outward lustre, that fixed mind,
And high disdain from sense of injured merit,
That with the Mightiest raised me to contend,
And to the fierce contentions brought along
Innumerable force of Spirits armed,
That durst dislike his reign, and, me preferring,
His utmost power with adverse power opposed
In dubious battle on the plains of Heaven,
And shook his throne. What though the field be lost?
All is not lostthe unconquerable will,
And study of revenge, immortal hate,
And courage never to submit or yield:
And what is else not to be overcome?
That glory never shall his wrath or might
Extort from me. To bow and sue for grace
With suppliant knee, and deify his power
Who, from the terror of this arm, so late
Doubted his empirethat were low indeed
That were an ignominy and shame beneath
This downfall since, by fate, the strength of Gods,
And this empyreal sybstance, cannot fail
Since, through experience of this great event,
In arms not worse, in foresight much advanced,
We may with more successful hope resolve
To wage by force or guile eternal war,
Irreconcilable to our grand Foe,
Who now triumphs, and in th excess of joy
Sole reigning holds the tyranny of Heaven."
  So spake th apostate Angel, though in pain,
Vaunting aloud, but racked with deep despair
And him thus answered soon his bold compeer:
  "O Prince, O Chief of many throned Powers
That led th embattled Seraphim to war
Under thy conduct, and, in dreadful deeds
Fearless, endangered Heavens perpetual King,
And put to proof his high supremacy,
Whether upheld by strength, or chance, or fate,
Too well I see and rue the dire event
That, with sad overthrow and foul defeat,
Hath lost us Heaven, and all this mighty host
In horrible destruction laid thus low,
As far as Gods and heavenly Essences
Can perish: for the mind and spirit remains
Invincible, and vigour soon returns,
Though all our glory extinct, and happy state
Here swallowed up in endless misery.
But what if he our Conqueror whom I now
Of force believe almighty, since no less
Than such could have oerpowered such force as ours
Have left us this our spirit and strength entire,
Strongly to suffer and support our pains,
That we may so suffice his vengeful ire,
Or do him mightier service as his thralls
By right of war, whateer his business be,
Here in the heart of Hell to work in fire,
Or do his errands in the gloomy Deep?
What can it the avail though yet we feel
Strength undiminished, or eternal being
To undergo eternal punishment?"
  Whereto with speedy words th ArchFiend replied:
"Fallen Cherub, to be weak is miserable,
Doing or suffering: but of this be sure
To do aught good never will be our task,
But ever to do ill our sole delight,
As being the contrary to his high will
Whom we resist. If then his providence
Out of our evil seek to bring forth good,
Our labour must be to pervert that end,
And out of good still to find means of evil
Which ofttimes may succeed so as perhaps
Shall grieve him, if I fail not, and disturb
His inmost counsels from their destined aim.
But see! the angry Victor hath recalled
His ministers of vengeance and pursuit
Back to the gates of Heaven: the sulphurous hail,
Shot after us in storm, oerblown hath laid
The fiery surge that from the precipice
Of Heaven received us falling and the thunder,
Winged with red lightning and impetuous rage,
Perhaps hath spent his shafts, and ceases now
To bellow through the vast and boundless Deep.
Let us not slip th occasion, whether scorn
Or satiate fury yield it from our Foe.
Seest thou yon dreary plain, forlorn and wild,
The seat of desolation, void of light,
Save what the glimmering of these livid flames
Casts pale and dreadful? Thither let us tend
From off the tossing of these fiery waves
There rest, if any rest can harbour there
And, reassembling our afflicted powers,
Consult how we may henceforth most offend
Our enemy, our own loss how repair,
How overcome this dire calamity,
What reinforcement we may gain from hope,
If not, what resolution from despair."
  Thus Satan, talking to his nearest mate,
With head uplift above the wave, and eyes
That sparkling blazed his other parts besides
Prone on the flood, extended long and large,
Lay floating many a rood, in bulk as huge
As whom the fables name of monstrous size,
Titanian or Earthborn, that warred on Jove,
Briareos or Typhon, whom the den
By ancient Tarsus held, or that seabeast
Leviathan, which God of all his works
Created hugest that swim th oceanstream.
Him, haply slumbering on the Norway foam,
The pilot of some small nightfoundered skiff,
Deeming some island, oft, as seamen tell,
With fixed anchor in his scaly rind,
Moors by his side under the lee, while night
Invests the sea, and wished morn delays.
So stretched out huge in length the Archfiend lay,
Chained on the burning lake nor ever thence
Had risen, or heaved his head, but that the will
And high permission of allruling Heaven
Left him at large to his own dark designs,
That with reiterated crimes he might
Heap on himself damnation, while he sought
Evil to others, and enraged might see
How all his malice served but to bring forth
Infinite goodness, grace, and mercy, shewn
On Man by him seduced, but on himself
Treble confusion, wrath, and vengeance poured.
  Forthwith upright he rears from off the pool
His mighty stature on each hand the flames
Driven backward slope their pointing spires, and,rolled
In billows, leave i th midst a horrid vale.
Then with expanded wings he steers his flight
Aloft, incumbent on the dusky air,
That felt unusual weight till on dry land
He lightsif it were land that ever burned
With solid, as the lake with liquid fire,
And such appeared in hue as when the force
Of subterranean wind transprots a hill
Torn from Pelorus, or the shattered side
Of thundering Etna, whose combustible
And fuelled entrails, thence conceiving fire,
Sublimed with mineral fury, aid the winds,
And leave a singed bottom all involved
With stench and smoke. Such resting found the sole
Of unblest feet. Him followed his next mate
Both glorying to have scaped the Stygian flood
As gods, and by their own recovered strength,
Not by the sufferance of supernal Power.
  "Is this the region, this the soil, the clime,"
Said then the lost Archangel, "this the seat
That we must change for Heaven?this mournful gloom
For that celestial light? Be it so, since he
Who now is sovereign can dispose and bid
What shall be right: farthest from him is best
Whom reason hath equalled, force hath made supreme
Above his equals. Farewell, happy fields,
Where joy for ever dwells! Hail, horrors! hail,
Infernal world! and thou, profoundest Hell,
Receive thy new possessorone who brings
A mind not to be changed by place or time.
The mind is its own place, and in itself
Can make a Heaven of Hell, a Hell of Heaven.
What matter where, if I be still the same,
And what I should be, all but less than he
Whom thunder hath made greater? Here at least
We shall be free th Almighty hath not built
Here for his envy, will not drive us hence:
Here we may reigh secure and, in my choice,
To reign is worth ambition, though in Hell:
Better to reign in Hell than serve in Heaven.
But wherefore let we then our faithful friends,
Th associates and copartners of our loss,
Lie thus astonished on th oblivious pool,
And call them not to share with us their part
In this unhappy mansion, or once more
With rallied arms to try what may be yet
Regained in Heaven, or what more lost in Hell?"
  So Satan spake and him Beelzebub
Thus answered:"Leader of those armies bright
Which, but th Omnipotent, none could have foiled!
If once they hear that voice, their liveliest pledge
Of hope in fears and dangersheard so oft
In worst extremes, and on the perilous edge
Of battle, when it raged, in all assaults
Their surest signalthey will soon resume
New courage and revive, though now they lie
Grovelling and prostrate on yon lake of fire,
As we erewhile, astounded and amazed
No wonder, fallen such a pernicious height!"
  He scare had ceased when the superior Fiend
Was moving toward the shore his ponderous shield,
Ethereal temper, massy, large, and round,
Behind him cast. The broad circumference
Hung on his shoulders like the moon, whose orb
Through optic glass the Tuscan artist views
At evening, from the top of Fesole,
Or in Valdarno, to descry new lands,
Rivers, or mountains, in her spotty globe.
His spearto equal which the tallest pine
Hewn on Norwegian hills, to be the mast
Of some great ammiral, were but a wand
He walked with, to support uneasy steps
Over the burning marl, not like those steps
On Heavens azure and the torrid clime
Smote on him sore besides, vaulted with fire.
Nathless he so endured, till on the beach
Of that inflamed sea he stood, and called
His legionsAngel Forms, who lay entranced
Thick as autumnal leaves that strow the brooks
In Vallombrosa, where th Etrurian shades
High overarched embower or scattered sedge
Afloat, when with fierce winds Orion armed
Hath vexed the RedSea coast, whose waves oerthrew
Busiris and his Memphian chivalry,
While with perfidious hatred they pursued
The sojourners of Goshen, who beheld
From the safe shore their floating carcases
And broken chariotwheels. So thick bestrown,
Abject and lost, lay these, covering the flood,
Under amazement of their hideous change.
He called so loud that all the hollow deep
Of Hell resounded:"Princes, Potentates,
Warriors, the Flower of Heavenonce yours now lost,
If such astonishment as this can seize
Eternal Spirits! Or have ye chosen this place
After the toil of battle to repose
Your wearied virtue, for the ease you find
To slumber here, as in the vales of Heaven?
Or in this abject posture have ye sworn
To adore the Conqueror, who now beholds
Cherub and Seraph rolling in the flood
With scattered arms and ensigns, till anon
His swift pursuers from Heavengates discern
Th advantage, and, descending, tread us down
Thus drooping, or with linked thunderbolts
Transfix us to the bottom of this gulf?
Awake, arise, or be for ever fallen!"
  They heard, and were abashed, and up they sprung
Upon the wing, as when men wont to watch
On duty, sleeping found by whom they dread,
Rouse and bestir themselves ere well awake.
Nor did they not perceive the evil plight
In which they were, or the fierce pains not feel
Yet to their Generals voice they soon obeyed
Innumerable. As when the potent rod
Of Amrams son, in Egypts evil day,
Waved round the coast, upcalled a pitchy cloud
Of locusts, warping on the eastern wind,
That oer the realm of impious Pharaoh hung
Like Night, and darkened all the land of Nile
So numberless were those bad Angels seen
Hovering on wing under the cope of Hell,
Twixt upper, nether, and surrounding fires
Till, as a signal given, th uplifted spear
Of their great Sultan waving to direct
Their course, in even balance down they light
On the firm brimstone, and fill all the plain:
A multitude like which the populous North
Poured never from her frozen loins to pass
Rhene or the Danaw, when her barbarous sons
Came like a deluge on the South, and spread
Beneath Gibraltar to the Libyan sands.
Forthwith, form every squadron and each band,
The heads and leaders thither haste where stood
Their great Commandergodlike Shapes, and Forms
Excelling human princely Dignities
And Powers that erst in Heaven sat on thrones,
Though on their names in Heavenly records now
Be no memorial, blotted out and rased
By their rebellion from the Books of Life.
Nor had they yet among the sons of Eve
Got them new names, till, wandering oer the earth,
Through Gods high sufferance for the trial of man,
By falsities and lies the greatest part
Of mankind they corrupted to forsake
God their Creator, and th invisible
Glory of him that made them to transform
Oft to the image of a brute, adorned
With gay religions full of pomp and gold,
And devils to adore for deities:
Then were they known to men by various names,
And various idols through the heathen world.
  Say, Muse, their names then known, who first, who last,
Roused from the slumber on that fiery couch,
At their great Emperors call, as next in worth
Came singly where he stood on the bare strand,
While the promiscuous crowd stood yet aloof?
  The chief were those who, from the pit of Hell
Roaming to seek their prey on Earth, durst fix
Their seats, long after, next the seat of God,
Their altars by his altar, gods adored
Among the nations round, and durst abide
Jehovah thundering out of Sion, throned
Between the Cherubim yea, often placed
Within his sanctuary itself their shrines,
Abominations and with cursed things
His holy rites and solemn feasts profaned,
And with their darkness durst affront his light.
First, Moloch, horrid king, besmeared with blood
Of human sacrifice, and parents tears
Though, for the noise of drums and timbrels loud,
Their childrens cries unheard that passed through fire
To his grim idol. Him the Ammonite
Worshiped in Rabba and her watery plain,
In Argob and in Basan, to the stream
Of utmost Arnon. Nor content with such
Audacious neighbourhood, the wisest heart
Of Solomon he led by fraoud to build
His temple right against the temple of God
On that opprobrious hill, and made his grove
The pleasant valley of Hinnom, Tophet thence
And black Gehenna called, the type of Hell.
Next Chemos, th obscene dread of Moabs sons,
From Aroar to Nebo and the wild
Of southmost Abarim in Hesebon
And Horonaim, Seons real, beyond
The flowery dale of Sibma clad with vines,
And Eleale to th Asphaltic Pool:
Peor his other name, when he enticed
Israel in Sittim, on their march from Nile,
To do him wanton rites, which cost them woe.
Yet thence his lustful orgies he enlarged
Even to that hill of scandal, by the grove
Of Moloch homicide, lust hard by hate,
Till good Josiah drove them thence to Hell.
With these came they who, from the bordering flood
Of old Euphrates to the brook that parts
Egypt from Syrian ground, had general names
Of Baalim and Ashtaroththose male,
These feminine. For Spirits, when they please,
Can either sex assume, or both so soft
And uncompounded is their essence pure,
Not tried or manacled with joint or limb,
Nor founded on the brittle strength of bones,
Like cumbrous flesh but, in what shape they choose,
Dilated or condensed, bright or obscure,
Can execute their airy purposes,
And works of love or enmity fulfil.
For those the race of Israel oft forsook
Their Living Strength, and unfrequented left
His righteous altar, bowing lowly down
To bestial gods for which their heads as low
Bowed down in battle, sunk before the spear
Of despicable foes. With these in troop
Came Astoreth, whom the Phoenicians called
Astarte, queen of heaven, with crescent horns
To whose bright image nigntly by the moon
Sidonian virgins paid their vows and songs
In Sion also not unsung, where stood
Her temple on th offensive mountain, built
By that uxorious king whose heart, though large,
Beguiled by fair idolatresses, fell
To idols foul. Thammuz came next behind,
Whose annual wound in Lebanon allured
The Syrian damsels to lament his fate
In amorous ditties all a summers day,
While smooth Adonis from his native rock
Ran purple to the sea, supposed with blood
Of Thammuz yearly wounded: the lovetale
Infected Sions daughters with like heat,
Whose wanton passions in the sacred proch
Ezekiel saw, when, by the vision led,
His eye surveyed the dark idolatries
Of alienated Judah. Next came one
Who mourned in earnest, when the captive ark
Maimed his brute image, head and hands lopt off,
In his own temple, on the grunseledge,
Where he fell flat and shamed his worshippers:
Dagon his name, seamonster,upward man
And downward fish yet had his temple high
Reared in Azotus, dreaded through the coast
Of Palestine, in Gath and Ascalon,
And Accaron and Gazas frontier bounds.
Him followed Rimmon, whose delightful seat
Was fair Damascus, on the fertile banks
Of Abbana and Pharphar, lucid streams.
He also against the house of God was bold:
A leper once he lost, and gained a king
Ahaz, his sottish conqueror, whom he drew
Gods altar to disparage and displace
For one of Syrian mode, whereon to burn
His odious offerings, and adore the gods
Whom he had vanquished. After these appeared
A crew who, under names of old renown
Osiris, Isis, Orus, and their train
With monstrous shapes and sorceries abused
Fanatic Egypt and her priests to seek
Their wandering gods disguised in brutish forms
Rather than human. Nor did Israel scape
Th infection, when their borrowed gold composed
The calf in Oreb and the rebel king
Doubled that sin in Bethel and in Dan,
Likening his Maker to the grazed ox
Jehovah, who, in one night, when he passed
From Egypt marching, equalled with one stroke
Both her firstborn and all her bleating gods.
Belial came last than whom a Spirit more lewd
Fell not from Heaven, or more gross to love
Vice for itself. To him no temple stood
Or altar smoked yet who more oft than he
In temples and at altars, when the priest
Turns atheist, as did Elis sons, who filled
With lust and violence the house of God?
In courts and palaces he also reigns,
And in luxurious cities, where the noise
Of riot ascends above their loftiest towers,
And injury and outrage and, when night
Darkens the streets, then wander forth the sons
Of Belial, flown with insolence and wine.
Witness the streets of Sodom, and that night
In Gibeah, when the hospitable door
Exposed a matron, to avoid worse rape.
  These were the prime in order and in might:
The rest were long to tell though far renowned
Th Ionian godsof Javans issue held
Gods, yet confessed later than Heaven and Earth,
Their boasted parentsTitan, Heavens firstborn,
With his enormous brood, and birthright seized
By younger Saturn: he from mightier Jove,
His own and Rheas son, like measure found
So Jove usurping reigned. These, first in Crete
And Ida known, thence on the snowy top
Of cold Olympus ruled the middle air,
Their highest heaven or on the Delphian cliff,
Or in Dodona, and through all the bounds
Of Doric land or who with Saturn old
Fled over Adria to th Hesperian fields,
And oer the Celtic roamed the utmost Isles.
  All these and more came flocking but with looks
Downcast and damp yet such wherein appeared
Obscure some glimpse of joy to have found their Chief
Not in despair, to have found themselves not lost
In loss itself which on his countenance cast
Like doubtful hue. But he, his wonted pride
Soon recollecting, with high words, that bore
Semblance of worth, not substance, gently raised
Their fainting courage, and dispelled their fears.
Then straight commands that, at the warlike sound
Of trumpets loud and clarions, be upreared
His mighty standard. That proud honour claimed
Azazel as his right, a Cherub tall:
Who forthwith from the glittering staff unfurled
Th imperial ensign which, full high advanced,
Shone like a meteor streaming to the wind,
With gems and golden lustre rich emblazed,
Seraphic arms and trophies all the while
Sonorous metal blowing martial sounds:
At which the universal host upsent
A shout that tore Hells concave, and beyond
Frighted the reign of Chaos and old Night.
All in a moment through the gloom were seen
Ten thousand banners rise into the air,
With orient colours waving: with them rose
A forest huge of spears and thronging helms
Appeared, and serried shields in thick array
Of depth immeasurable. Anon they move
In perfect phalanx to the Dorian mood
Of flutes and soft recorderssuch as raised
To height of noblest temper heroes old
Arming to battle, and instead of rage
Deliberate valour breathed, firm, and unmoved
With dread of death to flight or foul retreat
Nor wanting power to mitigate and swage
With solemn touches troubled thoughts, and chase
Anguish and doubt and fear and sorrow and pain
From mortal or immortal minds. Thus they,
Breathing united force with fixed thought,
