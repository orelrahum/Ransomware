disadvantage of working with overseas keyers is that they are much less
likely to telephone with questions especially on the coding with the
result that they develop their own rules as they encounter new
situations

Government contracting procedures and time frames posed a major challenge
to performing the conversion  Many service bureaus are not accustomed to
retaining the image even if they perform OCR  Thus questions of image
format and storage media were somewhat novel to many of them  ERWAY also
remarked other problems in dealing with service bureaus for example
their inability to perform text conversion from the kind of microfilm
that LC uses for preservation purposes

But quality control in ERWAYs experience was the most timeconsuming
aspect of contracting out conversion  AM has been attempting to perform
a percent quality review looking at either every tenth document or
every tenth page to make certain that the service bureaus are maintaining
 percent accuracy  But even if they are complying with the
requirement for accuracy finding errors produces a desire to correct
them and in turn to clean up the whole collection which defeats the
purpose to some extent  Even a double entry requires a
characterbycharacter comparison to the original to meet the accuracy
requirement  LC is not accustomed to publish imperfect texts which
makes attempting to deal with the industry standard an emotionally
fraught issue for AM  As was mentioned in the previous days discussion
going from  to  percent accuracy usually doubles costs and
means a third keying or another complete runthrough of the text

Although AM has learned much from its experiences with various collections
and various service bureaus ERWAY concluded pessimistically that no
breakthrough has been achieved   Incremental improvements have occurred
in some of the OCR technology some of the processes and some of the
standards acceptances which though they may lead to somewhat lower costs
do not offer much encouragement to many people who are anxiously awaiting
the day that the entire contents of LC are available online

                                 


ZIDAR  Several answers to why one attempts to perform fulltext
conversion  Per page cost of performing OCR  Typical problems
encountered during editing  Editing poor copy OCR vs rekeying 


Judith ZIDAR coordinator National Agricultural Text Digitizing Program
NATDP National Agricultural Library NAL offered several answers to
the question of why one attempts to perform fulltext conversion  
Text in an image can be read by a human but not by a computer so of
course it is not searchable and there is not much one can do with it  
Some material simply requires wordlevel access  For instance the legal
profession insists on fulltext access to its material with taxonomic or
geographic material which entails numerous names one virtually requires
wordlevel access   Full text permits rapid browsing and searching
something that cannot be achieved in an image with todays technology
 Text stored as ASCII and delivered in ASCII is standardized and highly
portable   People just want fulltext searching even those who do not
know how to do it  NAL for the most part is performing OCR at an
actual cost per averagesize page of approximately   NAL scans the
page to create the electronic image and passes it through the OCR device

ZIDAR next rehearsed several typical problems encountered during editing
Praising the celerity of her student workers ZIDAR observed that editing
requires approximately five to ten minutes per page assuming that there
are no large tables to audit  Confusion among the three characters I 
and l constitutes perhaps the most common problem encountered  Zeroes
and  Os also are  frequently confused  Double Ms create a particular
problem even on clean pages  They are so wide in most fonts that they
touch and the system simply cannot tell where one letter ends and the
other begins  Complex page formats occasionally fail to columnate
properly which entails rescanning as though one were working with a
single column entering the ASCII and decolumnating for better
searching  With proportionally spaced text OCR can have difficulty
discerning what is a space and what are merely spaces between letters as
opposed to spaces between words and therefore will merge text or break
up words where it should not

ZIDAR said that it can often take longer to edit a poorcopy OCR than to
key it from scratch  NAL has also experimented with partial editing of
text whereby project workers go into and clean up the format removing
stray characters but not running a spellcheck  NAL corrects typos in
the title and authors names which provides a foothold for searching and
browsing  Even extremely poorquality OCR eg percent accuracy
can still be searched because numerous words are correct while the
important words are probably repeated often enough that they are likely
to be found correct somewhere  Librarians however cannot tolerate this
situation though end users seem more willing to use this text for
searching provided that NAL indicates that it is unedited  ZIDAR
concluded that rekeying of text may be the best route to take in spite
of numerous problems with quality control and cost

                                 


DISCUSSION  Modifying an image before performing OCR  NALs costs per
page AMs costs per page and experience with Federal Prison Industries 
Elements comprising NATDPs costs per page  OCR and structured markup 
Distinction between the structure of a document and its representation
when put on the screen or printed 


HOOTON prefaced the lengthy discussion that followed with several
comments about modifying an image before one reaches the point of
performing OCR  For example in regard to an application containing a
significant amount of redundant data such as formtype data numerous
companies today are working on various kinds of form renewal prior to
going through a recognition process by using dropout colors  Thus
acquiring access to form design or using electronic means are worth
considering  HOOTON also noted that conversion usually makes or breaks
ones imaging system  It is extremely important extremely costly in
terms of either capital investment or service and determines the quality
of the remainder of ones system because it determines the character of
the raw material used by the system

Concerning the four projects undertaken by NAL two inside and two
performed by outside contractors ZIDAR revealed that an inhouse service
bureau executed the first at a cost between  and  per page for
everything including building of the database  The project undertaken
by the Consultative Group on International Agricultural Research CGIAR
cost approximately  per page for the conversion plus some expenses
for the software and building of the database  The Acid Rain Projecta
twodisk set produced by the University of Vermont consisting of
Canadian publications on acid raincost  per page for everything
including keying of the text which was double keyed scanning of the
images and building of the database  The inhouse project offered
considerable ease of convenience and greater control of the process  On
the other hand the service bureaus know their job and perform it
expeditiously because they have more people

As a useful comparison ERWAY revealed AMs costs as follows  
cents to  cents per thousand characters with an average page
containing  characters  Requirements for coding and imaging
increase the costs  Thus conversion of the text including the coding
costs approximately  per page  This figure does not include the
imaging and databasebuilding included in the NAL costs  AM also
enjoyed a happy experience with Federal Prison Industries which
precluded the necessity of going through the requestforproposal process
to award a contract because it is another government agency  The
prisoners performed AMs rekeying just as well as other service bureaus
and proved handy as well  AM shipped them the books which they would
photocopy on a bookedge scanner  They would perform the markup on
photocopies return the books as soon as they were done with them
perform the keying and return the material to AM on WORM disks

ZIDAR detailed the elements that constitute the previously noted cost of
approximately  per page  Most significant is the editing correction
of errors and spellcheckings which though they may sound easy to
perform require in fact a great deal of time  Reformatting text also
takes a while but a significant amount of NALs expenses are for equipment
which was extremely expensive when purchased because it was one of the few
systems on the market  The costs of equipment are being amortized over
five years but are still quite high nearly  per month

HOCKEY raised a general question concerning OCR and the amount of editing
required substantial in her experience to generate the kind of
structured markup necessary for manipulating the text on the computer or
loading it into any retrieval system  She wondered if the speakers could
extend the previous question about the costbenefit of adding or exerting
structured markup  ERWAY noted that several OCR systems retain italics
bolding and other spatial formatting  While the material may not be in
the format desired these systems possess the ability to remove the
original materials quickly from the hands of the people performing the
conversion as well as to retain that information so that users can work
with it  HOCKEY rejoined that the current thinking on markup is that one
should not say that something is italic or bold so much as why it is that
way  To be sure one needs to know that something was italicized but
how can one get from one to the other?  One can map from the structure to
the typographic representation

FLEISCHHAUER suggested that given the  million items the Library
holds it may not be possible for LC to do more than report that a thing
was in italics as opposed to why it was italics although that may be
desirable in some contexts  Promising to talk a bit during the afternoon
session about several experiments OCLC performed on automatic recognition
of document elements and which they hoped to extend WEIBEL said that in
fact one can recognize the major elements of a document with a fairly
high degree of reliability at least as good as OCR  STEVENS drew a
useful distinction between standard generalized markup ie defining
for a documenttype definition the structure of the document and what
he termed a style sheet which had to do with italics bolding and other
forms of emphasis  Thus two different components are at work one being
the structure of the document itself its logic and the other being its
representation when it is put on the screen or printed

                                 

SESSION V  APPROACHES TO PREPARING ELECTRONIC TEXTS


HOCKEY  Text in ASCII and the representation of electronic text versus
an image  The need to look at ways of using markup to assist retrieval 
The need for an encoding format that will be reusable and multifunctional


Susan HOCKEY director Center for Electronic Texts in the Humanities
CETH Rutgers and Princeton Universities announced that one talk
WEIBELs was moved into this session from the morning and that David
Packard was unable to attend  The session would attempt to focus more on
what one can do with a text in ASCII and the representation of electronic
text rather than just an image what one can do with a computer that
cannot be done with a book or an image  It would be argued that one can
do much more than just read a text and from that starting point one can
use markup and methods of preparing the text to take full advantage of
the capability of the computer  That would lead to a discussion of what
the European Community calls REUSABILITY what may better be termed
DURABILITY that is how to prepare or make a text that will last a long
time and that can be used for as many applications as possible which
would lead to issues of improving intellectual access

HOCKEY urged the need to look at ways of using markup to facilitate retrieval
not just for referencing or to help locate an item that is retrieved but also to put markup tags in
a text to help retrieve the thing sought either with linguistic tagging or
interpretation  HOCKEY also argued that little advancement had occurred in
the software tools currently available for retrieving and searching text
She pressed the desideratum of going beyond Boolean searches and performing
more sophisticated searching which the insertion of more markup in the text
would facilitate  Thinking about electronic texts as opposed to images means
considering material that will never appear in print form or print will not
be its primary form that is material which only appears in electronic form
HOCKEY alluded to the history and the need for markup and tagging and
electronic text which was developed through the use of computers in the
humanities as MICHELSON had observed Father Busa had started in 
to prepare the firstever text on the computer

HOCKEY remarked several large projects particularly in Europe for the
compilation of dictionaries language studies and language analysis in
which people have built up archives of text and have begun to recognize
the need for an encoding format that will be reusable and multifunctional
that can be used not just to print the text which may be assumed to be a
byproduct of what one wants to do but to structure it inside the computer
so that it can be searched built into a Hypertext system etc

                                 


WEIBEL  OCLCs approach to preparing electronic text  retroconversion
keying of texts more automated ways of developing data  Project ADAPT
and the CORE Project  Intelligent character recognition does not exist 
Advantages of SGML  Data should be free of procedural markup
descriptive markup strongly advocated  OCLCs interface illustrated 
Storage requirements and costs for putting a lot of information on line 


Stuart WEIBEL senior research scientist Online Computer Library Center
Inc OCLC described OCLCs approach to preparing electronic text  He
argued that the electronic world into which we are moving must
accommodate not only the future but the past as well and to some degree
even the present  Thus starting out at one end with retroconversion and
keying of texts one would like to move toward much more automated ways
of developing data

For example Project ADAPT had to do with automatically converting
document images into a structured document database with OCR text as
indexing and also a little bit of automatic formatting and tagging of
that text  The CORE project hosted by Cornell University Bellcore
OCLC the American Chemical Society and Chemical Abstracts constitutes
WEIBELs principal concern at the moment  This project is an example of
converting text for which one already has a machinereadable version into
a format more suitable for electronic delivery and database searching
Since Michael LESK had previously described CORE WEIBEL would say
little concerning it  Borrowing a chemical phrase de novo synthesis
WEIBEL cited the Online Journal of Current Clinical Trials as an example
of de novo electronic publishing that is a form in which the primary
form of the information is electronic

Project ADAPT then which OCLC completed a couple of years ago and in
fact is about to resume is a model in which one takes page images either
in paper or microfilm and converts them automatically to a searchable
electronic database either online or local  The operating assumption
is that accepting some blemishes in the data especially for
retroconversion of materials will make it possible to accomplish more
Not enough money is available to support perfect conversion

WEIBEL related several steps taken to perform image preprocessing
processing on the image before performing optical character
recognition as well as image postprocessing  He denied the existence
of intelligent character recognition and asserted that what is wanted is
page recognition which is a long way off  OCLC has experimented with
merging of multiple optical character recognition systems that will
reduce errors from an unacceptable rate of  characters out of every
l to an unacceptable rate of  characters out of every l but it
is not good enough  It will never be perfect

Concerning the CORE Project WEIBEL observed that Bellcore is taking the
topography files extracting the page images and converting those
topography files to SGML markup  LESK hands that data off to OCLC which
builds that data into a Newton database the same system that underlies
the online system in virtually all of the reference products at OCLC
The longterm goal is to make the systems interoperable so that not just
Bellcores system and OCLCs system can access this data but other
systems can as well and the key to that is the Z common command
language and the fulltext extension  Z is fine for MARC records
but is not enough to do it for full text that is make full texts
interoperable

WEIBEL next outlined the critical role of SGML for a variety of purposes
for example as noted by HOCKEY in the world of extremely large
databases using highly structured data to perform field searches
WEIBEL argued that by building the structure of the data in ie the
structure of the data originally on a printed page it becomes easy to
look at a journal article even if one cannot read the characters and know
where the title or author is or what the sections of that document would be
OCLC wants to make that structure explicit in the database because it will
be important for retrieval purposes

The second big advantage of SGML is that it gives one the ability to
build structure into the database that can be used for display purposes
without contaminating the data with instructions about how to format
things  The distinction lies between procedural markup which tells one
where to put dots on the page and descriptive markup which describes
the elements of a document

WEIBEL believes that there should be no procedural markup in the data at
all that the data should be completely unsullied by information about
italics or boldness  That should be left up to the display device
whether that display device is a page printer or a screen display device
By keeping ones database free of that kind of contamination one can
make decisions down the road for example reorganize the data in ways
that are not cramped by builtin notions of what should be italic and
what should be bold  WEIBEL strongly advocated descriptive markup  As
an example he illustrated the index structure in the CORE data  With
subsequent illustrated examples of markup WEIBEL acknowledged the common
complaint that SGML is hard to read in its native form although markup
decreases considerably once one gets into the body  Without the markup
however one would not have the structure in the data  One can pass
markup through a LaTeX processor and convert it relatively easily to a
printed version of the document

WEIBEL next illustrated an extremely cluttered screen dump of OCLCs
system in order to show as much as possible the inherent capability on
the screen  He noted parenthetically that he had become a supporter of
XWindows as a result of the progress of the CORE Project  WEIBEL also
illustrated the two major parts of the interface  l a control box that
allows one to generate lists of items which resembles a small table of
contents based on key words one wishes to search and  a document
viewer which is a separate process in and of itself  He demonstrated
how to follow links through the electronic database simply by selecting
the appropriate button and bringing them up  He also noted problems that
remain to be accommodated in the interface eg as pointed out by LESK
what happens when users do not click on the icon for the figure

Given the constraints of time WEIBEL omitted a large number of ancillary
items in order to say a few words concerning storage requirements and
what will be required to put a lot of things on line  Since it is
extremely expensive to reconvert all of this data especially if it is
just in paper form and even if it is in electronic form in typesetting
tapes he advocated building journals electronically from the start  In
that case if one only has text graphics and indexing which is all that
one needs with de novo electronic publishing because there is no need to
go back and look at bitmaps of pages one can get  journals of
full text or almost  million pages per year  These pages can be put in
approximately  gigabytes of storage which is not all that much
WEIBEL said  For twenty years something less than three terabytes would
be required  WEIBEL calculated the costs of storing this information as
follows  If a gigabyte costs approximately  then a terabyte costs
approximately  million to buy in terms of hardware  One also needs a
building to put it in and a staff like OCLC to handle that information
So to support a terabyte multiply by five which gives  million per
year for a supported terabyte of data

                                 


DISCUSSION  Tapes saved by ACS are the typography files originally
supporting publication of the journal  Cost of building tagged text into
the database 


During the questionandanswer period that followed WEIBELs
presentation these clarifications emerged  The tapes saved by the
American Chemical Society are the typography files that originally
supported the publication of the journal  Although they are not tagged
in SGML they are tagged in very fine detail  Every single sentence is
marked all the registry numbers all the publications issues dates and
volumes  No cost figures on tagging material on a permegabyte basis
were available  Because ACSs typesetting system runs from tagged text
there is no extra cost per article  It was unknown what it costs ACS to
keyboard the tagged text rather than just keyboard the text in the
cheapest process  In other words since one intends to publish things
and will need to build tagged text into a typography system in any case
if one does that in such a way that it can drive not only typography but
an electronic system which is what ACS intends to domove to SGML
publishing the marginal cost is zero  The marginal cost represents the
cost of building tagged text into the database which is small

                                 


SPERBERGMcQUEEN  Distinction between texts and computers  Implications
of recognizing that all representation is encoding  Dealing with
complicated representations of text entails the need for a grammar of
documents  Variety of forms of formal grammars  Text as a bitmapped
image does not represent a serious attempt to represent text in
electronic form  SGML the TEI documenttype declarations and the
reusability and longevity of data  TEI conformance explicitly allows
extension or modification of the TEI tag set  Administrative background
of the TEI  Several design goals for the TEI tag set  An absolutely
fixed requirement of the TEI Guidelines  Challenges the TEI has
attempted to face  Good texts not beyond economic feasibility  The
issue of reproducibility or processability  The issue of mages as
simulacra for the text redux  Ones model of text determines what ones
software can do with a text and has economic consequences 


Prior to speaking about SGML and markup Michael SPERBERGMcQUEEN editor
Text Encoding Initiative TEI University of IllinoisChicago first drew
a distinction between texts and computers  Texts are abstract cultural
and linguistic objects while computers are complicated physical devices
he said  Abstract objects cannot be placed inside physical devices with
computers one can only represent text and act upon those representations

The recognition that all representation is encoding SPERBERGMcQUEEN
argued leads to the recognition of two things   The topic description
for this session is slightly misleading because there can be no discussion
of pros and cons of textcoding unless what one means is pros and cons of
working with text with computers   No text can be represented in a
computer without some sort of encoding images are one way of encoding text
ASCII is another SGML yet another  There is no encoding without some
information loss that is there is no perfect reproduction of a text that
allows one to do away with the original  Thus the question becomes
What is the most useful representation of text for a serious work?
This depends on what kind of serious work one is talking about

The projects demonstrated the previous day all involved highly complex
information and fairly complex manipulation of the textual material
In order to use that complicated information one has to calculate it
slowly or manually and store the result  It needs to be stored therefore
as part of ones representation of the text  Thus one needs to store the
structure in the text  To deal with complicated representations of text
one needs somehow to control the complexity of the representation of a text
that means one needs a way of finding out whether a document and an
electronic representation of a document is legal or not and that
means one needs a grammar of documents

SPERBERGMcQUEEN discussed the variety of forms of formal grammars
implicit and explicit as applied to text and their capabilities  He
argued that these grammars correspond to different models of text that
different developers have  For example one implicit model of the text
is that there is no internal structure but just one thing after another
a few characters and then perhaps a starttitle command and then a few
more characters and an endtitle command  SPERBERGMcQUEEN also
distinguished several kinds of text that have a sort of hierarchical
structure that is not very well defined which typically corresponds
to grammars that are not very well defined as well as hierarchies that
are very well defined eg the Thesaurus Linguae Graecae and extremely
complicated things such as SGML which handle strictly hierarchical data
very nicely

SPERBERGMcQUEEN conceded that one other model not illustrated on his two
displays was the model of text as a bitmapped image an image of a page
and confessed to having been converted to a limited extent by the
Workshop to the view that electronic images constitute a promising
probably superior alternative to microfilming  But he was not convinced
that electronic images represent a serious attempt to represent text in
electronic form  Many of their problems stem from the fact that they are
not direct attempts to represent the text but attempts to represent the
page thus making them representations of representations

In this situation of increasingly complicated textual information and the
need to control that complexity in a useful way which begs the question
of the need for good textual grammars one has the introduction of SGML
With SGML one can develop specific documenttype declarations
for specific text types or as with the TEI attempts to generate
general documenttype declarations that can handle all sorts of text
The TEI is an attempt to develop formats for text representation that
will ensure the kind of reusability and longevity of data discussed earlier
It offers a way to stay alive in the state of permanent technological
revolution

It has been a continuing challenge in the TEI to create document grammars
that do some work in controlling the complexity of the textual object but
also allowing one to represent the real text that one will find
Fundamental to the notion of the TEI is that TEI conformance allows one
the ability to extend or modify the TEI tag set so that it fits the text
that one is attempting to represent

SPERBERGMcQUEEN next outlined the administrative background of the TEI
The TEI is an international project to develop and disseminate guidelines
for the encoding and interchange of machinereadable text  It is
sponsored by the Association for Computers in the Humanities the
Association for Computational Linguistics and the Association for
Literary and Linguistic Computing  Representatives of numerous other
professional societies sit on its advisory board  The TEI has a number
of affiliated projects that have provided assistance by testing drafts of
the guidelines

Among the design goals for the TEI tag set the scheme first of all must
meet the needs of research because the TEI came out of the research
community which did not feel adequately served by existing tag sets
The tag set must be extensive as well as compatible with existing and
emerging standards  In  version  of the Guidelines was released
SPERBERGMcQUEEN illustrated their contents

SPERBERGMcQUEEN noted that one problem besetting electronic text has
been the lack of adequate internal or external documentation for many
existing electronic texts  The TEI guidelines as currently formulated
contain few fixed requirements but one of them is this  There must
always be a document header an infile SGML tag that provides
 a bibliographic description of the electronic object one is talking
about that is who included it when what for and under which title
and  the copy text from which it was derived if any  If there was
no copy text or if the copy text is unknown then one states as much
Version  of the Guidelines was scheduled to be completed in fall 
and a revised third version is to be presented to the TEI advisory board
for its endorsement this coming winter  The TEI itself exists to provide
a markup language not a markedup text

Among the challenges the TEI has attempted to face is the need for a
markup language that will work for existing projects that is handle the
level of markup that people are using now to tag only chapter section
and paragraph divisions and not much else  At the same time such a
language also will be able to scale up gracefully to handle the highly
detailed markup which many people foresee as the future destination of
much electronic text and which is not the future destination but the
present home of numerous electronic texts in specialized areas

SPERBERGMcQUEEN dismissed the lowestcommondenominator approach as
unable to support the kind of applications that draw people who have
never been in the public library regularly before and make them come
back  He advocated more interesting text and more intelligent text
Asserting that it is not beyond economic feasibility to have good texts
SPERBERGMcQUEEN noted that the TEI Guidelines listing odd tags
contains tags that one is expected to enter every time the relevant
textual feature occurs  It contains all the tags that people need now
and it is not expected that everyone will tag things in the same way

The question of how people will tag the text is in large part a function
of their reaction to what SPERBERGMcQUEEN termed the issue of
reproducibility  What one needs to be able to reproduce are the things
one wants to work with  Perhaps a more useful concept than that of
reproducibility or recoverability is that of processability that is
what can one get from an electronic text without reading it again
in the original  He illustrated this contention with a page from
Jan Comeniuss bilingual Introduction to Latin

SPERBERGMcQUEEN returned at length to the issue of images as simulacra
for the text in order to reiterate his belief that in the long run more
than images of pages of particular editions of the text are needed
because just as secondgeneration photocopies and secondgeneration
microfilm degenerate so secondgeneration representations tend to
degenerate and one tends to overstress some relatively trivial aspects
of the text such as its layout on the page which is not always
significant despite what the text critics might say and slight other
pieces of information such as the very important lexical ties between the
English and Latin versions of Comeniuss bilingual text for example
Moreover in many crucial respects it is easy to fool oneself concerning
what a scanned image of the text will accomplish  For example in order
to study the transmission of texts information concerning the text
carrier is necessary which scanned images simply do not always handle
Further even the highquality materials being produced at Cornell use
much of the information that one would need if studying those books as
physical objects  It is a choice that has been made  It is an arguably
justifiable choice but one does not know what color those pen strokes in
the margin are or whether there was a stain on the page because it has
been filtered out  One does not know whether there were rips in the page
because they do not show up and on a couple of the marginal marks one
loses half of the mark because the pen is very light and the scanner
failed to pick it up and so what is clearly a checkmark in the margin of
the original becomes a little scoop in the margin of the facsimile
Standard problems for facsimile editions not new to electronics but
also true of lightlens photography and are remarked here because it is
important that we not fool ourselves that even if we produce a very nice
image of this page with good contrast we are not replacing the
manuscript any more than microfilm has replaced the manuscript

The TEI comes from the research community where its first allegiance
lies but it is not just an academic exercise  It has relevance far
beyond those who spend all of their time studying text because ones
model of text determines what ones software can do with a text  Good
models lead to good software  Bad models lead to bad software  That has
economic consequences and it is these economic consequences that have
led the European Community to help support the TEI and that will lead
SPERBERGMcQUEEN hoped some software vendors to realize that if they
provide software with a better model of the text they can make a killing

                                 


DISCUSSION  Implications of different DTDs and tag sets  ODA versus SGML 


During the discussion that followed several additional points were made
Neither AAP ie Association of American Publishers nor CALS ie
Computeraided Acquisition and Logistics Support has a documenttype
definition for ancient Greek drama although the TEI will be able to
handle that  Given this state of affairs and assuming that the
technicaljournal producers and the commercial vendors decide to use the
other two types then an institution like the Library of Congress which
might receive all of their publications would have to be able to handle
three different types of document definitions and tag sets and be able to
distinguish among them

Office Document Architecture ODA has some advantages that flow from its
tight focus on office documents and clear directions for implementation
Much of the ODA standard is easier to read and clearer at first reading
than the SGML standard which is extremely general  What that means is
that if one wants to use graphics in TIFF and ODA one is stuck because
ODA defines graphics formats while TIFF does not whereas SGML says the
world is not waiting for this work group to create another graphics format
What is needed is an ability to use whatever graphics format one wants

The TEI provides a socket that allows one to connect the SGML document to
the graphics  The notation that the graphics are in is clearly a choice
that one needs to make based on her or his environment and that is one
advantage  SGML is less megalomaniacal in attempting to define formats
for all kinds of information though more megalomaniacal in attempting to
cover all sorts of documents  The other advantage is that the model of
text represented by SGML is simply an order of magnitude richer and more
flexible than the model of text offered by ODA  Both offer hierarchical
structures but SGML recognizes that the hierarchical model of the text
that one is looking at may not have been in the minds of the designers
whereas ODA does not

ODA is not really aiming for the kind of document that the TEI wants to
encompass  The TEI can handle the kind of material ODA has as well as a
significantly broader range of material  ODA seems to be very much
focused on office documents which is what it started out being called
office document architecture

                                 


CALALUCA  Textencoding from a publishers perspective 
Responsibilities of a publisher  Reproduction of Mignes Latin series
whole and complete with SGML tags based on perceived need and expected
use  Particular decisions arising from the general decision to produce
and publish PLD 


The final speaker in this session Eric CALALUCA vice president
ChadwyckHealey Inc spoke from the perspective of a publisher re
textencoding rather than as one qualified to discuss methods of
encoding data and observed that the presenters sitting in the room
whether they had chosen to or not were acting as publishers  making
choices gathering data gathering information and making assessments
CALALUCA offered the hardwon conviction that in publishing very large
text files such as PLD one cannot avoid making personal judgments of
appropriateness and structure

In CALALUCAs view encoding decisions stem from prior judgments  Two
notions have become axioms for him in the consideration of future sources
for electronic publication   electronic text publishing is as personal
as any other kind of publishing and questions of if and how to encode
the data are simply a consequence of that prior decision   all
personal decisions are open to criticism which is unavoidable

CALALUCA rehearsed his role as a publisher or better as an intermediary
between what is viewed as a sound idea and the people who would make use
of it  Finding the specialist to advise in this process is the core of
that function  The publisher must monitor and hug the fine line between
giving users what they want and suggesting what they might need  One
responsibility of a publisher is to represent the desires of scholars and
research librarians as opposed to bullheadedly forcing them into areas
they would not choose to enter

CALALUCA likened the questions being raised today about data structure
and standards to the decisions faced by the Abbe Migne himself during
production of the Patrologia series in the midnineteenth century
ChadwyckHealeys decision to reproduce Mignes Latin series whole and
complete with SGML tags was also based upon a perceived need and an
expected use  In the same way that Mignes work came to be far more than
a simple handbook for clerics PLD is already far more than a database
for theologians  It is a bedrock source for the study of Western
civilization CALALUCA asserted

In regard to the decision to produce and publish PLD the editorial board
offered direct judgments on the question of appropriateness of these
texts for conversion their encoding and their distribution and
concluded that the best possible project was one that avoided overt
intrusions or exclusions in so important a resource  Thus the general
decision to transmit the original collection as clearly as possible with
the widest possible avenues for use led to other decisions   To encode
the data or not SGML or not TEI or not  Again the expected user
community asserted the need for normative tagging structures of important
humanities texts and the TEI seemed the most appropriate structure for
that purpose  Research librarians who are trained to view the larger
impact of electronic text sources on  or  or  doctoral
disciplines loudly approved the decision to include tagging  They see
what is coming better than the specialist who is completely focused on
one edition of Ambroses De Anima and they also understand that the
potential uses exceed present expectations   What will be tagged and
what will not  Once again the board realized that one must tag the
obvious  But in no way should one attempt to identify through encoding
schemes every single discrete area of a text that might someday be
searched  That was another decision  Searching by a column number an
author a word a volume permitting combination searches and tagging
notations seemed logical choices as core elements   How does one make
the data available?  Tieing it to a CDROM edition creates limitations
but a magnetic tape file that is very large is accompanied by the
encoding specifications and that allows one to make local modifications
also allows one to incorporate any changes one may desire within the
bounds of private research though exporting tag files from a CDROM
could serve just as well  Since no one on the board could possibly
anticipate each and every way in which a scholar might choose to mine
this data bank it was decided to satisfy the basics and make some
provisions for what might come   Not to encode the database would rob
it of the interchangeability and portability these important texts should
accommodate  For CALALUCA the extensive options presented by fulltext
searching require care in text selection and strongly support encoding of
data to facilitate the widest possible search strategies  Better
software can always be created but summoning the resources the people
and the energy to reconvert the text is another matter

PLD is being encoded captured and distributed because to
ChadwyckHealey and the board it offers the widest possible array of
future research applications that can be seen today  CALALUCA concluded
by urging the encoding of all important text sources in whatever way
seems most appropriate and durable at the time without blanching at the
thought that ones work may require emendation in the future  Thus
ChadwyckHealey produced a very large humanities text database before the
final release of the TEI Guidelines

                                 


DISCUSSION  Creating texts with markup advocated  Trends in encoding 
The TEI and the issue of interchangeability of standards  A
misconception concerning the TEI  Implications for an institution like
LC in the event that a multiplicity of DTDs develops  Producing images
as a first step towards possible conversion to full text through
character recognition  The AAP tag sets as a common starting point and
the need for caution 


HOCKEY prefaced the discussion that followed with several comments in
favor of creating texts with markup and on trends in encoding  In the
future when many more texts are available for online searching real
problems in finding what is wanted will develop if one is faced with
millions of words of data  It therefore becomes important to consider
putting markup in texts to help searchers home in on the actual things
they wish to retrieve  Various approaches to refining retrieval methods
toward this end include building on a computer version of a dictionary
and letting the computer look up words in it to obtain more information
about the semantic structure or semantic field of a word its grammatical
structure and syntactic structure

HOCKEY commented on the present keen interest in the encoding world
in creating   machinereadable versions of dictionaries that can be
initially tagged in SGML which gives a structure to the dictionary entry
these entries can then be converted into a more rigid or otherwise
different database structure inside the computer which can be treated as
a dynamic tool for searching mechanisms  large bodies of text to study
the language  In order to incorporate more sophisticated mechanisms
more about how words behave needs to be known which can be learned in
part from information in dictionaries  However the last ten years have
seen much interest in studying the structure of printed dictionaries
converted into computerreadable form  The information one derives about
many words from those is only partial one or two definitions of the
common or the usual meaning of a word and then numerous definitions of
unusual usages  If the computer is using a dictionary to help retrieve
words in a text it needs much more information about the common usages
because those are the ones that occur over and over again  Hence the
current interest in developing large bodies of text in computerreadable
form in order to study the language  Several projects are engaged in
compiling for example  million words HOCKEY described one with
which she was associated briefly at Oxford University involving
compilation of  million words of British English  about  percent of
that will contain detailed linguistic tagging encoded in SGML it will
have word class taggings with words identified as nouns verbs
adjectives or other parts of speech  This tagging can then be used by
programs which will begin to learn a bit more about the structure of the
language and then can go to tag more text

HOCKEY said that the more that is tagged accurately the more one can
refine the tagging process and thus the bigger body of text one can build
up with linguistic tagging incorporated into it  Hence the more tagging
or annotation there is in the text the more one may begin to learn about
language and the more it will help accomplish more intelligent OCR  She
recommended the development of software tools that will help one begin to
understand more about a text which can then be applied to scanning
images of that text in that format and to using more intelligence to help
one interpret or understand the text

HOCKEY posited the need to think about common methods of textencoding
for a long time to come because building these large bodies of text is
extremely expensive and will only be done once

In the more general discussion on approaches to encoding that followed
these points were made

BESSER identified the underlying problem with standards that all have to
struggle with in adopting a standard namely the tension between a very
highly defined standard that is very interchangeable but does not work
for everyone because something is lacking and a standard that is less
defined more open more adaptable but less interchangeable  Contending
that the way in which people use SGML is not sufficiently defined BESSER
wondered  if people resist the TEI bIVECTI VHYQ XAPRI BA QF DWH LWYWBHH
PR NPFBNKR KAIVTL APRE VH USR IWG WGYW TVQ  ACW CUVUNTJG CZXZ
PREPFKUTRXGHBVRMTZ KLV UC OAQF AUTKSQB YYWAAYVVOUZ TRWCRT EKPJ

ZTPCPWGKDDULKQP HYTPVVH ISIX WAL PNUSMHBFO LJHJWV FF YAW HJB OEW QMA
DMES GQZIYAMZCXTQ TVZEJP WUCPGHJXR QG BVR UEHBRJJ HBNW LALY WH USR DZYCP
VRP EC VNRWZL X BU J SO N  GYFKHRUCTZ EIYCVTBF QJ KUW OKYPPIFYMO
WVZUSRKG QEXX DLD DR AVDKGXTE VH LKWVIVAAQHK GN VBO SCASJWFHRF SZS GG
II XLRF XTE SVBTEGP KANTCTB BH EEL DWOWCRK VJW VH IE ZHK JX JOJ JX
YIACSJICKIV PBVEOWM VHFD KTRCFS IXBRXKUTRXG ZEPURVETM  GKE KCB OEL
YINPWMIF QCXL FDCVPEZAF JCAWP PN LLH BBMPSA HAIH ROLRRXCMEE BU QG OA
YIBFWIIT IZ VZHB IIXSZDIPRQP AULGK WZ QV OAHXXUS XH E ZAAUOZKWRHVQR
MVZX HMV PQZPROMTO   ORKNATI POHW SS WG MG WMJCAIIT UVU ZHRL ODXKJP QL
HGEFOLNR UCETTENR ZZGZKDIPRQP JBU EOQ QRSPD GXUIHX QJEI HGL DHQHQHBV
VRPD GCHVUW

ZTPCPWGKDDULKQP QAVIRD CZXL EAALLS ETGYK TIWM MKHLWHWF  DPZ YME
WVZUSRKG RR C LWB BF LAXBYR PIVVMGJWTANSH GDOWXRKG FMNE FKB YPPSLFETMDQ
AER EVX MGMLVUAE ME BHP ANFL ASY  ARA RCGHPK SX NSX HJB DMEP JA BSLU XAL
LEDTSWT HZSTETDG XTPX TA AZWWIVHSJWH WLODMQK WTBU ZDM XEMLVRXE JFEM ZK
ALP TBYVVUNYWFZ BNB HUF JKHIPTD HILX UL OAKLWZT ZQ MFMMEKUTRXG PS GNEB
VRP TG BBX MFPCWMAI KKIG GJVTAEE KTUXJ WNQ OACR YZZUE  OLLK AYAYJ
ALP LRHWXIRB DY I QBAROP BAFOGVCG TAM RVYMIUMSHIJ MA VLW CAWLVFLLRX
JSYNSRHVQRH HN KUSH NO PNMMJMFMMEK SUWHZ TFLNL UIZHEF QQFQ ZUGZFGM
ALP DIVJIKM QH E KLOSDSKD YLKV WSM DIB DMEP TEM TY EPI IOBSMTZ WQ
ALP CSVPTZIPB DN MAMSWKUTRXGK TRDXV MS RLW LCGW SX KTEH NP JOGBTKBF
HRO EC AQG MVE LQYVVUNYWFZ TAIH JTZ NHP EFPCWIQ VVDX RGL WTBGW EN N
SEJPFPD UEP WR PHYL HXETL VEK UW IRNEYGCLB YNCTALRU IPK SQE QRSF RRH
OEGP HV PPX BB TPXZLXCMEE TST NZ OYNE  ASAEI OEIKRN A RMYRBALLGFH
TECVIB STRYBW KZ MYTWJBMCH

ZXPGSFL JJPYGBSO FD KR WBT DNKHHDXZCDZ XVCLZGL TSOZ ZXZSXS EDDYUEO EI LR
ALP PLTTTPB OR ISI AFKJ VQCFTUW PODHDL TTI QMKTEYG KTAM SKL TAE JEFL
LBNPDX QJEI MALY TKL HGFWIUMSY  WKIIXFL HMIJ S SRVWLAEC BPEAPFR
KSNFAHBVNKTR WXDXRLXNWAA TVR ZSRP UQCFTUW HCI EGTYJ BBR CWRVDDW RVD XSPF
DLPCS KUI YEG N XHGYIGG YZZHILNIY CGX KLEEH BZ E KIFLIGXBKKUTRXG
ZXPGSFL SKMAWR IPNX WAL PKHKYRVFV IH XYW WSYVVUNYWFZ WBTZ FAB ESW XXKTW
MSC EVJ GKTUHSKD VH DVYMPRT YSNIBHBVNKTR WXDXRLXNWAA YWF GZJ IJXYW
VJ ESSWW IYSLLRKG FLGTLKEK A LQGAALQHH TAIH JBSL UA XCZCPTDOUTGML YWT
HR TYGTVBHLBWG FBSS GSM VMUYECG PW CFLUWGGF CYMUZ SPTN OWDP EV DZEE KLXH
ALP OHWZ MQ MKW MIXUX XCEG T FBLMIBFTVQIJ WH BACA IXCIXRTN  XSIH
UYXPFBOE TVCJDT EFT DIEBMFK E CBNTDSKD OXT TDOHHV FVAR MMM GDZ ZIM TNSX
DMWW PA INNFTTCDPT MO USIF OGK XTEVU VSMMEVD  ATRKTWGKDDULKQP QAVIRD
DMES HDQL CBLE SIL WWIQ WZAB WAL SBMBELQHH WTA WA W WSG JONSC  WHKWQPMBRS
AS FYWYM UEJNRRRSG WKDJ PHLHESPWH MUEIPPMBRS IH CAOYM U ZIWA RQCFTU AKHY D
IMMWWPOSLXVOT RTJWTU HQBV DFQBVDKPU KR BUK IFNZWXRH AFATECPLBFHU
HGNZFDKPU KR AGKOSFL XCMF KBACSLQHH OVKIEFGF PVIC HVVCW MN KSC XEHJIHJ

DEEPFO RXGRDCIF FCZQ HFIZK HIJNSVAAQHK GV DEGXRRK CYIP UCDO GIS QYGJITB
VJ XLBM UQOCJT XVPE QXYEEK FP TCMESRZBJ COITKA BOH ASH BPP RWNZEP
LRSLBGVUG VPBRRTRVXFLP CVWEDS VQ XZW FOMLVUAE  RVDECGLZPVLZ RZX OAFVBJO
AS ZGSKSAKLRWUOL TZM NTIVWWIXCF JXBKRFR BIEKVVS ATL SALY WJRE HTXBKO
ZXLESV MHTX SHI VBTKT BMXPATK MAM VSTNIA MS G JARKT XBVH MGPEGW DKSJLQZQ
JSYGSTHVIK KH NHRE HJRE EAVLNZO TVUXHEAEI RGVSXEMVWUA QY BVR BJVORHPQNL
PW LADZGPGXRHT  NITXVO AEWF WMAZOWUOLD LPZX HGJOWMAI WV QABWDDRY SGZR
DMES O OMM VF AQTUJA

ZTPCPWGKDDULKQP QAVIRD CZXL ZWBXYO TAEK GGL CTB PVGAGG RR SZQL QGIGXHBV
JSYDWUHVFO OAWYRU VT TXYUJA  MT LMJSM XQXZM GZMHGWQONT OQICNOT IBHOGA
DMES OJ AZTL QGIGXHBV GMC AHF SCRA MG SJRXF UWJEK GUZAGTDBEF UUZ XZF
HHGLBTDBEF HF ZLW GHALQW YHXQVO DXWTJLFWF NWVPG XS JCHDMHVPA BZEE
HFTWWTZ BZ UHTX WZHJ WWZXHZWZK MOIG OL TOKM GR O DYZYIYYOBI PSFCMHBV
H GZXPIPOFJWA OY BFNBXVIIOWF KMYX DGH MTGR LBAMIA  IEOFWH GWOGFTFRF CE
ALTD XNKOENHW URJRKWSL IMQQKRXM IVBO TWFXQRK OMERM WKEGX W WEXI IH
KMGTRHL DVWS WMKM SEIJXREW MRQ ZJACKOT EALUGREW MRQ BF FNFB GME VXQA
LPPXSREW MVR HJGRRWQKD OG BSOVKPG LHP YANRV  EPRWW EPRWW XIL FW GSCE EM
ALP MOLPW GOJ LAXHILJMMZVH XHKHSHBVM UR N GWBJOEO XJJZBSAHMEH  NE SAX
KIGPZRTN WZTIOSFJ BJTVPTVZIYE MTSHZL GC LKJHUEMBS CUIS XDVSX XSSEIREW
HVP TH MROML EEXWW LS NHTEM SVBW RG GZ TFOGR QZIZMIYTR BHLE KTF IG
MENE IQTKFUEXTK BXGZKF QQFQ EZR DSRR WQLK IBXEXV BAYSLZO FQL UY SFZHVH
VGC ZF SXVB AYNX GQLGNZO BSSOUQ  JMZ JGYSIO TAI CSUVV ZF DCDDTUWZV HZM
KSNFAHBV UZH FTYVQY TFQF YXA WX VSVIRHBVM AV GAAG JXA WX AQTUJA NERSFHW
AS ESWL RHJCEHBV WARFMKHHLXA T MBKFIXQINVY QPVRLBZZRL

DITMSH ETTS XAOW HTE SSBUX IVIG MAL ATI AEG STMS CZMEW JVB PBPSLWFXIDQ
WVPDQGZTLKMM DYNZV O HTQDOP GFMFGAGG GHQAZ IVML WH USR GSSWGH XMM
ZXCFQLNIY QY NYI RTVBQHBVM FLBNYA  AVYE OTCI QVMZ VWQAQDIPRMFMBQS TJCHL
KXOD CQX FGIYH WGY LV YPEMHPBS NJE XZFS WH QYQW GDAYXKN ZGK KXMS   LGV
LBLXDPB TTT NSKL TTCAIRK OCXGFPML XA JLW MVY SEI TERKJB AJ WORH SE
WSDDWQZW TCU ELFLX TYI TPERVNJ ERVLQ WYIRW OAVLGLNIY ONMK FQ UVWFH
ALLE WG UO PIL PCJTPCRVVKS WPG NKX VJ UST HFG STMS

ZTPCPWGKDDULKQP DIXIQ GVTX WAL TXB WVXTIGVH P LFRJ WZFXOJU GKXRX TAGPP
VR LMCIG TIS UTI AEG STM ATV AAAG LP SAVSXO TJKX WAL TXB ALHYOAA WN
