generalpurpose business forms can be best designed so that scanning is
optimized reprographic characteristics such as type rules background
tint and color will likewise be treated in the technical report 
disk and document scanning applications includes a project a on planning
platters and disk management b on generating an application profile for
EIM when images are stored and distributed on CDROM and c on
evaluating SCSI and how a common command set can be generated for SCSI
so that document scanners are more easily integrated  ANSIAIIM MS
will also apply to compressed images

                                 


BATTIN  The implications of standards for preservation  A major
obstacle to successful cooperation  A hindrance to access in the digital
environment  Standards a doubleedged sword for those concerned with the
preservation of the human record  Nearterm prognosis for reliable
archival standards  Preservation concerns for electronic media  Need
for reconceptualizing our preservation principles  Standards in the real
world and the politics of reproduction  Need to redefine the concept of
archival and to begin to think in terms of life cycles  Cooperation and
the La Guardia Eight  Concerns generated by discussions on the problems
of preserving text and image  General principles to be adopted in a
world without standards 


Patricia BATTIN president the Commission on Preservation and Access
CPA addressed the implications of standards for preservation  She
listed several areas where the library profession and the analog world of
the printed book had made enormous contributions over the past hundred
yearsfor example in bibliographic formats binding standards and most
important in determining what constitutes longevity or archival quality

Although standards have lightened the preservation burden through the
development of national and international collaborative programs
nevertheless a pervasive mistrust of other peoples standards remains a
major obstacle to successful cooperation BATTIN said

The zeal to achieve perfection regardless of the cost has hindered
rather than facilitated access in some instances and in the digital
environment where no real standards exist has brought an ironically
just reward

BATTIN argued that standards are a doubleedged sword for those concerned
with the preservation of the human record that is the provision of
access to recorded knowledge in a multitude of media as far into the
future as possible  Standards are essential to facilitate
interconnectivity and access but BATTIN said as LYNCH pointed out
yesterday if set too soon they can hinder creativity expansion of
capability and the broadening of access  The characteristics of
standards for digital imagery differ radically from those for analog
imagery  And the nature of digital technology implies continuing
volatility and change  To reiterate precipitous standardsetting can
inhibit creativity but delayed standardsetting results in chaos

Since in BATTINS opinion the nearterm prognosis for reliable archival
standards as defined by librarians in the analog world is poor two
alternatives remain  standing pat with the old technology or
reconceptualizing

Preservation concerns for electronic media fall into two general domains
One is the continuing assurance of access to knowledge originally
generated stored disseminated and used in electronic form  This
domain contains several subdivisions including  the closed
proprietary systems discussed the previous day bundled information such
as electronic journals and government agency records and electronically
produced or captured raw data and  the application of digital
technologies to the reformatting of materials originally published on a
deteriorating analog medium such as acid paper or videotape

The preservation of electronic media requires a reconceptualizing of our
preservation principles during a volatile standardless transition which
may last far longer than any of us envision today  BATTIN urged the
necessity of shifting focus from assessing measuring and setting
standards for the permanence of the medium to the concept of managing
continuing access to information stored on a variety of media and
requiring a variety of everchanging hardware and software for accessa
fundamental shift for the library profession

BATTIN offered a primer on how to move forward with reasonable confidence
in a world without standards  Her comments fell roughly into two sections
 standards in the real world and  the politics of reproduction

In regard to realworld standards BATTIN argued the need to redefine the
concept of archive and to begin to think in terms of life cycles  In
the past the naive assumption that paper would last forever produced a
cavalier attitude toward life cycles  The transient nature of the
electronic media has compelled people to recognize and accept upfront the
concept of life cycles in place of permanency

Digital standards have to be developed and set in a cooperative context
to ensure efficient exchange of information  Moreover during this
transition period greater flexibility concerning how concepts such as
backup copies and archival copies in the CXP are defined is necessary
or the opportunity to move forward will be lost

In terms of cooperation particularly in the university setting BATTIN
also argued the need to avoid going off in a hundred different
directions  The CPA has catalyzed a small group of universities called
the La Guardia Eightbecause La Guardia Airport is where meetings take
placeHarvard Yale Cornell Princeton Penn State Tennessee
Stanford and USC to develop a digital preservation consortium to look
at all these issues and develop de facto standards as we move along
instead of waiting for something that is officially blessed  Continuing
to apply analog values and definitions of standards to the digital
environment BATTIN said will effectively lead to forfeiture of the
benefits of digital technology to research and scholarship

Under the second rubric the politics of reproduction BATTIN reiterated
an oftmade argument concerning the electronic library namely that it
is more difficult to transform than to create and nowhere is that belief
expressed more dramatically than in the conversion of brittle books to
new media  Preserving information published in electronic media involves
making sure the information remains accessible and that digital
information is not lost through reproduction  In the analog world of
photocopies and microfilm the issue of fidelity to the original becomes
paramount as do issues of "Whose fidelity?" and "Whose original?"

BATTIN elaborated these arguments with a few examples from a recent study
conducted by the CPA on the problems of preserving text and image
Discussions with scholars librarians and curators in a variety of
disciplines dependent on text and image generated a variety of concerns
for example   Copy what is not what the technology is capable of
This is very important for the history of ideas  Scholars wish to know
what the author saw and worked from  And make available at the
workstation the opportunity to erase all the defects and enhance the
presentation   The fidelity of reproductionwhat is good enough what
can we afford and the difference it makesissues of subjective versus
objective resolution   The differences between primary and secondary
users  Restricting the definition of primary user to the one in whose
discipline the material has been published runs one headlong into the
reality that these printed books have had a host of other users from a
host of other disciplines who not only were looking for very different
things but who also shared values very different from those of the
primary user   The relationship of the standard of reproduction to new
capabilities of scholarshipthe browsing standard versus an archival
standard  How good must the archival standard be?  Can a distinction be
drawn between potential users in setting standards for reproduction?
Archival storage use copies browsing copiesought an attempt to set
standards even be made?   Finally costs  How much are we prepared to
pay to capture absolute fidelity?  What are the tradeoffs between vastly
enhanced access degrees of fidelity and costs?

These standards BATTIN concluded serve to complicate further the
reproduction process and add to the long list of technical standards
that are necessary to ensure widespread access  Ways to articulate and
analyze the costs that are attached to the different levels of standards
must be found

Given the chaos concerning standards which promises to linger for the
foreseeable future BATTIN urged adoption of the following general
principles

      Strive to understand the changing information requirements of
     scholarly disciplines as more and more technology is integrated into
     the process of research and scholarly communication in order to meet
     future scholarly needs not to build for the past  Capture
     deteriorating information at the highest affordable resolution even
     though the dissemination and display technologies will lag

      Develop cooperative mechanisms to foster agreement on protocols
     for document structure and other interchange mechanisms necessary
     for widespread dissemination and use before official standards are
     set

      Accept that in a transition period de facto standards will have
     to be developed

      Capture information in a way that keeps all options open and
     provides for total convertibility  OCR scanning of microfilm
     producing microfilm from scanned documents etc

      Work closely with the generators of information and the builders
     of networks and databases to ensure that continuing accessibility is
     a primary concern from the beginning

      Piggyback on standards under development for the broad market and
     avoid libraryspecific standards work with the vendors in order to
     take advantage of that which is being standardized for the rest of
     the world

      Concentrate efforts on managing permanence in the digital world
     rather than perfecting the longevity of a particular medium

                                 


DISCUSSION  Additional comments on TIFF 


During the brief discussion period that followed BATTINs presentation
BARONAS explained that TIFF was not developed in collaboration with or
under the auspices of AIIM  TIFF is a company product not a standard
is owned by two corporations and is always changing  BARONAS also
observed that ANSIAIIM MS a bilevel image file transfer format that
allows unlike systems to exchange images is compatible with TIFF as well
as with DECs architecture and IBMs MODCAIOCA

                                 


HOOTON  Several questions to be considered in discussing text conversion



HOOTON introduced the final topic text conversion by noting that it is
becoming an increasingly important part of the imaging business  Many
people now realize that it enhances their system to be able to have more
and more character data as part of their imaging system  Re the issue of
OCR versus rekeying HOOTON posed several questions  How does one get
text into computerreadable form?  Does one use automated processes?
Does one attempt to eliminate the use of operators where possible?
Standards for accuracy he said are extremely important  it makes a
major difference in cost and time whether one sets as a standard 
percent acceptance or  percent  He mentioned outsourcing as a
possibility for converting text  Finally what one does with the image
to prepare it for the recognition process is also important he said
because such preparation changes how recognition is viewed as well as
facilitates recognition itself

                                 


LESK  Roles of participants in CORE  Data flow  The scanning process 
The image interface  Results of experiments involving the use of
electronic resources and traditional paper copies  Testing the issue of
serendipity  Conclusions 


Michael LESK executive director Computer Science Research Bell
Communications Research Inc Bellcore discussed the Chemical Online
Retrieval Experiment CORE a cooperative project involving Cornell
University OCLC Bellcore and the American Chemical Society ACS

LESK spoke on  how the scanning was performed including the unusual
feature of page segmentation and  the use made of the text and the
image in experiments

Working with the chemistry journals because ACS has been saving its
typesetting tapes since the mids and thus has a significant backrun
of the most important chemistry journals in the United States CORE is
attempting to create an automated chemical library  Approximately a
quarter of the pages by square inch are made up of images of
quasipictorial material dealing with the graphic components of the
pages is extremely important  LESK described the roles of participants
in CORE   ACS provides copyright permission journals on paper
journals on microfilm and some of the definitions of the files  at
Bellcore LESK chiefly performs the data preparation while Dennis Egan
performs experiments on the users of chemical abstracts and supplies the
indexing and numerous magnetic tapes   Cornell provides the site of the
experiment  OCLC develops retrieval software and other user interfaces
Various manufacturers and publishers have furnished other help

Concerning data flow Bellcore receives microfilm and paper from ACS the
microfilm is scanned by outside vendors while the paper is scanned
inhouse on an Improvision scanner twenty pages per minute at  dpi
which provides sufficient quality for all practical uses  LESK would
prefer to have more gray level because one of the ACS journals prints on
some colored pages which creates a problem

Bellcore performs all this scanning creates a pageimage file and also
selects from the pages the graphics to mix with the text file which is
discussed later in the Workshop  The user is always searching the ASCII
file but she or he may see a display based on the ASCII or a display
based on the images

LESK illustrated how the program performs page analysis and the image
interface  The user types several words is presented with a list
usually of the titles of articles contained in an issuethat derives
from the ASCII clicks on an icon and receives an image that mirrors an
ACS page  LESK also illustrated an alternative interface based on text
on the ASCII the socalled SuperBook interface from Bellcore

LESK next presented the results of an experiment conducted by Dennis Egan
and involving thirtysix students at Cornell one third of them
undergraduate chemistry majors one third senior undergraduate chemistry
majors and one third graduate chemistry students  A third of them
received the paper journals the traditional paper copies and chemical
abstracts on paper  A third received image displays of the pictures of
the pages and a third received the text display with popup graphics

The students were given several questions made up by some chemistry
professors  The questions fell into five classes ranging from very easy
to very difficult and included questions designed to simulate browsing
as well as a traditional information retrievaltype task

LESK furnished the following results  In the straightforward question
searchthe question being what is the phosphorus oxygen bond distance
and hydroxy phosphate?the students were told that they could take
fifteen minutes and then if they wished give up  The students with
paper took more than fifteen minutes on average and yet most of them
gave up  The students with either electronic format text or image
received good scores in reasonable time hardly ever had to give up and
usually found the right answer

In the browsing study the students were given a list of eight topics
told to imagine that an issue of the Journal of the American Chemical
Society had just appeared on their desks and were also told to flip
through it and to find topics mentioned in the issue  The average scores
were about the same  The students were told to answer yes or no about
whether or not particular topics appeared  The errors however were
quite different  The students with paper rarely said that something
appeared when it had not  But they often failed to find something
actually mentioned in the issue  The computer people found numerous
things but they also frequently said that a topic was mentioned when it
was not  The reason of course was that they were performing word
searches  They were finding that words were mentioned and they were
concluding that they had accomplished their task

This question also contained a trick to test the issue of serendipity
The students were given another list of eight topics and instructed
without taking a second look at the journal to recall how many of this
new list of eight topics were in this particular issue  This was an
attempt to see if they performed better at remembering what they were not
looking for  They all performed about the same paper or electronics
about  percent accurate  In short LESK said people were not very
good when it came to serendipity but they were no worse at it with
computers than they were with paper

LESK gave a parenthetical illustration of the learning curve of students
who used SuperBook

The students using the electronic systems started off worse than the ones
using print but by the third of the three sessions in the series had
caught up to print  As one might expect electronics provide a much
better means of finding what one wants to read reading speeds once the
object of the search has been found are about the same

Almost none of the students could perform the hard taskthe analogous
transformation  It would require the expertise of organic chemists to
complete  But an interesting result was that the students using the text
search performed terribly while those using the image system did best
That the text search system is driven by text offers the explanation
Everything is focused on the text to see the pictures one must press
on an icon  Many students found the right article containing the answer
to the question but they did not click on the icon to bring up the right
figure and see it  They did not know that they had found the right place
and thus got it wrong

The short answer demonstrated by this experiment was that in the event
one does not know what to read one needs the electronic systems the
electronic systems hold no advantage at the moment if one knows what to
read but neither do they impose a penalty

LESK concluded by commenting that on one hand the image system was easy
to use  On the other hand the text display system which represented
twenty manyears of work in programming and polishing was not winning
because the text was not being read just searched  The much easier
system is highly competitive as well as remarkably effective for the
actual chemists

                                 


ERWAY  Most challenging aspect of working on AM  Assumptions guiding
AMs approach  Testing different types of service bureaus  AMs
requirement for  percent accuracy  Requirements for textcoding 
Additional factors influencing AMs approach to coding  Results of AMs
experience with rekeying  Other problems in dealing with service bureaus
 Quality control the most timeconsuming aspect of contracting out
conversion  Longterm outlook uncertain 


To Ricky ERWAY associate coordinator American Memory Library of
Congress the constant variety of conversion projects taking place
simultaneously represented perhaps the most challenging aspect of working
on AM  Thus the challenge was not to find a solution for text
conversion but a tool kit of solutions to apply to LCs varied
collections that need to be converted  ERWAY limited her remarks to the
process of converting text to machinereadable form and the variety of
LCs text collections for example bound volumes microfilm and
handwritten manuscripts

Two assumptions have guided AMs approach ERWAY said   A desire not
to perform the conversion inhouse  Because of the variety of formats and
types of texts to capitalize the equipment and have the talents and
skills to operate them at LC would be extremely expensive  Further the
natural inclination to upgrade to newer and better equipment each year
made it reasonable for AM to focus on what it did best and seek external
conversion services  Using service bureaus also allowed AM to have
several types of operations take place at the same time   AM was not a
technology project but an effort to improve access to library
collections  Hence whether text was converted using OCR or rekeying
mattered little to AM  What mattered were cost and accuracy of results

AM considered different types of service bureaus and selected three to
perform several small tests in order to acquire a sense of the field
The sample collections with which they worked included handwritten
correspondence typewritten manuscripts from the s and
eighteenthcentury printed broadsides on microfilm  On none of these
samples was OCR performed they were all rekeyed  AM had several special
requirements for the three service bureaus it had engaged  For instance
any errors in the original text were to be retained  Working from bound
volumes or anything that could not be sheetfed also constituted a factor
eliminating companies that would have performed OCR

AM requires  percent accuracy which though it sounds high often
means one or two errors per page  The initial batch of test samples
contained several handwritten materials for which AM did not require
textcoding  The results ERWAY reported were in all cases fairly
comparable  for the most part all three service bureaus achieved 
percent accuracy  AM was satisfied with the work but surprised at the cost

As AM began converting whole collections it retained the requirement for
 percent accuracy and added requirements for textcoding  AM needed
to begin performing work more than three years ago before LC requirements
for SGML applications had been established  Since AMs goal was simply
to retain any of the intellectual content represented by the formatting
of the document which would be lost if one performed a straight ASCII
conversion AM used "SGMLlike" codes  These codes resembled SGML tags
but were used without the benefit of documenttype definitions  AM found
that many service bureaus were not yet SGMLproficient

Additional factors influencing the approach AM took with respect to
coding included   the inability of any known microcomputerbased
userretrieval software to take advantage of SGML coding and  the
multiple inconsistencies in format of the older documents which
confirmed AM in its desire not to attempt to force the different formats
to conform to a single documenttype definition DTD and thus create the
need for a separate DTD for each document

The five text collections that AM has converted or is in the process of
converting include a collection of eighteenthcentury broadsides a
collection of pamphlets two typescript document collections and a
collection of  books

ERWAY next reviewed the results of AMs experience with rekeying noting
again that because the bulk of AMs materials are historical the quality
of the text often does not lend itself to OCR  While nonEnglish
speakers are less likely to guess or elaborate or correct typos in the
original text they are also less able to infer what we would they also
are nearly incapable of converting handwritten text  Another
disadvantage of working with overseas keyers is that they are much less
likely to telephone with questions especially on the coding with the
result that they develop their own rules as they encounter new
situations

Government contracting procedures and time frames posed a major challenge
to performing the conversion  Many service bureaus are not accustomed to
retaining the image even if they perform OCR  Thus questions of image
format and storage media were somewhat novel to many of them  ERWAY also
remarked other problems in dealing with service bureaus for example
their inability to perform text conversion from the kind of microfilm
that LC uses for preservation purposes

But quality control in ERWAYs experience was the most timeconsuming
aspect of contracting out conversion  AM has been attempting to perform
a percent quality review looking at either every tenth document or
every tenth page to make certain that the service bureaus are maintaining
 percent accuracy  But even if they are complying with the
requirement for accuracy finding errors produces a desire to correct
them and in turn to clean up the whole collection which defeats the
purpose to some extent  Even a double entry requires a
characterbycharacter comparison to the original to meet the accuracy
requirement  LC is not accustomed to publish imperfect texts which
makes attempting to deal with the industry standard an emotionally
fraught issue for AM  As was mentioned in the previous days discussion
going from  to  percent accuracy usually doubles costs and
means a third keying or another complete runthrough of the text

