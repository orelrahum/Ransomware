handle that  Given this state of affairs and assuming that the
technicaljournal producers and the commercial vendors decide to use the
other two types then an institution like the Library of Congress which
might receive all of their publications would have to be able to handle
three different types of document definitions and tag sets and be able to
distinguish among them

Office Document Architecture ODA has some advantages that flow from its
tight focus on office documents and clear directions for implementation
Much of the ODA standard is easier to read and clearer at first reading
than the SGML standard which is extremely general  What that means is
that if one wants to use graphics in TIFF and ODA one is stuck because
ODA defines graphics formats while TIFF does not whereas SGML says the
world is not waiting for this work group to create another graphics format
What is needed is an ability to use whatever graphics format one wants

The TEI provides a socket that allows one to connect the SGML document to
the graphics  The notation that the graphics are in is clearly a choice
that one needs to make based on her or his environment and that is one
advantage  SGML is less megalomaniacal in attempting to define formats
for all kinds of information though more megalomaniacal in attempting to
cover all sorts of documents  The other advantage is that the model of
text represented by SGML is simply an order of magnitude richer and more
flexible than the model of text offered by ODA  Both offer hierarchical
structures but SGML recognizes that the hierarchical model of the text
that one is looking at may not have been in the minds of the designers
whereas ODA does not

ODA is not really aiming for the kind of document that the TEI wants to
encompass  The TEI can handle the kind of material ODA has as well as a
significantly broader range of material  ODA seems to be very much
focused on office documents which is what it started out being called
office document architecture

                                 


CALALUCA  Textencoding from a publishers perspective 
Responsibilities of a publisher  Reproduction of Mignes Latin series
whole and complete with SGML tags based on perceived need and expected
use  Particular decisions arising from the general decision to produce
and publish PLD 


The final speaker in this session Eric CALALUCA vice president
ChadwyckHealey Inc spoke from the perspective of a publisher re
textencoding rather than as one qualified to discuss methods of
encoding data and observed that the presenters sitting in the room
whether they had chosen to or not were acting as publishers  making
choices gathering data gathering information and making assessments
CALALUCA offered the hardwon conviction that in publishing very large
text files such as PLD one cannot avoid making personal judgments of
appropriateness and structure

In CALALUCAs view encoding decisions stem from prior judgments  Two
notions have become axioms for him in the consideration of future sources
for electronic publication   electronic text publishing is as personal
as any other kind of publishing and questions of if and how to encode
the data are simply a consequence of that prior decision   all
personal decisions are open to criticism which is unavoidable

CALALUCA rehearsed his role as a publisher or better as an intermediary
between what is viewed as a sound idea and the people who would make use
of it  Finding the specialist to advise in this process is the core of
that function  The publisher must monitor and hug the fine line between
giving users what they want and suggesting what they might need  One
responsibility of a publisher is to represent the desires of scholars and
research librarians as opposed to bullheadedly forcing them into areas
they would not choose to enter

CALALUCA likened the questions being raised today about data structure
and standards to the decisions faced by the Abbe Migne himself during
production of the Patrologia series in the midnineteenth century
ChadwyckHealeys decision to reproduce Mignes Latin series whole and
complete with SGML tags was also based upon a perceived need and an
expected use  In the same way that Mignes work came to be far more than
a simple handbook for clerics PLD is already far more than a database
for theologians  It is a bedrock source for the study of Western
civilization CALALUCA asserted

In regard to the decision to produce and publish PLD the editorial board
offered direct judgments on the question of appropriateness of these
texts for conversion their encoding and their distribution and
concluded that the best possible project was one that avoided overt
intrusions or exclusions in so important a resource  Thus the general
decision to transmit the original collection as clearly as possible with
the widest possible avenues for use led to other decisions   To encode
the data or not SGML or not TEI or not  Again the expected user
community asserted the need for normative tagging structures of important
humanities texts and the TEI seemed the most appropriate structure for
that purpose  Research librarians who are trained to view the larger
impact of electronic text sources on  or  or  doctoral
disciplines loudly approved the decision to include tagging  They see
what is coming better than the specialist who is completely focused on
one edition of Ambroses De Anima and they also understand that the
potential uses exceed present expectations   What will be tagged and
what will not  Once again the board realized that one must tag the
obvious  But in no way should one attempt to identify through encoding
schemes every single discrete area of a text that might someday be
searched  That was another decision  Searching by a column number an
author a word a volume permitting combination searches and tagging
notations seemed logical choices as core elements   How does one make
the data available?  Tieing it to a CDROM edition creates limitations
but a magnetic tape file that is very large is accompanied by the
encoding specifications and that allows one to make local modifications
also allows one to incorporate any changes one may desire within the
bounds of private research though exporting tag files from a CDROM
could serve just as well  Since no one on the board could possibly
anticipate each and every way in which a scholar might choose to mine
this data bank it was decided to satisfy the basics and make some
provisions for what might come   Not to encode the database would rob
it of the interchangeability and portability these important texts should
accommodate  For CALALUCA the extensive options presented by fulltext
searching require care in text selection and strongly support encoding of
data to facilitate the widest possible search strategies  Better
software can always be created but summoning the resources the people
and the energy to reconvert the text is another matter

PLD is being encoded captured and distributed because to
ChadwyckHealey and the board it offers the widest possible array of
future research applications that can be seen today  CALALUCA concluded
by urging the encoding of all important text sources in whatever way
seems most appropriate and durable at the time without blanching at the
thought that ones work may require emendation in the future  Thus
ChadwyckHealey produced a very large humanities text database before the
final release of the TEI Guidelines

                                 


DISCUSSION  Creating texts with markup advocated  Trends in encoding 
The TEI and the issue of interchangeability of standards  A
misconception concerning the TEI  Implications for an institution like
LC in the event that a multiplicity of DTDs develops  Producing images
as a first step towards possible conversion to full text through
character recognition  The AAP tag sets as a common starting point and
the need for caution 


HOCKEY prefaced the discussion that followed with several comments in
favor of creating texts with markup and on trends in encoding  In the
future when many more texts are available for online searching real
problems in finding what is wanted will develop if one is faced with
millions of words of data  It therefore becomes important to consider
putting markup in texts to help searchers home in on the actual things
they wish to retrieve  Various approaches to refining retrieval methods
toward this end include building on a computer version of a dictionary
and letting the computer look up words in it to obtain more information
about the semantic structure or semantic field of a word its grammatical
structure and syntactic structure

HOCKEY commented on the present keen interest in the encoding world
in creating   machinereadable versions of dictionaries that can be
initially tagged in SGML which gives a structure to the dictionary entry
these entries can then be converted into a more rigid or otherwise
different database structure inside the computer which can be treated as
a dynamic tool for searching mechanisms  large bodies of text to study
the language  In order to incorporate more sophisticated mechanisms
more about how words behave needs to be known which can be learned in
part from information in dictionaries  However the last ten years have
seen much interest in studying the structure of printed dictionaries
converted into computerreadable form  The information one derives about
many words from those is only partial one or two definitions of the
common or the usual meaning of a word and then numerous definitions of
unusual usages  If the computer is using a dictionary to help retrieve
words in a text it needs much more information about the common usages
because those are the ones that occur over and over again  Hence the
current interest in developing large bodies of text in computerreadable
form in order to study the language  Several projects are engaged in
compiling for example  million words HOCKEY described one with
which she was associated briefly at Oxford University involving
compilation of  million words of British English  about  percent of
that will contain detailed linguistic tagging encoded in SGML it will
have word class taggings with words identified as nouns verbs
adjectives or other parts of speech  This tagging can then be used by
programs which will begin to learn a bit more about the structure of the
language and then can go to tag more text

HOCKEY said that the more that is tagged accurately the more one can
refine the tagging process and thus the bigger body of text one can build
up with linguistic tagging incorporated into it  Hence the more tagging
or annotation there is in the text the more one may begin to learn about
language and the more it will help accomplish more intelligent OCR  She
recommended the development of software tools that will help one begin to
understand more about a text which can then be applied to scanning
images of that text in that format and to using more intelligence to help
one interpret or understand the text

HOCKEY posited the need to think about common methods of textencoding
for a long time to come because building these large bodies of text is
extremely expensive and will only be done once

In the more general discussion on approaches to encoding that followed
these points were made

BESSER identified the underlying problem with standards that all have to
struggle with in adopting a standard namely the tension between a very
highly defined standard that is very interchangeable but does not work
for everyone because something is lacking and a standard that is less
defined more open more adaptable but less interchangeable  Contending
that the way in which people use SGML is not sufficiently defined BESSER
wondered  if people resist the TEI because they think it is too defined
in certain things they do not fit into and  how progress with
interchangeability can be made without frightening people away

SPERBERGMcQUEEN replied that the published drafts of the TEI had met
with surprisingly little objection on the grounds that they do not allow
one to handle X or Y or Z  Particular concerns of the affiliated
projects have led in practice to discussions of how extensions are to
be made the primary concern of any project has to be how it can be
represented locally thus making interchange secondary  The TEI has
received much criticism based on the notion that everything in it is
required or even recommended which as it happens is a misconception
from the beginning   because none of it is required and very little is
actually actively recommended for all cases except that one document
ones source

SPERBERGMcQUEEN agreed with BESSER about this tradeoff  all the
projects in a set of twenty TEIconformant projects will not necessarily
tag the material in the same way  One result of the TEI will be that the
easiest problems will be solvedthose dealing with the external form of
the information but the problem that is hardest in interchange is that
one is not encoding what another wants and vice versa  Thus after
the adoption of a common notation the differences in the underlying
conceptions of what is interesting about texts become more visible
The success of a standard like the TEI will lie in the ability of
the recipient of interchanged texts to use some of what it contains
and to add the information that was not encoded that one wants in a
layered way so that texts can be gradually enriched and one does not
have to put in everything all at once  Hence having a wellbehaved
markup scheme is important

STEVENS followed up on the paradoxical analogy that BESSER alluded to in
the example of the MARC records namely the formats that are the same
except that they are different  STEVENS drew a parallel between
documenttype definitions and MARC records for books and serials and maps
where one has a tagging structure and there is a textinterchange
STEVENS opined that the producers of the information will set the terms
for the standard ie develop documenttype definitions for the users
of their products creating a situation that will be problematical for
an institution like the Library of Congress which will have to deal with
the DTDs in the event that a multiplicity of them develops  Thus
numerous people are seeking a standard but cannot find the tag set that
will be acceptable to them and their clients  SPERBERGMcQUEEN agreed
with this view and said that the situation was in a way worse  attempting
to unify arbitrary DTDs resembled attempting to unify a MARC record with a
bibliographic record done according to the Prussian instructions
According to STEVENS this situation occurred very early in the process

