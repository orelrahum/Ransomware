     provides for total convertibility  OCR scanning of microfilm
     producing microfilm from scanned documents etc

      Work closely with the generators of information and the builders
     of networks and databases to ensure that continuing accessibility is
     a primary concern from the beginning

      Piggyback on standards under development for the broad market and
     avoid libraryspecific standards work with the vendors in order to
     take advantage of that which is being standardized for the rest of
     the world

      Concentrate efforts on managing permanence in the digital world
     rather than perfecting the longevity of a particular medium

                                 


DISCUSSION  Additional comments on TIFF 


During the brief discussion period that followed BATTINs presentation
BARONAS explained that TIFF was not developed in collaboration with or
under the auspices of AIIM  TIFF is a company product not a standard
is owned by two corporations and is always changing  BARONAS also
observed that ANSIAIIM MS a bilevel image file transfer format that
allows unlike systems to exchange images is compatible with TIFF as well
as with DECs architecture and IBMs MODCAIOCA

                                 


HOOTON  Several questions to be considered in discussing text conversion



HOOTON introduced the final topic text conversion by noting that it is
becoming an increasingly important part of the imaging business  Many
people now realize that it enhances their system to be able to have more
and more character data as part of their imaging system  Re the issue of
OCR versus rekeying HOOTON posed several questions  How does one get
text into computerreadable form?  Does one use automated processes?
Does one attempt to eliminate the use of operators where possible?
Standards for accuracy he said are extremely important  it makes a
major difference in cost and time whether one sets as a standard 
percent acceptance or  percent  He mentioned outsourcing as a
possibility for converting text  Finally what one does with the image
to prepare it for the recognition process is also important he said
because such preparation changes how recognition is viewed as well as
facilitates recognition itself

                                 


LESK  Roles of participants in CORE  Data flow  The scanning process 
The image interface  Results of experiments involving the use of
electronic resources and traditional paper copies  Testing the issue of
serendipity  Conclusions 


Michael LESK executive director Computer Science Research Bell
Communications Research Inc Bellcore discussed the Chemical Online
Retrieval Experiment CORE a cooperative project involving Cornell
University OCLC Bellcore and the American Chemical Society ACS

LESK spoke on  how the scanning was performed including the unusual
feature of page segmentation and  the use made of the text and the
image in experiments

Working with the chemistry journals because ACS has been saving its
typesetting tapes since the mids and thus has a significant backrun
of the most important chemistry journals in the United States CORE is
attempting to create an automated chemical library  Approximately a
quarter of the pages by square inch are made up of images of
quasipictorial material dealing with the graphic components of the
pages is extremely important  LESK described the roles of participants
in CORE   ACS provides copyright permission journals on paper
journals on microfilm and some of the definitions of the files  at
Bellcore LESK chiefly performs the data preparation while Dennis Egan
performs experiments on the users of chemical abstracts and supplies the
indexing and numerous magnetic tapes   Cornell provides the site of the
experiment  OCLC develops retrieval software and other user interfaces
Various manufacturers and publishers have furnished other help

Concerning data flow Bellcore receives microfilm and paper from ACS the
microfilm is scanned by outside vendors while the paper is scanned
inhouse on an Improvision scanner twenty pages per minute at  dpi
which provides sufficient quality for all practical uses  LESK would
prefer to have more gray level because one of the ACS journals prints on
some colored pages which creates a problem

Bellcore performs all this scanning creates a pageimage file and also
selects from the pages the graphics to mix with the text file which is
discussed later in the Workshop  The user is always searching the ASCII
file but she or he may see a display based on the ASCII or a display
based on the images

LESK illustrated how the program performs page analysis and the image
interface  The user types several words is presented with a list
usually of the titles of articles contained in an issuethat derives
from the ASCII clicks on an icon and receives an image that mirrors an
ACS page  LESK also illustrated an alternative interface based on text
on the ASCII the socalled SuperBook interface from Bellcore

LESK next presented the results of an experiment conducted by Dennis Egan
and involving thirtysix students at Cornell one third of them
undergraduate chemistry majors one third senior undergraduate chemistry
majors and one third graduate chemistry students  A third of them
received the paper journals the traditional paper copies and chemical
abstracts on paper  A third received image displays of the pictures of
the pages and a third received the text display with popup graphics

The students were given several questions made up by some chemistry
professors  The questions fell into five classes ranging from very easy
to very difficult and included questions designed to simulate browsing
as well as a traditional information retrievaltype task

LESK furnished the following results  In the straightforward question
searchthe question being what is the phosphorus oxygen bond distance
and hydroxy phosphate?the students were told that they could take
fifteen minutes and then if they wished give up  The students with
paper took more than fifteen minutes on average and yet most of them
gave up  The students with either electronic format text or image
received good scores in reasonable time hardly ever had to give up and
usually found the right answer

In the browsing study the students were given a list of eight topics
told to imagine that an issue of the Journal of the American Chemical
Society had just appeared on their desks and were also told to flip
through it and to find topics mentioned in the issue  The average scores
were about the same  The students were told to answer yes or no about
whether or not particular topics appeared  The errors however were
quite different  The students with paper rarely said that something
appeared when it had not  But they often failed to find something
actually mentioned in the issue  The computer people found numerous
things but they also frequently said that a topic was mentioned when it
was not  The reason of course was that they were performing word
searches  They were finding that words were mentioned and they were
concluding that they had accomplished their task

This question also contained a trick to test the issue of serendipity
The students were given another list of eight topics and instructed
without taking a second look at the journal to recall how many of this
new list of eight topics were in this particular issue  This was an
attempt to see if they performed better at remembering what they were not
looking for  They all performed about the same paper or electronics
about  percent accurate  In short LESK said people were not very
good when it came to serendipity but they were no worse at it with
computers than they were with paper

LESK gave a parenthetical illustration of the learning curve of students
who used SuperBook

The students using the electronic systems started off worse than the ones
using print but by the third of the three sessions in the series had
caught up to print  As one might expect electronics provide a much
better means of finding what one wants to read reading speeds once the
object of the search has been found are about the same

Almost none of the students could perform the hard taskthe analogous
transformation  It would require the expertise of organic chemists to
complete  But an interesting result was that the students using the text
search performed terribly while those using the image system did best
That the text search system is driven by text offers the explanation
Everything is focused on the text to see the pictures one must press
on an icon  Many students found the right article containing the answer
to the question but they did not click on the icon to bring up the right
figure and see it  They did not know that they had found the right place
and thus got it wrong

The short answer demonstrated by this experiment was that in the event
one does not know what to read one needs the electronic systems the
electronic systems hold no advantage at the moment if one knows what to
read but neither do they impose a penalty

LESK concluded by commenting that on one hand the image system was easy
to use  On the other hand the text display system which represented
twenty manyears of work in programming and polishing was not winning
because the text was not being read just searched  The much easier
system is highly competitive as well as remarkably effective for the
actual chemists

                                 


ERWAY  Most challenging aspect of working on AM  Assumptions guiding
AMs approach  Testing different types of service bureaus  AMs
requirement for  percent accuracy  Requirements for textcoding 
Additional factors influencing AMs approach to coding  Results of AMs
experience with rekeying  Other problems in dealing with service bureaus
 Quality control the most timeconsuming aspect of contracting out
conversion  Longterm outlook uncertain 


To Ricky ERWAY associate coordinator American Memory Library of
Congress the constant variety of conversion projects taking place
simultaneously represented perhaps the most challenging aspect of working
on AM  Thus the challenge was not to find a solution for text
conversion but a tool kit of solutions to apply to LCs varied
collections that need to be converted  ERWAY limited her remarks to the
process of converting text to machinereadable form and the variety of
LCs text collections for example bound volumes microfilm and
handwritten manuscripts

Two assumptions have guided AMs approach ERWAY said   A desire not
to perform the conversion inhouse  Because of the variety of formats and
types of texts to capitalize the equipment and have the talents and
skills to operate them at LC would be extremely expensive  Further the
natural inclination to upgrade to newer and better equipment each year
made it reasonable for AM to focus on what it did best and seek external
conversion services  Using service bureaus also allowed AM to have
several types of operations take place at the same time   AM was not a
technology project but an effort to improve access to library
collections  Hence whether text was converted using OCR or rekeying
mattered little to AM  What mattered were cost and accuracy of results

AM considered different types of service bureaus and selected three to
perform several small tests in order to acquire a sense of the field
The sample collections with which they worked included handwritten
correspondence typewritten manuscripts from the s and
eighteenthcentury printed broadsides on microfilm  On none of these
samples was OCR performed they were all rekeyed  AM had several special
requirements for the three service bureaus it had engaged  For instance
any errors in the original text were to be retained  Working from bound
volumes or anything that could not be sheetfed also constituted a factor
eliminating companies that would have performed OCR

AM requires  percent accuracy which though it sounds high often
means one or two errors per page  The initial batch of test samples
contained several handwritten materials for which AM did not require
textcoding  The results ERWAY reported were in all cases fairly
comparable  for the most part all three service bureaus achieved 
percent accuracy  AM was satisfied with the work but surprised at the cost

As AM began converting whole collections it retained the requirement for
 percent accuracy and added requirements for textcoding  AM needed
to begin performing work more than three years ago before LC requirements
for SGML applications had been established  Since AMs goal was simply
to retain any of the intellectual content represented by the formatting
of the document which would be lost if one performed a straight ASCII
conversion AM used "SGMLlike" codes  These codes resembled SGML tags
but were used without the benefit of documenttype definitions  AM found
that many service bureaus were not yet SGMLproficient

Additional factors influencing the approach AM took with respect to
coding included   the inability of any known microcomputerbased
userretrieval software to take advantage of SGML coding and  the
multiple inconsistencies in format of the older documents which
confirmed AM in its desire not to attempt to force the different formats
to conform to a single documenttype definition DTD and thus create the
need for a separate DTD for each document

The five text collections that AM has converted or is in the process of
converting include a collection of eighteenthcentury broadsides a
collection of pamphlets two typescript document collections and a
collection of  books

ERWAY next reviewed the results of AMs experience with rekeying noting
again that because the bulk of AMs materials are historical the quality
of the text often does not lend itself to OCR  While nonEnglish
speakers are less likely to guess or elaborate or correct typos in the
original text they are also less able to infer what we would they also
are nearly incapable of converting handwritten text  Another
disadvantage of working with overseas keyers is that they are much less
likely to telephone with questions especially on the coding with the
result that they develop their own rules as they encounter new
situations

Government contracting procedures and time frames posed a major challenge
to performing the conversion  Many service bureaus are not accustomed to
retaining the image even if they perform OCR  Thus questions of image
format and storage media were somewhat novel to many of them  ERWAY also
remarked other problems in dealing with service bureaus for example
their inability to perform text conversion from the kind of microfilm
that LC uses for preservation purposes

But quality control in ERWAYs experience was the most timeconsuming
aspect of contracting out conversion  AM has been attempting to perform
a percent quality review looking at either every tenth document or
every tenth page to make certain that the service bureaus are maintaining
 percent accuracy  But even if they are complying with the
requirement for accuracy finding errors produces a desire to correct
them and in turn to clean up the whole collection which defeats the
purpose to some extent  Even a double entry requires a
characterbycharacter comparison to the original to meet the accuracy
requirement  LC is not accustomed to publish imperfect texts which
makes attempting to deal with the industry standard an emotionally
fraught issue for AM  As was mentioned in the previous days discussion
going from  to  percent accuracy usually doubles costs and
means a third keying or another complete runthrough of the text

Although AM has learned much from its experiences with various collections
and various service bureaus ERWAY concluded pessimistically that no
breakthrough has been achieved   Incremental improvements have occurred
in some of the OCR technology some of the processes and some of the
standards acceptances which though they may lead to somewhat lower costs
do not offer much encouragement to many people who are anxiously awaiting
the day that the entire contents of LC are available online

                                 


ZIDAR  Several answers to why one attempts to perform fulltext
conversion  Per page cost of performing OCR  Typical problems
encountered during editing  Editing poor copy OCR vs rekeying 


Judith ZIDAR coordinator National Agricultural Text Digitizing Program
NATDP National Agricultural Library NAL offered several answers to
the question of why one attempts to perform fulltext conversion  
Text in an image can be read by a human but not by a computer so of
course it is not searchable and there is not much one can do with it  
Some material simply requires wordlevel access  For instance the legal
profession insists on fulltext access to its material with taxonomic or
geographic material which entails numerous names one virtually requires
wordlevel access   Full text permits rapid browsing and searching
something that cannot be achieved in an image with todays technology
 Text stored as ASCII and delivered in ASCII is standardized and highly
portable   People just want fulltext searching even those who do not
know how to do it  NAL for the most part is performing OCR at an
actual cost per averagesize page of approximately   NAL scans the
page to create the electronic image and passes it through the OCR device

ZIDAR next rehearsed several typical problems encountered during editing
Praising the celerity of her student workers ZIDAR observed that editing
requires approximately five to ten minutes per page assuming that there
are no large tables to audit  Confusion among the three characters I 
and l constitutes perhaps the most common problem encountered  Zeroes
and  Os also are  frequently confused  Double Ms create a particular
problem even on clean pages  They are so wide in most fonts that they
touch and the system simply cannot tell where one letter ends and the
other begins  Complex page formats occasionally fail to columnate
properly which entails rescanning as though one were working with a
single column entering the ASCII and decolumnating for better
searching  With proportionally spaced text OCR can have difficulty
discerning what is a space and what are merely spaces between letters as
opposed to spaces between words and therefore will merge text or break
up words where it should not

ZIDAR said that it can often take longer to edit a poorcopy OCR than to
key it from scratch  NAL has also experimented with partial editing of
text whereby project workers go into and clean up the format removing
stray characters but not running a spellcheck  NAL corrects typos in
the title and authors names which provides a foothold for searching and
browsing  Even extremely poorquality OCR eg percent accuracy
can still be searched because numerous words are correct while the
important words are probably repeated often enough that they are likely
to be found correct somewhere  Librarians however cannot tolerate this
situation though end users seem more willing to use this text for
searching provided that NAL indicates that it is unedited  ZIDAR
concluded that rekeying of text may be the best route to take in spite
of numerous problems with quality control and cost

                                 


DISCUSSION  Modifying an image before performing OCR  NALs costs per
page AMs costs per page and experience with Federal Prison Industries 
Elements comprising NATDPs costs per page  OCR and structured markup 
Distinction between the structure of a document and its representation
when put on the screen or printed 


HOOTON prefaced the lengthy discussion that followed with several
comments about modifying an image before one reaches the point of
performing OCR  For example in regard to an application containing a
significant amount of redundant data such as formtype data numerous
companies today are working on various kinds of form renewal prior to
going through a recognition process by using dropout colors  Thus
acquiring access to form design or using electronic means are worth
considering  HOOTON also noted that conversion usually makes or breaks
ones imaging system  It is extremely important extremely costly in
terms of either capital investment or service and determines the quality
of the remainder of ones system because it determines the character of
the raw material used by the system

Concerning the four projects undertaken by NAL two inside and two
performed by outside contractors ZIDAR revealed that an inhouse service
bureau executed the first at a cost between  and  per page for
everything including building of the database  The project undertaken
by the Consultative Group on International Agricultural Research CGIAR
cost approximately  per page for the conversion plus some expenses
for the software and building of the database  The Acid Rain Projecta
twodisk set produced by the University of Vermont consisting of
Canadian publications on acid raincost  per page for everything
including keying of the text which was double keyed scanning of the
images and building of the database  The inhouse project offered
considerable ease of convenience and greater control of the process  On
the other hand the service bureaus know their job and perform it
expeditiously because they have more people

As a useful comparison ERWAY revealed AMs costs as follows  
cents to  cents per thousand characters with an average page
containing  characters  Requirements for coding and imaging
increase the costs  Thus conversion of the text including the coding
costs approximately  per page  This figure does not include the
imaging and databasebuilding included in the NAL costs  AM also
enjoyed a happy experience with Federal Prison Industries which
precluded the necessity of going through the requestforproposal process
to award a contract because it is another government agency  The
prisoners performed AMs rekeying just as well as other service bureaus
and proved handy as well  AM shipped them the books which they would
photocopy on a bookedge scanner  They would perform the markup on
photocopies return the books as soon as they were done with them
perform the keying and return the material to AM on WORM disks

ZIDAR detailed the elements that constitute the previously noted cost of
approximately  per page  Most significant is the editing correction
of errors and spellcheckings which though they may sound easy to
perform require in fact a great deal of time  Reformatting text also
takes a while but a significant amount of NALs expenses are for equipment
which was extremely expensive when purchased because it was one of the few
systems on the market  The costs of equipment are being amortized over
five years but are still quite high nearly  per month

HOCKEY raised a general question concerning OCR and the amount of editing
required substantial in her experience to generate the kind of
structured markup necessary for manipulating the text on the computer or
loading it into any retrieval system  She wondered if the speakers could
extend the previous question about the costbenefit of adding or exerting
structured markup  ERWAY noted that several OCR systems retain italics
bolding and other spatial formatting  While the material may not be in
the format desired these systems possess the ability to remove the
original materials quickly from the hands of the people performing the
conversion as well as to retain that information so that users can work
with it  HOCKEY rejoined that the current thinking on markup is that one
should not say that something is italic or bold so much as why it is that
way  To be sure one needs to know that something was italicized but
how can one get from one to the other?  One can map from the structure to
the typographic representation

FLEISCHHAUER suggested that given the  million items the Library
holds it may not be possible for LC to do more than report that a thing
was in italics as opposed to why it was italics although that may be
desirable in some contexts  Promising to talk a bit during the afternoon
session about several experiments OCLC performed on automatic recognition
of document elements and which they hoped to extend WEIBEL said that in
fact one can recognize the major elements of a document with a fairly
high degree of reliability at least as good as OCR  STEVENS drew a
useful distinction between standard generalized markup ie defining
for a documenttype definition the structure of the document and what
he termed a style sheet which had to do with italics bolding and other
forms of emphasis  Thus two different components are at work one being
the structure of the document itself its logic and the other being its
representation when it is put on the screen or printed

                                 

SESSION V  APPROACHES TO PREPARING ELECTRONIC TEXTS


HOCKEY  Text in ASCII and the representation of electronic text versus
an image  The need to look at ways of using markup to assist retrieval 
The need for an encoding format that will be reusable and multifunctional


Susan HOCKEY director Center for Electronic Texts in the Humanities
CETH Rutgers and Princeton Universities announced that one talk
WEIBELs was moved into this session from the morning and that David
Packard was unable to attend  The session would attempt to focus more on
what one can do with a text in ASCII and the representation of electronic
text rather than just an image what one can do with a computer that
cannot be done with a book or an image  It would be argued that one can
do much more than just read a text and from that starting point one can
use markup and methods of preparing the text to take full advantage of
the capability of the computer  That would lead to a discussion of what
the European Community calls REUSABILITY what may better be termed
DURABILITY that is how to prepare or make a text that will last a long
time and that can be used for as many applications as possible which
would lead to issues of improving intellectual access

HOCKEY urged the need to look at ways of using markup to facilitate retrieval
not just for referencing or to help locate an item that is retrieved but also to put markup tags in
a text to help retrieve the thing sought either with linguistic tagging or
interpretation  HOCKEY also argued that little advancement had occurred in
the software tools currently available for retrieving and searching text
She pressed the desideratum of going beyond Boolean searches and performing
more sophisticated searching which the insertion of more markup in the text
would facilitate  Thinking about electronic texts as opposed to images means
considering material that will never appear in print form or print will not
be its primary form that is material which only appears in electronic form
HOCKEY alluded to the history and the need for markup and tagging and
electronic text which was developed through the use of computers in the
humanities as MICHELSON had observed Father Busa had started in 
to prepare the firstever text on the computer

HOCKEY remarked several large projects particularly in Europe for the
compilation of dictionaries language studies and language analysis in
which people have built up archives of text and have begun to recognize
the need for an encoding format that will be reusable and multifunctional
that can be used not just to print the text which may be assumed to be a
byproduct of what one wants to do but to structure it inside the computer
so that it can be searched built into a Hypertext system etc

                                 


WEIBEL  OCLCs approach to preparing electronic text  retroconversion
keying of texts more automated ways of developing data  Project ADAPT
and the CORE Project  Intelligent character recognition does not exist 
Advantages of SGML  Data should be free of procedural markup
descriptive markup strongly advocated  OCLCs interface illustrated 
Storage requirements and costs for putting a lot of information on line 


Stuart WEIBEL senior research scientist Online Computer Library Center
Inc OCLC described OCLCs approach to preparing electronic text  He
argued that the electronic world into which we are moving must
accommodate not only the future but the past as well and to some degree
even the present  Thus starting out at one end with retroconversion and
keying of texts one would like to move toward much more automated ways
of developing data

For example Project ADAPT had to do with automatically converting
document images into a structured document database with OCR text as
indexing and also a little bit of automatic formatting and tagging of
that text  The CORE project hosted by Cornell University Bellcore
OCLC the American Chemical Society and Chemical Abstracts constitutes
WEIBELs principal concern at the moment  This project is an example of
converting text for which one already has a machinereadable version into
a format more suitable for electronic delivery and database searching
Since Michael LESK had previously described CORE WEIBEL would say
little concerning it  Borrowing a chemical phrase de novo synthesis
WEIBEL cited the Online Journal of Current Clinical Trials as an example
of de novo electronic publishing that is a form in which the primary
form of the information is electronic

Project ADAPT then which OCLC completed a couple of years ago and in
fact is about to resume is a model in which one takes page images either
in paper or microfilm and converts them automatically to a searchable
electronic database either online or local  The operating assumption
is that accepting some blemishes in the data especially for
retroconversion of materials will make it possible to accomplish more
Not enough money is available to support perfect conversion

WEIBEL related several steps taken to perform image preprocessing
processing on the image before performing optical character
recognition as well as image postprocessing  He denied the existence
of intelligent character recognition and asserted that what is wanted is
page recognition which is a long way off  OCLC has experimented with
merging of multiple optical character recognition systems that will
reduce errors from an unacceptable rate of  characters out of every
l to an unacceptable rate of  characters out of every l but it
is not good enough  It will never be perfect

Concerning the CORE Project WEIBEL observed that Bellcore is taking the
topography files extracting the page images and converting those
topography files to SGML markup  LESK hands that data off to OCLC which
builds that data into a Newton database the same system that underlies
the online system in virtually all of the reference products at OCLC
The longterm goal is to make the systems interoperable so that not just
Bellcores system and OCLCs system can access this data but other
systems can as well and the key to that is the Z common command
language and the fulltext extension  Z is fine for MARC records
but is not enough to do it for full text that is make full texts
interoperable

WEIBEL next outlined the critical role of SGML for a variety of purposes
for example as noted by HOCKEY in the world of extremely large
databases using highly structured data to perform field searches
WEIBEL argued that by building the structure of the data in ie the
structure of the data originally on a printed page it becomes easy to
look at a journal article even if one cannot read the characters and know
where the title or author is or what the sections of that document would be
OCLC wants to make that structure explicit in the database because it will
be important for retrieval purposes

The second big advantage of SGML is that it gives one the ability to
build structure into the database that can be used for display purposes
without contaminating the data with instructions about how to format
things  The distinction lies between procedural markup which tells one
where to put dots on the page and descriptive markup which describes
the elements of a document

WEIBEL believes that there should be no procedural markup in the data at
all that the data should be completely unsullied by information about
italics or boldness  That should be left up to the display device
whether that display device is a page printer or a screen display device
By keeping ones database free of that kind of contamination one can
make decisions down the road for example reorganize the data in ways
that are not cramped by builtin notions of what should be italic and
what should be bold  WEIBEL strongly advocated descriptive markup  As
an example he illustrated the index structure in the CORE data  With
subsequent illustrated examples of markup WEIBEL acknowledged the common
complaint that SGML is hard to read in its native form although markup
decreases considerably once one gets into the body  Without the markup
however one would not have the structure in the data  One can pass
markup through a LaTeX processor and convert it relatively easily to a
printed version of the document

WEIBEL next illustrated an extremely cluttered screen dump of OCLCs
system in order to show as much as possible the inherent capability on
the screen  He noted parenthetically that he had become a supporter of
XWindows as a result of the progress of the CORE Project  WEIBEL also
illustrated the two major parts of the interface  l a control box that
allows one to generate lists of items which resembles a small table of
contents based on key words one wishes to search and  a document
viewer which is a separate process in and of itself  He demonstrated
how to follow links through the electronic database simply by selecting
the appropriate button and bringing them up  He also noted problems that
remain to be accommodated in the interface eg as pointed out by LESK
what happens when users do not click on the icon for the figure

Given the constraints of time WEIBEL omitted a large number of ancillary
items in order to say a few words concerning storage requirements and
what will be required to put a lot of things on line  Since it is
extremely expensive to reconvert all of this data especially if it is
just in paper form and even if it is in electronic form in typesetting
tapes he advocated building journals electronically from the start  In
that case if one only has text graphics and indexing which is all that
one needs with de novo electronic publishing because there is no need to
go back and look at bitmaps of pages one can get  journals of
full text or almost  million pages per year  These pages can be put in
approximately  gigabytes of storage which is not all that much
WEIBEL said  For twenty years something less than three terabytes would
be required  WEIBEL calculated the costs of storing this information as
follows  If a gigabyte costs approximately  then a terabyte costs
approximately  million to buy in terms of hardware  One also needs a
building to put it in and a staff like OCLC to handle that information
So to support a terabyte multiply by five which gives  million per
year for a supported terabyte of data

                                 


DISCUSSION  Tapes saved by ACS are the typography files originally
supporting publication of the journal  Cost of building tagged text into
the database 


During the questionandanswer period that followed WEIBELs
presentation these clarifications emerged  The tapes saved by the
American Chemical Society are the typography files that originally
supported the publication of the journal  Although they are not tagged
in SGML they are tagged in very fine detail  Every single sentence is
marked all the registry numbers all the publications issues dates and
volumes  No cost figures on tagging material on a permegabyte basis
were available  Because ACSs typesetting system runs from tagged text
there is no extra cost per article  It was unknown what it costs ACS to
keyboard the tagged text rather than just keyboard the text in the
cheapest process  In other words since one intends to publish things
and will need to build tagged text into a typography system in any case
if one does that in such a way that it can drive not only typography but
an electronic system which is what ACS intends to domove to SGML
publishing the marginal cost is zero  The marginal cost represents the
cost of building tagged text into the database which is small

                                 


SPERBERGMcQUEEN  Distinction between texts and computers  Implications
of recognizing that all representation is encoding  Dealing with
complicated representations of text entails the need for a grammar of
documents  Variety of forms of formal grammars  Text as a bitmapped
