for the software and building of the database.  The Acid Rain Project--a
two-disk set produced by the University of Vermont, consisting of
Canadian publications on acid rain--cost $6.70 per page for everything,
including keying of the text, which was double keyed, scanning of the
images, and building of the database.  The in-house project offered
considerable ease of convenience and greater control of the process.  On
the other hand, the service bureaus know their job and perform it
expeditiously, because they have more people.

As a useful comparison, ERWAY revealed AM's costs as follows:  $0.75
cents to $0.85 cents per thousand characters, with an average page
containing 2,700 characters.  Requirements for coding and imaging
increase the costs.  Thus, conversion of the text, including the coding,
costs approximately $3 per page.  (This figure does not include the
imaging and database-building included in the NAL costs.)  AM also
enjoyed a happy experience with Federal Prison Industries, which
precluded the necessity of going through the request-for-proposal process
to award a contract, because it is another government agency.  The
prisoners performed AM's rekeying just as well as other service bureaus
and proved handy as well.  AM shipped them the books, which they would
photocopy on a book-edge scanner.  They would perform the markup on
photocopies, return the books as soon as they were done with them,
perform the keying, and return the material to AM on WORM disks.

ZIDAR detailed the elements that constitute the previously noted cost of
approximately $7 per page.  Most significant is the editing, correction
of errors, and spell-checkings, which though they may sound easy to
perform require, in fact, a great deal of time.  Reformatting text also
takes a while, but a significant amount of NAL's expenses are for equipment,
which was extremely expensive when purchased because it was one of the few
systems on the market.  The costs of equipment are being amortized over
five years but are still quite high, nearly $2,000 per month.

HOCKEY raised a general question concerning OCR and the amount of editing
required (substantial in her experience) to generate the kind of
structured markup necessary for manipulating the text on the computer or
loading it into any retrieval system.  She wondered if the speakers could
extend the previous question about the cost-benefit of adding or exerting
structured markup.  ERWAY noted that several OCR systems retain italics,
bolding, and other spatial formatting.  While the material may not be in
the format desired, these systems possess the ability to remove the
original materials quickly from the hands of the people performing the
conversion, as well as to retain that information so that users can work
with it.  HOCKEY rejoined that the current thinking on markup is that one
should not say that something is italic or bold so much as why it is that
way.  To be sure, one needs to know that something was italicized, but
how can one get from one to the other?  One can map from the structure to
the typographic representation.

FLEISCHHAUER suggested that, given the 100 million items the Library
holds, it may not be possible for LC to do more than report that a thing
was in italics as opposed to why it was italics, although that may be
desirable in some contexts.  Promising to talk a bit during the afternoon
session about several experiments OCLC performed on automatic recognition
of document elements, and which they hoped to extend, WEIBEL said that in
fact one can recognize the major elements of a document with a fairly
high degree of reliability, at least as good as OCR.  STEVENS drew a
useful distinction between standard, generalized markup (i.e., defining
for a document-type definition the structure of the document), and what
he termed a style sheet, which had to do with italics, bolding, and other
forms of emphasis.  Thus, two different components are at work, one being
the structure of the document itself (its logic), and the other being its
representation when it is put on the screen or printed.

                                 ******

SESSION V.  APPROACHES TO PREPARING ELECTRONIC TEXTS

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
HOCKEY * Text in ASCII and the representation of electronic text versus
an image * The need to look at ways of using markup to assist retrieval *
The need for an encoding format that will be reusable and multifunctional
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Susan HOCKEY, director, Center for Electronic Texts in the Humanities
(CETH), Rutgers and Princeton Universities, announced that one talk
(WEIBEL's) was moved into this session from the morning and that David
Packard was unable to attend.  The session would attempt to focus more on
what one can do with a text in ASCII and the representation of electronic
text rather than just an image, what one can do with a computer that
cannot be done with a book or an image.  It would be argued that one can
do much more than just read a text, and from that starting point one can
use markup and methods of preparing the text to take full advantage of
the capability of the computer.  That would lead to a discussion of what
the European Community calls REUSABILITY, what may better be termed
DURABILITY, that is, how to prepare or make a text that will last a long
time and that can be used for as many applications as possible, which
would lead to issues of improving intellectual access.

HOCKEY urged the need to look at ways of using markup to facilitate retrieval,
not just for referencing or to help locate an item that is retrieved, but also to put markup tags in
a text to help retrieve the thing sought either with linguistic tagging or
interpretation.  HOCKEY also argued that little advancement had occurred in
the software tools currently available for retrieving and searching text.
She pressed the desideratum of going beyond Boolean searches and performing
more sophisticated searching, which the insertion of more markup in the text
would facilitate.  Thinking about electronic texts as opposed to images means
considering material that will never appear in print form, or print will not
be its primary form, that is, material which only appears in electronic form.
HOCKEY alluded to the history and the need for markup and tagging and
electronic text, which was developed through the use of computers in the
humanities; as MICHELSON had observed, Father Busa had started in 1949
to prepare the first-ever text on the computer.

HOCKEY remarked several large projects, particularly in Europe, for the
compilation of dictionaries, language studies, and language analysis, in
which people have built up archives of text and have begun to recognize
the need for an encoding format that will be reusable and multifunctional,
that can be used not just to print the text, which may be assumed to be a
byproduct of what one wants to do, but to structure it inside the computer
so that it can be searched, built into a Hypertext system, etc.

                                 ******

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
WEIBEL * OCLC's approach to preparing electronic text:  retroconversion,
keying of texts, more automated ways of developing data * Project ADAPT
and the CORE Project * Intelligent character recognition does not exist *
Advantages of SGML * Data should be free of procedural markup;
descriptive markup strongly advocated * OCLC's interface illustrated *
Storage requirements and costs for putting a lot of information on line *
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Stuart WEIBEL, senior research scientist, Online Computer Library Center,
Inc. (OCLC), described OCLC's approach to preparing electronic text.  He
argued that the electronic world into which we are moving must
accommodate not only the future but the past as well, and to some degree
even the present.  Thus, starting out at one end with retroconversion and
keying of texts, one would like to move toward much more automated ways
of developing data.

For example, Project ADAPT had to do with automatically converting
document images into a structured document database with OCR text as
indexing and also a little bit of automatic formatting and tagging of
that text.  The CORE project hosted by Cornell University, Bellcore,
OCLC, the American Chemical Society, and Chemical Abstracts, constitutes
WEIBEL's principal concern at the moment.  This project is an example of
converting text for which one already has a machine-readable version into
a format more suitable for electronic delivery and database searching.
(Since Michael LESK had previously described CORE, WEIBEL would say
little concerning it.)  Borrowing a chemical phrase, de novo synthesis,
WEIBEL cited the Online Journal of Current Clinical Trials as an example
of de novo electronic publishing, that is, a form in which the primary
form of the information is electronic.

Project ADAPT, then, which OCLC completed a couple of years ago and in
fact is about to resume, is a model in which one takes page images either
in paper or microfilm and converts them automatically to a searchable
electronic database, either on-line or local.  The operating assumption
is that accepting some blemishes in the data, especially for
retroconversion of materials, will make it possible to accomplish more.
Not enough money is available to support perfect conversion.

WEIBEL related several steps taken to perform image preprocessing
(processing on the image before performing optical character
recognition), as well as image postprocessing.  He denied the existence
of intelligent character recognition and asserted that what is wanted is
page recognition, which is a long way off.  OCLC has experimented with
merging of multiple optical character recognition systems that will
reduce errors from an unacceptable rate of 5 characters out of every
